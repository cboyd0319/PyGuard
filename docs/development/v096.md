# PyGuard v0.9.6 - AI/ML Auto-Fix Group E: Output Validation & Filtering

**Version:** 0.9.6
**Phase:** Phase 6 - Auto-Fix & Remediation (Group E Implementation)
**Target:** Implement auto-fixes for LLM output validation and filtering
**Status:** 🎯 PLANNING
**Started:** 2025-10-27
**Completed:** TBD

## Progress Overview

**Previous State (v0.9.5):** 46/510 auto-fixes (9%)
**Current State:** 46/510 auto-fixes (9%)
**Target State:** 56/510 auto-fixes (11%)
**Phase 1 Progress:** 46/80 fixes complete (58%)

## Phase 6.5 Objective

Implement **Group E: Output Validation & Filtering** auto-fixes covering LLM response security.

### Goal
According to the AI/ML plan (docs/copilot/ai_ml.md), Phase 1.1 includes:
- **Output Validation & Filtering (10 checks)** - AIML061-AIML070

Implement automatic remediation for LLM output security:
- Missing output sanitization (AIML061) ✅ Partial implementation exists
- Code execution in LLM responses (AIML062)
- SQL injection via generated queries (AIML063)
- XSS via generated HTML (AIML064)
- Command injection via generated shell scripts (AIML065)
- Path traversal in generated file paths (AIML066)
- Arbitrary file access via generated code (AIML067)
- Sensitive data leakage in responses (AIML068)
- PII disclosure from training data (AIML069)
- Copyright violation risks (memorized content) (AIML070)

## Current Implementation Status

### Completed Auto-Fixes (46/80 Phase 1)

#### Foundation Fixes (17 fixes) ✅
- ✅ PyTorch model loading security
- ✅ Hugging Face trust parameters
- ✅ API security basics
- ✅ GPU memory limits
- ✅ Model versioning
- ✅ Basic prompt injection

#### Group A: Delimiter & Encoding Attacks (8 fixes) ✅
- ✅ AIML012-AIML023 (Unicode, role confusion, markdown, XML/JSON, SQL comments, Base64, ROT13)

#### Group B: Context & Token Manipulation (6 fixes) ✅
- ✅ AIML019-AIML028 (Escape sequences, token stuffing, recursive injection, templates, f-strings, variable substitution)

#### Group C: External Content Injection (6 fixes) ✅
- ✅ AIML031-AIML045 (URL, API, database, RAG, vector DB, conversation history)

#### Group D: LLM API Security (9 fixes) ✅
- ✅ AIML046-AIML060 (Parameter validation, max_tokens, streaming, functions, tools, system messages, model selection, token counting, cost overflow)

### Group E: Output Validation & Filtering (10 checks) 🎯 CURRENT

**Detection Status:** 
- ✅ Detection rules exist for AIML061-AIML070 in `pyguard/lib/ai_ml_security.py`
- ⚠️ 1 auto-fix function partially implemented (`_fix_output_sanitization`)
- ❌ 9 auto-fix functions need implementation

**Implementation Plan:**

#### Already Implemented (Partial) ✅
- ⚠️ AIML061: Missing output sanitization - `_fix_output_sanitization` exists but incomplete

#### High-Priority Fixes (10 fixes) - TO IMPLEMENT 🎯

**Fix 1: AIML061 - Missing Output Sanitization** (Complete existing)
- 🎯 Complete `_fix_output_sanitization` implementation
- 🎯 Detect missing output validation
- 🎯 Add sanitization warnings for LLM responses
- 🎯 Recommend output filtering
- 🎯 Write unit tests (2 tests)

**Fix 2: AIML062 - Code Execution in LLM Responses**
- 🎯 Implement `_fix_code_execution_in_response` function
- 🎯 Detect eval/exec on LLM output
- 🎯 Add critical security warnings
- 🎯 Recommend sandboxing
- 🎯 Write unit tests (2 tests)

**Fix 3: AIML063 - SQL Injection via Generated Queries**
- 🎯 Implement `_fix_sql_injection_generated` function
- 🎯 Detect SQL execution from LLM output
- 🎯 Add parameterization warnings
- 🎯 Recommend query validation
- 🎯 Write unit tests (2 tests)

**Fix 4: AIML064 - XSS via Generated HTML**
- 🎯 Implement `_fix_xss_generated_html` function
- 🎯 Detect HTML rendering from LLM output
- 🎯 Add XSS prevention warnings
- 🎯 Recommend HTML escaping
- 🎯 Write unit tests (2 tests)

**Fix 5: AIML065 - Command Injection via Generated Scripts**
- 🎯 Implement `_fix_command_injection_generated` function
- 🎯 Detect shell execution from LLM output
- 🎯 Add command injection warnings
- 🎯 Recommend input validation
- 🎯 Write unit tests (2 tests)

**Fix 6: AIML066 - Path Traversal in Generated Paths**
- 🎯 Implement `_fix_path_traversal_generated` function
- 🎯 Detect file operations with LLM paths
- 🎯 Add path validation warnings
- 🎯 Recommend path sanitization
- 🎯 Write unit tests (2 tests)

**Fix 7: AIML067 - Arbitrary File Access via Generated Code**
- 🎯 Implement `_fix_arbitrary_file_access` function
- 🎯 Detect file operations from LLM code
- 🎯 Add file access control warnings
- 🎯 Recommend whitelist validation
- 🎯 Write unit tests (2 tests)

**Fix 8: AIML068 - Sensitive Data Leakage in Responses**
- 🎯 Implement `_fix_sensitive_data_leakage` function
- 🎯 Detect logging/storage of LLM output
- 🎯 Add PII/credential warnings
- 🎯 Recommend output filtering
- 🎯 Write unit tests (2 tests)

**Fix 9: AIML069 - PII Disclosure from Training Data**
- 🎯 Implement `_fix_pii_disclosure_training` function
- 🎯 Detect unfiltered LLM responses
- 🎯 Add PII detection warnings
- 🎯 Recommend output validation
- 🎯 Write unit tests (2 tests)

**Fix 10: AIML070 - Copyright Violation Risks**
- 🎯 Implement `_fix_copyright_violation_risks` function
- 🎯 Detect unvalidated LLM content usage
- 🎯 Add copyright warnings
- 🎯 Recommend content moderation
- 🎯 Write unit tests (2 tests)

**Total New Tests:** 21 tests (20 individual + 1 integration test)

## Implementation Strategy

### Phase E.1: Critical Injection Risks (4 fixes)
Focus on highest severity output attacks:
1. Code execution in responses (AIML062)
2. SQL injection via generated queries (AIML063)
3. Command injection via generated scripts (AIML065)
4. Arbitrary file access (AIML067)

**Rationale:** These represent the most critical security risks with immediate exploitation potential.

### Phase E.2: Web Security (2 fixes)
Focus on web application security:
5. XSS via generated HTML (AIML064)
6. Path traversal in generated paths (AIML066)

**Rationale:** Common web attack vectors in LLM-powered applications.

### Phase E.3: Data Protection (4 fixes)
Focus on data security and compliance:
7. Missing output sanitization (AIML061)
8. Sensitive data leakage (AIML068)
9. PII disclosure (AIML069)
10. Copyright violations (AIML070)

**Rationale:** Data protection is essential for regulatory compliance.

## Testing Strategy

### Per Fix Requirements (Minimum 15 tests)

1. **Basic Detection (3 tests)**
   - Vulnerable output handling
   - Safe output validation
   - Edge cases

2. **Fix Application (3 tests)**
   - Comment insertion correctness
   - Warning message accuracy
   - Multiple occurrences

3. **Framework Coverage (3 tests)**
   - OpenAI responses
   - Generic LLM outputs
   - Multi-step processing

4. **Integration (3 tests)**
   - Real-world LLM applications
   - Output chain processing
   - Mixed security patterns

5. **Regression (3 tests)**
   - Idempotency validation
   - No false positives
   - Preserves functionality

### Expected Test Class

```python
class TestGroupEOutputValidationFixes:
    """Test Group E: Output Validation & Filtering auto-fixes (AIML061-070)."""
    
    def test_output_sanitization_fix(self, tmp_path):
        """Test AIML061: Output sanitization detection."""
        # Test missing output validation
        
    def test_code_execution_response_fix(self, tmp_path):
        """Test AIML062: Code execution prevention."""
        # Test eval/exec on LLM output
        
    def test_sql_injection_generated_fix(self, tmp_path):
        """Test AIML063: SQL injection via generated queries."""
        # Test SQL execution from LLM
        
    def test_xss_generated_html_fix(self, tmp_path):
        """Test AIML064: XSS prevention."""
        # Test HTML rendering from LLM
        
    def test_command_injection_generated_fix(self, tmp_path):
        """Test AIML065: Command injection prevention."""
        # Test shell execution from LLM
        
    def test_path_traversal_generated_fix(self, tmp_path):
        """Test AIML066: Path traversal prevention."""
        # Test file operations with LLM paths
        
    def test_arbitrary_file_access_fix(self, tmp_path):
        """Test AIML067: File access control."""
        # Test file operations from LLM code
        
    def test_sensitive_data_leakage_fix(self, tmp_path):
        """Test AIML068: Data leakage prevention."""
        # Test logging/storage of LLM output
        
    def test_pii_disclosure_training_fix(self, tmp_path):
        """Test AIML069: PII disclosure prevention."""
        # Test unfiltered LLM responses
        
    def test_copyright_violation_fix(self, tmp_path):
        """Test AIML070: Copyright protection."""
        # Test unvalidated LLM content
        
    def test_group_e_integration(self, tmp_path):
        """Test all Group E fixes working together."""
        # Comprehensive integration test
```

## Success Metrics

**Technical Targets:**
- 🎯 **10 new auto-fixes implemented** (Group E)
- 🎯 **21 unit tests** (2 per fix + 1 integration test)
- 🎯 **100% test pass rate** (all AI/ML tests passing)
- 🎯 **56/80 Phase 1 progress** (70% complete)

**Quality Targets:**
- 🎯 All fixes preserve code functionality
- 🎯 All fixes include educational comments
- 🎯 All fixes have OWASP/CWE references (OWASP LLM02, CWE-79, CWE-89, CWE-94)
- 🎯 <5% false positive rate (AST-based detection)

## Timeline

**Estimated Duration:** 2-3 weeks for Group E (10 fixes)
**Current Status:** Planning phase
**Next Steps:** Implement Phase E.1 (Critical Injection Risks)

**Breakdown:**
- Phase E.1: 4-5 days (4 critical injection fixes)
- Phase E.2: 2-3 days (2 web security fixes)
- Phase E.3: 3-4 days (4 data protection fixes)
- Testing & Integration: 2-3 days
- Documentation & Review: 1-2 days

## Implementation Steps

### Immediate Actions
1. 🎯 Complete `_fix_output_sanitization` in `pyguard/lib/ai_ml_security.py`
2. 🎯 Create `_fix_code_execution_in_response`
3. 🎯 Create `_fix_sql_injection_generated`
4. 🎯 Create `_fix_xss_generated_html`
5. 🎯 Create `_fix_command_injection_generated`
6. 🎯 Create `_fix_path_traversal_generated`
7. 🎯 Create `_fix_arbitrary_file_access`
8. 🎯 Create `_fix_sensitive_data_leakage`
9. 🎯 Create `_fix_pii_disclosure_training`
10. 🎯 Create `_fix_copyright_violation_risks`
11. 🎯 Add all fixes to `fix_file` method
12. 🎯 Update safety classifier in `pyguard/lib/fix_safety.py`
13. 🎯 Create test class `TestGroupEOutputValidationFixes` in `tests/unit/test_ai_ml_security.py`
14. 🎯 Write 21 comprehensive tests (2 per fix + integration)
15. 🎯 Run full test suite and validate
16. 🎯 Update v096.md to mark as complete
17. 🎯 Create v097.md for next phase

## Architecture Notes

### Auto-Fix Pattern for Output Validation

```python
def _fix_<output_vulnerability>(self, content: str) -> str:
    """
    Validate LLM output before using in sensitive operations.
    
    Classification: SAFE (Warning Only)
    - Detects unsafe output handling
    - Adds validation warnings
    - Prevents injection via LLM responses
    
    Before: result = execute_code(llm_response.content)
    After:  # PyGuard: Validate LLM output before execution [AIML0XX]
            # Risk: LLM can generate malicious code/queries/commands
            # Recommendation: Sandbox, validate, and sanitize all LLM outputs
    
    Reference: AIML0XX, OWASP LLM02, CWE-XXX
    """
    fix_id = "output_validation_<specific>"
    if not self.safety_classifier.should_apply_fix(fix_id, self.allow_unsafe):
        return content
    
    # Check for LLM output usage patterns
    output_patterns = [
        'llm_response', 'response.content', 'completion.text',
        'generated_', 'llm_output', 'model_response'
    ]
    
    dangerous_operations = [
        'eval(', 'exec(', 'execute(', 'subprocess.', 'os.system',
        'cursor.execute', 'open(', 'render_template'
    ]
    
    if any(output in content for output in output_patterns):
        if any(op in content for op in dangerous_operations):
            # Add output validation warning
            pass
    
    return content
```

## References

- OWASP LLM Top 10 2023 (LLM02: Insecure Output Handling)
- CWE-79: Cross-site Scripting (XSS)
- CWE-89: SQL Injection
- CWE-94: Code Injection
- CWE-78: OS Command Injection
- CWE-22: Path Traversal
- CWE-200: Information Disclosure
- `docs/copilot/ai_ml.md` - Complete AI/ML plan (Phase 1.1.4)
- `docs/development/v095.md` - Previous phase (Group D)

---

**Previous Phase:** See `v095.md` for Group D (LLM API Security)
**Current Phase:** 🎯 Planning Group E (Output Validation & Filtering)
**Next Phase:** See `v097.md` for Phase 1.2 (Model Serialization)

---

**Status:** 🎯 PLANNING - Group E implementation starting
**Achievement:** 46/80 Phase 1 fixes complete (58%)
**Focus:** LLM output security - injection prevention, data protection, compliance
**Impact:** Critical for production LLM applications handling generated content

## Phase 1 Completion Status

After Group E, Phase 1.1 (Prompt Injection & LLM Security) will have:
- ✅ Direct Prompt Injection (20 checks) - Groups A, B partially complete
- ✅ Indirect Prompt Injection (15 checks) - Group C complete  
- ✅ LLM API Security (15 checks) - Group D complete
- 🎯 Output Validation & Filtering (10 checks) - Group E in progress

**Next Major Phase:** Phase 1.2 - Model Serialization & Loading (40 checks)
- PyTorch Model Security (15 checks)
- TensorFlow/Keras Security (15 checks)
- Hugging Face & Transformers (10 checks)
