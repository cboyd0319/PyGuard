# PyGuard v0.8.7 - AI/ML Security Dominance - Phase 6: Stage 3 - Prompt Injection Auto-Fixes

**Version:** 0.8.7
**Phase:** Phase 6 - Auto-Fix & Remediation (Stage 3)
**Target:** Implement 30 prompt injection auto-fixes (input sanitization & output validation)
**Status:** ðŸš§ In Progress
**Started:** 2025-10-24
**Completed:** TBD

## Progress Overview

**Previous State (v0.8.6):** 15 auto-fixes implemented with full test coverage (Stage 2 complete)
**Current State:** Implementing Stage 3 - Prompt Injection auto-fixes
**Target State:** 45 auto-fixes total (15 existing + 30 new)
**Progress:** 15/500 auto-fixes implemented (3.0%)
**Tests:** 18 tests passing

## Phase 6 Stage 3 Goals - Prompt Injection Auto-Fixes

Following the comprehensive plan in `docs/copilot/ai_ml.md`:

### Stage 3: Prompt Injection Auto-Fixes (30 checks)

**Objective:** Secure LLM applications against prompt injection attacks

**Input Sanitization (20 auto-fixes):**
- AIML011: System prompt override â†’ input validation
- AIML012: Unicode injection â†’ normalize input
- AIML013: Role confusion â†’ role validation
- AIML014: Instruction concatenation â†’ input parsing
- AIML015: Multi-language injection â†’ language detection
- AIML016: Markdown injection â†’ escape/sanitize
- AIML017: XML/JSON payload injection â†’ validate structure
- AIML018: SQL-style comment injection â†’ remove comments
- AIML019: Escape sequence injection â†’ normalize sequences
- AIML020: Token stuffing â†’ limit token count
- AIML021: Recursive prompt injection â†’ detect recursion
- AIML022: Base64 encoded injection â†’ detect/decode
- AIML023: ROT13/Caesar cipher â†’ detect obfuscation
- AIML024: Invisible character injection â†’ remove zero-width chars
- AIML025: Right-to-left override â†’ normalize bidi
- AIML026: Template literal injection â†’ sanitize templates
- AIML027: F-string injection â†’ escape f-strings
- AIML028: Variable substitution â†’ validate substitutions
- AIML029: Context window overflow â†’ enforce limits
- AIML030: Attention manipulation â†’ detect patterns

**Output Validation (10 auto-fixes):**
- AIML062: Code execution in responses â†’ sanitize code blocks
- AIML063: SQL injection via generated queries â†’ parameterize
- AIML064: XSS via generated HTML â†’ escape HTML
- AIML065: Command injection via shell scripts â†’ escape commands
- AIML066: Path traversal in file paths â†’ normalize paths
- AIML067: Arbitrary file access â†’ validate paths
- AIML068: Sensitive data leakage â†’ redact PII
- AIML069: PII disclosure â†’ detect and mask
- AIML070: Copyright violation â†’ detect memorized content

## Implementation Approach

### Challenge: Complex Context-Dependent Fixes

Prompt injection fixes are inherently complex because they require:
- Understanding the context of LLM API calls
- Identifying user input variables
- Determining appropriate sanitization strategies
- Preserving intended functionality

### Strategy: Progressive Enhancement

**Phase 1 (Current):** Warning-based approach
- Add educational comments at injection points
- Suggest sanitization patterns
- Provide code examples
- Link to security resources

**Phase 2 (Future):** AST-based transformations
- Analyze code flow to identify user input
- Add sanitization wrapper functions
- Transform dangerous patterns to safe alternatives
- Validate input before LLM calls

### Implementation Plan for v0.8.7

**Stage 3A: Input Sanitization Warnings (10 fixes)**
- [ ] AIML011-015: Basic injection patterns (delimiter, Unicode, role, concatenation, language)
- [ ] AIML016-020: Payload injection (Markdown, XML/JSON, SQL comments, escape sequences, token stuffing)

**Stage 3B: Advanced Input Sanitization (10 fixes)**
- [ ] AIML021-025: Sophisticated attacks (recursion, Base64, cipher, invisible chars, bidi)
- [ ] AIML026-030: Template attacks (template literals, f-strings, substitution, overflow, attention)

**Stage 3C: Output Validation (10 fixes)**
- [ ] AIML062-066: Code execution prevention (code blocks, SQL, XSS, commands, paths)
- [ ] AIML067-070: Data protection (file access, PII, copyright)

## Auto-Fix Pattern: Comment-Based Warnings

Since prompt injection fixes require deep context analysis, we'll use a warning-based approach:

### Pattern 1: User Input Detection

```python
# Before (Vulnerable):
user_input = request.args.get('query')
prompt = f"Answer this: {user_input}"
response = openai.ChatCompletion.create(messages=[{"role": "user", "content": prompt}])

# After (With Warning):
# PyGuard: Validate and sanitize user input before LLM call (AIML011)
# Consider: input validation, delimiter detection, role validation
# Example: user_input = sanitize_llm_input(request.args.get('query'))
user_input = request.args.get('query')
prompt = f"Answer this: {user_input}"
response = openai.ChatCompletion.create(messages=[{"role": "user", "content": prompt}])
```

### Pattern 2: Output Sanitization

```python
# Before (Vulnerable):
llm_response = response.choices[0].message.content
return render_template('result.html', result=llm_response)

# After (With Warning):
llm_response = response.choices[0].message.content
# PyGuard: Sanitize LLM output before rendering (AIML064)
# Consider: HTML escaping, XSS prevention, content validation
# Example: safe_response = html.escape(llm_response)
return render_template('result.html', result=llm_response)
```

## Implementation Details

### Detection Strategies

1. **F-string detection in LLM context**
   - Pattern: `f"...{variable}..."` near OpenAI/Anthropic/Cohere calls
   - Action: Add sanitization warning

2. **Direct user input to LLM**
   - Pattern: `request.`, `input()`, `sys.argv` flowing to LLM
   - Action: Add validation warning

3. **Unsanitized output rendering**
   - Pattern: LLM response â†’ render_template/HTML without escaping
   - Action: Add sanitization warning

4. **Template injection patterns**
   - Pattern: `.format()`, `%` formatting, template engines
   - Action: Add template security warning

### Safety Classification

All Stage 3 fixes will be classified as **WARNING_ONLY** because:
- Require deep semantic analysis
- Context-dependent implementation
- No single "correct" fix approach
- Manual review recommended

## Testing Strategy

### Test Categories

1. **Detection Tests:** Verify warnings are added at correct locations
2. **Idempotency Tests:** Warnings not added multiple times
3. **Preservation Tests:** Original code structure maintained
4. **Coverage Tests:** All injection patterns detected

### Example Test

```python
def test_f_string_prompt_injection_warning(tmp_path):
    """Test f-string prompt injection warning."""
    code = '''
import openai
user_input = input()
prompt = f"Answer: {user_input}"
response = openai.ChatCompletion.create(messages=[{"role": "user", "content": prompt}])
'''
    test_file = tmp_path / "test.py"
    test_file.write_text(code)
    
    fixer = AIMLSecurityFixer(allow_unsafe=False)
    success, fixes = fixer.fix_file(test_file)
    
    assert success
    assert any("AIML011" in fix or "prompt injection" in fix for fix in fixes)
    
    fixed_code = test_file.read_text()
    assert "PyGuard:" in fixed_code
    assert "sanitize" in fixed_code.lower()
```

## Success Metrics

**Stage 3 Targets:**
- ðŸŽ¯ **30 auto-fixes implemented** (prompt injection)
- ðŸŽ¯ **30+ tests created** (1+ per fix)
- ðŸŽ¯ **100% test pass rate** maintained
- ðŸŽ¯ **Warning-based approach** for all prompt injection
- ðŸŽ¯ **Educational comments** with examples
- ðŸŽ¯ **Total progress: 45/500 (9%)**

**Quality Standards:**
- All fixes preserve code structure
- All fixes include educational comments
- All fixes link to security frameworks (OWASP LLM01, CWE-79, etc.)
- All fixes provide actionable guidance
- All fixes tested for idempotency

## Timeline

**Day 3 (Current):**
- Stage 3A: Input sanitization warnings (10 fixes)
- Stage 3B: Advanced input sanitization (10 fixes)
- Stage 3C: Output validation (10 fixes)
- Testing and validation

**Target Completion:** Day 3 (2025-10-24)

## Progress Tracking

### Implemented (15/500 = 3.0%)

From v0.8.5 and v0.8.6:
1. torch_load_weights_only (AIML071)
2. from_pretrained_trust (AIML101)
3. api_key_exposure (AIML054)
4. gpu_memory_limits (AIML081)
5. llm_rate_limiting (AIML046)
6. model_versioning (AIML440)
7. output_sanitization (AIML061)
8. prompt_injection_basic (AIML011-045)
9. api_parameter_validation (AIML047)
10. missing_timeout (AIML056)
11. unhandled_api_errors (AIML057)
12. untrusted_url_loading (AIML074)
13. torch_jit_load (AIML077)
14. model_integrity_verification (AIML073)
15. model_card_credentials (AIML102)

### To Implement (Stage 3: 30 fixes)

- [ ] AIML011-030: Input sanitization (20 fixes)
- [ ] AIML062-070: Output validation (10 fixes)

### Remaining (455 fixes)

- Stage 4: Training & Pipeline (60 fixes)
- Stage 5: Framework-Specific (115 fixes)
- Stage 6: Supply Chain (80 fixes)
- Stage 7: Emerging Threats (45 fixes)
- Stage 8: Additional coverage (155 fixes)

## References

- OWASP LLM Top 10 - LLM01: Prompt Injection
- OWASP LLM02: Insecure Output Handling
- CWE-79: Improper Neutralization of Input
- CWE-94: Improper Control of Generation of Code
- MITRE ATLAS: LLM Prompt Injection Techniques
- `docs/copilot/ai_ml.md` - Complete AI/ML security plan

---

**Previous Phase:** See `v086.md` for Stage 2 completion (15 auto-fixes)
**Current Phase:** Stage 3 - Prompt Injection Auto-Fixes (In Progress)
**Next Phase:** Stage 4 - Training & Pipeline Security (60 auto-fixes)

---

**Status:** ðŸš§ Stage 3 implementation starting
**Next Steps:** Implement input sanitization warnings (AIML011-030)
