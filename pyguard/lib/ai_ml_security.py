"""
AI/ML Security Analysis.

Detects security vulnerabilities in AI/ML applications including prompt injection,
model serialization risks, training data poisoning, and GPU memory leakage.

Security Areas Covered:
- Prompt injection in LLM applications
- System prompt override attempts (delimiter injection) - AIML011
- Unicode/homoglyph injection (zero-width characters, bi-directional overrides) - AIML012
- Role confusion attacks (DAN mode, jailbreaks) - AIML013
- Instruction concatenation bypasses - AIML014
- Multi-language prompt injection (non-English) - AIML015
- Markdown injection in prompts - AIML016
- XML/JSON payload injection - AIML017
- SQL-style comment injection - AIML018
- Escape sequence injection - AIML019
- Token stuffing attacks (context window exhaustion) - AIML020
- Recursive prompt injection (prompts containing prompts) - AIML021
- Base64 encoded injection attempts - AIML022
- ROT13/Caesar cipher obfuscation - AIML023
- Invisible character injection (zero-width spaces) - AIML024
- Right-to-left override attacks (Unicode bidi) - AIML025
- Prompt template literal injection - AIML026
- F-string injection in prompts - AIML027
- Variable substitution attacks - AIML028
- Context window overflow - AIML029
- Attention mechanism manipulation - AIML030
- URL-based injection (fetched web content) - AIML031
- Document poisoning (PDF, DOCX injection) - AIML032
- Image-based prompt injection (OCR manipulation) - AIML033
- API response injection (3rd party data) - AIML034
- Database content injection - AIML035
- File upload injection vectors - AIML036
- Email content injection - AIML037
- Social media scraping injection - AIML038
- RAG poisoning (retrieval augmented generation) - AIML039
- Vector database injection - AIML040
- Knowledge base tampering - AIML041
- Citation manipulation - AIML042
- Search result poisoning - AIML043
- User profile injection - AIML044
- Conversation history injection - AIML045
- Missing rate limiting on LLM API calls - AIML046
- Unvalidated temperature/top_p parameters - AIML047
- Max_tokens manipulation (DoS) - AIML048
- Streaming response injection - AIML049
- Function calling injection - AIML050
- Tool use parameter tampering - AIML051
- System message manipulation via API - AIML052
- Model selection bypass - AIML053
- API key exposure in client code - AIML054
- Hardcoded model names (version lock-in) - AIML055
- Missing timeout configurations - AIML056
- Unhandled API errors (info disclosure) - AIML057
- Token counting bypass - AIML058
- Cost overflow attacks - AIML059
- Multi-turn conversation state injection - AIML060
- Missing output sanitization - AIML061
- Code execution in LLM responses - AIML062
- SQL injection via generated queries - AIML063
- XSS via generated HTML - AIML064
- Command injection via generated shell scripts - AIML065
- Path traversal in generated file paths - AIML066
- Arbitrary file access via generated code - AIML067
- Sensitive data leakage in responses - AIML068
- PII disclosure from training data - AIML069
- Copyright violation risks (memorized content) - AIML070
- torch.load() without weights_only=True - AIML071
- Unsafe pickle in torch.save/load - AIML072
- Missing model integrity verification - AIML073
- Untrusted model URL loading - AIML074
- Model poisoning in state_dict - AIML075
- Custom layer/module injection - AIML076
- Unsafe torch.jit.load() - AIML077
- TorchScript deserialization risks - AIML078
- ONNX model tampering - AIML079
- Model metadata injection - AIML080
- Missing GPU memory limits - AIML081
- Tensor size attacks (memory exhaustion) - AIML082
- Quantization vulnerabilities - AIML083
- Mixed precision attacks - AIML084
- Model zoo trust verification - AIML085
- SavedModel arbitrary code execution - AIML086
- HDF5 deserialization attacks - AIML087
- Custom object injection in model.load - AIML088
- TensorFlow Hub model trust - AIML089
- Graph execution injection - AIML090
- Checkpoint poisoning - AIML091
- Keras Lambda layer code injection - AIML092
- Custom metric/loss function tampering - AIML093
- TF Lite model manipulation - AIML094
- TensorBoard log injection - AIML095
- Model serving vulnerabilities (TF Serving) - AIML096
- GraphDef manipulation - AIML097
- Operation injection attacks - AIML098
- Resource exhaustion via model architecture - AIML099
- TFRecord poisoning - AIML100
- from_pretrained() trust issues - AIML101
- Model card credential leakage - AIML102
- Tokenizer vulnerabilities - AIML103
- Pipeline injection attacks - AIML104
- Dataset poisoning (Hugging Face Datasets) - AIML105
- Missing model signature verification - AIML106
- Arbitrary file loading in model config - AIML107
- Space app injection (Gradio/Streamlit) - AIML108
- Model repository tampering - AIML109
- Private model access control - AIML110
- Unvalidated training data sources - AIML111
- Missing data sanitization - AIML112
- PII leakage in training datasets - AIML113
- Copyright-infringing data inclusion - AIML114
- Data poisoning detection (label flipping) - AIML115
- Backdoor injection in datasets - AIML116
- Trigger pattern insertion - AIML117
- Data augmentation attacks - AIML118
- Synthetic data vulnerabilities - AIML119
- Web scraping data risks - AIML120
- User-generated content risks - AIML121
- Missing data provenance tracking - AIML122
- Gradient manipulation attacks - AIML123
- Learning rate manipulation - AIML124
- Optimizer state poisoning - AIML125
- Checkpoint tampering during training - AIML126
- Early stopping bypass - AIML127
- Validation set poisoning - AIML128
- TensorBoard logging injection - AIML129
- Experiment tracking manipulation - AIML130
- Distributed training node compromise - AIML131
- Parameter server vulnerabilities - AIML132
- Base model poisoning - AIML133
- Fine-tuning data injection - AIML134
- Catastrophic forgetting exploitation - AIML135
- PEFT attacks - AIML136
- LoRA poisoning - AIML137
- Adapter injection - AIML138
- Prompt tuning manipulation - AIML139
- Instruction fine-tuning risks - AIML140
- Missing input adversarial defense - AIML141
- No FGSM protection - AIML142
- PGD vulnerability - AIML143
- C&W attack surface - AIML144
- DeepFool susceptibility - AIML145
- Universal adversarial perturbations - AIML146
- Black-box attack vulnerability - AIML147
- Transfer attack risks - AIML148
- Physical adversarial examples - AIML149
- Adversarial patch detection missing - AIML150
- Missing adversarial training - AIML151
- No certified defenses - AIML152
- Input gradient masking - AIML153
- Defensive distillation gaps - AIML154
- Ensemble defenses missing - AIML155
- Randomization defense gaps - AIML156
- Input transformation missing - AIML157
- Detection mechanism missing - AIML158
- Rejection option missing - AIML159
- Robustness testing absent - AIML160
- Model inversion attack vectors
- Adversarial input acceptance
- Model extraction vulnerabilities
- AI bias detection in code
- Insecure model serialization (PyTorch, TensorFlow)
- Missing input validation for ML models
- GPU memory leakage
- Federated learning privacy risks

Phase 3.3: Reinforcement Learning Security (AIML351-370):
- Q-learning poisoning - AIML351
- DQN replay buffer manipulation - AIML352
- Policy gradient attacks - AIML353
- Actor-critic tampering - AIML354
- PPO injection - AIML355
- A3C risks - AIML356
- DDPG attacks - AIML357
- SAC vulnerabilities - AIML358
- TD3 manipulation - AIML359
- TRPO bypass - AIML360
- Reward function poisoning - AIML361
- Reward shaping attacks - AIML362
- OpenAI Gym environment injection - AIML363
- Gymnasium API manipulation - AIML364
- Custom environment backdoors - AIML365
- State space poisoning - AIML366
- Action space tampering - AIML367
- Observation function attacks - AIML368
- Reward function manipulation - AIML369
- Multi-agent RL vulnerabilities - AIML370

Phase 3.4: Specialized ML Libraries (AIML371-380):
- Optuna trial manipulation - AIML371
- Ray Tune search space poisoning - AIML372
- Hyperopt objective injection - AIML373
- Auto-sklearn pipeline tampering - AIML374
- AutoKeras architecture injection - AIML375
- PyTorch Geometric injection - AIML376
- DGL manipulation - AIML377
- Graph structure poisoning - AIML378
- Node feature injection - AIML379
- Message passing attacks - AIML380
- Untrusted notebook execution - AIML381
- Output cell code injection - AIML382
- Matplotlib backend exploitation - AIML383
- IPython magic command injection - AIML384
- Kernel manipulation attacks - AIML385
- Widget state poisoning - AIML386
- Display object injection - AIML387
- Markdown cell XSS - AIML388
- LaTeX injection in outputs - AIML389
- SVG code execution - AIML390
- HTML display risks - AIML391
- JavaScript execution in notebooks - AIML392
- nbviewer security gaps - AIML393
- Binder configuration injection - AIML394
- JupyterHub authentication bypass - AIML395
- Notebook sharing credential leakage - AIML396
- Git integration risks (.ipynb in repos) - AIML397
- Colab notebook sharing vulnerabilities - AIML398
- Kaggle notebook injection - AIML399
- Paperspace Gradient risks - AIML400
- Model checkpoints in notebooks - AIML401
- Credentials in notebook metadata - AIML402
- Large model loading (DoS) - AIML403
- GPU memory exhaustion in notebooks - AIML404
- Dataset downloading vulnerabilities - AIML405
- Hugging Face Datasets poisoning - AIML406
- Kaggle dataset injection - AIML407
- TensorFlow Datasets manipulation - AIML408
- Torchvision datasets tampering - AIML409
- Papers with Code dataset risks - AIML410
- Google Dataset Search injection - AIML411
- UCI ML Repository vulnerabilities - AIML412
- Common Crawl poisoning - AIML413
- ImageNet distribution attacks - AIML414
- Custom dataset loader risks - AIML415
- PyTorch DataLoader injection - AIML416
- TensorFlow tf.data manipulation - AIML417
- Pandas data loading risks - AIML418
- NumPy data loading vulnerabilities - AIML419
- H5PY file injection - AIML420
- Zarr array poisoning - AIML421
- Parquet file tampering - AIML422
- Arrow data format risks - AIML423
- Data augmentation library vulnerabilities - AIML424
- Albumentations injection - AIML425
- DVC (Data Version Control) injection - AIML426
- LakeFS tampering - AIML427
- Delta Lake poisoning - AIML428
- Iceberg table manipulation - AIML429
- Hudi data versioning risks - AIML430
- MLflow Model Registry injection - AIML431
- Weights & Biases artifact tampering - AIML432
- Neptune.ai model poisoning - AIML433
- Comet.ml registry manipulation - AIML434
- AWS SageMaker model registry risks - AIML435
- Azure ML model registry gaps - AIML436
- Google Vertex AI model registry - AIML437
- Custom registry vulnerabilities - AIML438
- Model metadata injection - AIML439
- Model versioning bypass - AIML440
- Model lineage tampering - AIML441
- Model approval workflow gaps - AIML442
- Docker image model injection - AIML443
- Model as a service (MaaS) risks - AIML444
- Model marketplace vulnerabilities - AIML445
- Model license bypass - AIML446
- Model watermark removal - AIML447
- Model fingerprinting attacks - AIML448
- Model compression tampering - AIML449
- Model conversion vulnerabilities - AIML450
- AWS SageMaker notebook injection - AIML451
- Azure ML workspace tampering - AIML452
- Google Vertex AI pipeline manipulation - AIML453
- Databricks ML runtime risks - AIML454
- Snowflake ML vulnerabilities - AIML455
- BigQuery ML injection - AIML456
- Redshift ML tampering - AIML457
- Lambda ML inference risks - AIML458
- Cloud Functions ML serving gaps - AIML459
- Serverless ML vulnerabilities - AIML460
- Prompt leaking attacks - AIML461
- Training data extraction - AIML462
- Memorization exploitation - AIML463
- Copyright infringement detection - AIML464
- Generated code security (Copilot-style) - AIML465
- Jailbreak detection - AIML466
- Toxicity generation risks - AIML467
- Bias amplification detection - AIML468
- Hallucination exploitation - AIML469
- Output filtering bypass - AIML470
- Stable Diffusion prompt injection - AIML471
- DALL-E manipulation - AIML472
- Midjourney prompt engineering - AIML473
- GAN mode collapse exploitation - AIML474
- VAE latent space manipulation - AIML475
- Diffusion model backdoors - AIML476
- Video generation injection (Runway, Gen-2) - AIML477
- 3D generation vulnerabilities (Point-E, Shap-E) - AIML478
- Music generation risks (Jukebox, MusicLM) - AIML479
- Audio generation injection (AudioLM, Whisper) - AIML480
- CLIP contrastive learning poisoning - AIML481
- ALIGN multimodal injection - AIML482
- Flamingo few-shot manipulation - AIML483
- BLIP-2 query injection - AIML484
- GPT-4 Vision prompt attacks - AIML485
- LLaVA instruction tuning risks - AIML486
- MiniGPT-4 alignment bypass - AIML487
- CoCa caption poisoning - AIML488
- Audio-text alignment poisoning - AIML489
- Video-text retrieval manipulation - AIML490
- Speech-to-text injection - AIML491
- Text-to-speech vulnerabilities - AIML492
- Visual grounding attacks - AIML493
- Embodied AI risks (robotics) - AIML494
- Sensor fusion manipulation - AIML495
- Federated averaging poisoning - AIML496
- Client selection manipulation - AIML497
- Model aggregation attacks - AIML498
- Byzantine client detection bypass - AIML499
- Privacy budget exploitation - AIML500
- Differential privacy bypass - AIML501
- Secure aggregation vulnerabilities - AIML502
- Homomorphic encryption weaknesses - AIML503
- Trusted execution environment gaps - AIML504
- Split learning injection - AIML505
- Differential privacy parameter manipulation - AIML506
- SMPC (Secure Multi-Party Computation) risks - AIML507
- Trusted execution environment bypass - AIML508
- Encrypted inference vulnerabilities - AIML509
- Zero-knowledge proof gaps - AIML510

Total Security Checks: 500 AI/ML rules (v0.8.4 - Phase 5.3 Complete: Federated & Privacy-Preserving ML) ðŸŽ¯

References:
- OWASP LLM Top 10 | https://owasp.org/www-project-top-10-for-large-language-model-applications/ | Critical
- NIST AI Risk Management | https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf | High
- CWE-94 (Code Injection) | https://cwe.mitre.org/data/definitions/94.html | Critical
- CWE-502 (Deserialization of Untrusted Data) | https://cwe.mitre.org/data/definitions/502.html | Critical
- CWE-400 (Resource Exhaustion) | https://cwe.mitre.org/data/definitions/400.html | High
- CWE-327 (Broken Cryptographic Algorithm) | https://cwe.mitre.org/data/definitions/327.html | High
"""

import ast
import re
from pathlib import Path
from typing import List, Set, Tuple

from pyguard.lib.core import FileOperations, PyGuardLogger
from pyguard.lib.fix_safety import FixSafetyClassifier
from pyguard.lib.rule_engine import (
    FixApplicability,
    Rule,
    RuleCategory,
    RuleSeverity,
    RuleViolation,
)


class AIMLSecurityVisitor(ast.NodeVisitor):
    """AST visitor for detecting AI/ML security vulnerabilities."""

    def __init__(self, file_path: Path, code: str):
        self.file_path = file_path
        self.code = code
        self.lines = code.splitlines()
        self.violations: List[RuleViolation] = []
        self.has_llm_framework = False
        self.has_ml_framework = False
        self.has_pytorch = False
        self.has_tensorflow = False
        self.has_transformers = False
        self.has_snowflake = False
        self.model_loads: Set[str] = set()

    def visit_ImportFrom(self, node: ast.ImportFrom) -> None:
        """Track AI/ML framework imports."""
        if node.module:
            # LLM frameworks
            if any(x in node.module for x in ["openai", "langchain", "llama", "anthropic"]):
                self.has_llm_framework = True
            # ML frameworks
            elif "torch" in node.module or "pytorch" in node.module:
                self.has_pytorch = True
            elif "tensorflow" in node.module or "tf" in node.module:
                self.has_tensorflow = True
            elif "transformers" in node.module or "huggingface" in node.module:
                self.has_transformers = True
            elif any(x in node.module for x in ["sklearn", "keras", "jax"]):
                self.has_ml_framework = True
            # Cloud platforms
            elif "snowflake" in node.module:
                self.has_snowflake = True
        self.generic_visit(node)

    def visit_Import(self, node: ast.Import) -> None:
        """Track AI/ML framework imports (import statements)."""
        for alias in node.names:
            if any(x in alias.name for x in ["openai", "langchain", "llama", "anthropic"]):
                self.has_llm_framework = True
            elif "torch" in alias.name or "pytorch" in alias.name:
                self.has_pytorch = True
            elif "tensorflow" in alias.name:
                self.has_tensorflow = True
            elif "transformers" in alias.name:
                self.has_transformers = True
            elif any(x in alias.name for x in ["sklearn", "keras", "jax"]):
                self.has_ml_framework = True
        self.generic_visit(node)

    def visit_Call(self, node: ast.Call) -> None:
        """Check for AI/ML security vulnerabilities in function calls."""
        # AIML001: Prompt injection
        self._check_prompt_injection(node)
        
        # AIML011: System prompt override (delimiter injection)
        self._check_system_prompt_override(node)
        
        # AIML012: Unicode/homoglyph injection
        self._check_unicode_injection(node)
        
        # AIML013: Role confusion attacks (DAN mode)
        self._check_role_confusion(node)
        
        # AIML014: Instruction concatenation bypasses
        self._check_instruction_concatenation(node)
        
        # AIML015: Multi-language prompt injection
        self._check_multilanguage_injection(node)
        
        # AIML016: Markdown injection in prompts
        self._check_markdown_injection(node)
        
        # AIML017: XML/JSON payload injection
        self._check_payload_injection(node)
        
        # AIML018: SQL-style comment injection
        self._check_sql_comment_injection(node)
        
        # AIML019: Escape sequence injection
        self._check_escape_sequence_injection(node)
        
        # AIML020: Token stuffing attacks
        self._check_token_stuffing(node)
        
        # AIML021: Recursive prompt injection
        self._check_recursive_prompt_injection(node)
        
        # AIML022: Base64 encoded injection
        self._check_base64_injection(node)
        
        # AIML023: ROT13/Caesar cipher obfuscation
        self._check_rot13_obfuscation(node)
        
        # AIML024: Invisible character injection (zero-width spaces)
        self._check_invisible_char_injection(node)
        
        # AIML025: Right-to-left override attacks (Unicode bidi)
        self._check_bidi_override(node)
        
        # AIML026: Prompt template literal injection
        self._check_template_literal_injection(node)
        
        # AIML027: F-string injection in prompts
        self._check_fstring_injection(node)
        
        # AIML028: Variable substitution attacks
        self._check_variable_substitution(node)
        
        # AIML029: Context window overflow
        self._check_context_window_overflow(node)
        
        # AIML030: Attention mechanism manipulation
        self._check_attention_manipulation(node)
        
        # AIML031: URL-based injection (fetched web content)
        self._check_url_based_injection(node)
        
        # AIML032: Document poisoning (PDF, DOCX injection)
        self._check_document_poisoning(node)
        
        # AIML033: Image-based prompt injection (OCR manipulation)
        self._check_image_injection(node)
        
        # AIML034: API response injection (3rd party data)
        self._check_api_response_injection(node)
        
        # AIML035: Database content injection
        self._check_database_injection(node)
        
        # AIML036: File upload injection vectors
        self._check_file_upload_injection(node)
        
        # AIML037: Email content injection
        self._check_email_injection(node)
        
        # AIML038: Social media scraping injection
        self._check_social_scraping_injection(node)
        
        # AIML039: RAG poisoning (retrieval augmented generation)
        self._check_rag_poisoning(node)
        
        # AIML040: Vector database injection
        self._check_vector_db_injection(node)
        
        # AIML041: Knowledge base tampering
        self._check_knowledge_base_tampering(node)
        
        # AIML042: Citation manipulation
        self._check_citation_manipulation(node)
        
        # AIML043: Search result poisoning
        self._check_search_poisoning(node)
        
        # AIML044: User profile injection
        self._check_user_profile_injection(node)
        
        # AIML045: Conversation history injection
        self._check_conversation_history_injection(node)
        
        # Phase 1.1.3: LLM API Security (15 checks)
        # AIML046: Missing rate limiting on LLM API calls
        self._check_missing_rate_limiting(node)
        
        # AIML047: Unvalidated temperature/top_p parameters
        self._check_unvalidated_llm_parameters(node)
        
        # AIML048: Max_tokens manipulation (DoS)
        self._check_max_tokens_manipulation(node)
        
        # AIML049: Streaming response injection
        self._check_streaming_response_injection(node)
        
        # AIML050: Function calling injection
        self._check_function_calling_injection(node)
        
        # AIML051: Tool use parameter tampering
        self._check_tool_use_tampering(node)
        
        # AIML052: System message manipulation via API
        self._check_system_message_manipulation(node)
        
        # AIML053: Model selection bypass
        self._check_model_selection_bypass(node)
        
        # AIML054: API key exposure in client code
        self._check_api_key_exposure(node)
        
        # AIML055: Hardcoded model names (version lock-in)
        self._check_hardcoded_model_names(node)
        
        # AIML056: Missing timeout configurations
        self._check_missing_timeout(node)
        
        # AIML057: Unhandled API errors (info disclosure)
        self._check_unhandled_api_errors(node)
        
        # AIML058: Token counting bypass
        self._check_token_counting_bypass(node)
        
        # AIML059: Cost overflow attacks
        self._check_cost_overflow(node)
        
        # AIML060: Multi-turn conversation state injection
        self._check_conversation_state_injection(node)
        
        # Phase 1.1.4: Output Validation & Filtering (10 checks)
        # AIML061: Missing output sanitization
        self._check_missing_output_sanitization(node)
        
        # AIML062: Code execution in LLM responses
        self._check_code_execution_in_response(node)
        
        # AIML063: SQL injection via generated queries
        self._check_sql_injection_in_generated(node)
        
        # AIML064: XSS via generated HTML
        self._check_xss_in_generated_html(node)
        
        # AIML065: Command injection via generated shell scripts
        self._check_command_injection_in_generated(node)
        
        # AIML066: Path traversal in generated file paths
        self._check_path_traversal_in_generated(node)
        
        # AIML067: Arbitrary file access via generated code
        self._check_arbitrary_file_access_in_generated(node)
        
        # AIML068: Sensitive data leakage in responses
        self._check_sensitive_data_leakage(node)
        
        # AIML069: PII disclosure from training data
        self._check_pii_disclosure(node)
        
        # AIML070: Copyright violation risks (memorized content)
        self._check_copyright_violation_risk(node)
        
        # Phase 1.2: Model Serialization & Loading (40 checks)
        # Phase 1.2.1: PyTorch Model Security (15 checks - AIML071-AIML085)
        # AIML071: torch.load() without weights_only=True
        self._check_torch_load_unsafe(node)
        
        # AIML072: Unsafe pickle in torch.save/load
        self._check_torch_pickle_unsafe(node)
        
        # AIML073: Missing model integrity verification
        self._check_missing_model_integrity(node)
        
        # AIML074: Untrusted model URL loading
        self._check_untrusted_model_url(node)
        
        # AIML075: Model poisoning in state_dict
        self._check_state_dict_poisoning(node)
        
        # AIML076: Custom layer/module injection
        self._check_custom_module_injection(node)
        
        # AIML077: Unsafe torch.jit.load()
        self._check_torch_jit_unsafe(node)
        
        # AIML078: TorchScript deserialization risks
        self._check_torchscript_deserialization(node)
        
        # AIML079: ONNX model tampering
        self._check_onnx_tampering(node)
        
        # AIML080: Model metadata injection
        self._check_model_metadata_injection(node)
        
        # AIML081: Missing GPU memory limits
        self._check_missing_gpu_limits(node)
        
        # AIML082: Tensor size attacks
        self._check_tensor_size_attacks(node)
        
        # AIML083: Quantization vulnerabilities
        self._check_quantization_vulnerabilities(node)
        
        # AIML084: Mixed precision attacks
        self._check_mixed_precision_attacks(node)
        
        # AIML085: Model zoo trust verification
        self._check_model_zoo_trust(node)
        
        # Phase 1.2.2: TensorFlow/Keras Security (15 checks - AIML086-AIML100)
        # AIML086: SavedModel arbitrary code execution
        self._check_savedmodel_unsafe(node)
        
        # AIML087: HDF5 deserialization attacks
        self._check_hdf5_deserialization(node)
        
        # AIML088: Custom object injection in model.load
        self._check_keras_custom_object_injection(node)
        
        # AIML089: TensorFlow Hub model trust
        self._check_tf_hub_trust(node)
        
        # AIML090: Graph execution injection
        self._check_graph_execution_injection(node)
        
        # AIML091: Checkpoint poisoning
        self._check_checkpoint_poisoning(node)
        
        # AIML092: Keras Lambda layer code injection
        self._check_keras_lambda_injection(node)
        
        # AIML093: Custom metric/loss function tampering
        self._check_custom_metric_tampering(node)
        
        # AIML094: TF Lite model manipulation
        self._check_tflite_manipulation(node)
        
        # AIML095: TensorBoard log injection
        self._check_tensorboard_injection(node)
        
        # AIML096: Model serving vulnerabilities (TF Serving)
        self._check_tf_serving_vulnerabilities(node)
        
        # AIML097: GraphDef manipulation
        self._check_graphdef_manipulation(node)
        
        # AIML098: Operation injection attacks
        self._check_operation_injection(node)
        
        # AIML099: Resource exhaustion via model architecture
        self._check_resource_exhaustion_model(node)
        
        # AIML100: TFRecord poisoning
        self._check_tfrecord_poisoning(node)
        
        # Phase 1.2.3: Hugging Face & Transformers (10 checks - AIML101-AIML110)
        # AIML101: from_pretrained() trust issues
        self._check_from_pretrained_trust(node)
        
        # AIML102: Model card credential leakage
        self._check_model_card_credentials(node)
        
        # AIML103: Tokenizer vulnerabilities
        self._check_tokenizer_vulnerabilities(node)
        
        # AIML104: Pipeline injection attacks
        self._check_pipeline_injection(node)
        
        # AIML105: Dataset poisoning (Hugging Face Datasets)
        self._check_hf_dataset_poisoning(node)
        
        # AIML106: Missing model signature verification
        self._check_missing_model_signature(node)
        
        # AIML107: Arbitrary file loading in model config
        self._check_arbitrary_file_in_config(node)
        
        # AIML108: Space app injection (Gradio/Streamlit)
        self._check_space_app_injection(node)
        
        # AIML109: Model repository tampering
        self._check_model_repo_tampering(node)
        
        # AIML110: Private model access control
        self._check_private_model_access(node)
        
        # Phase 1.3: Training & Fine-Tuning Security (30 checks)
        # Phase 1.3.1: Training Data Security (12 checks - AIML111-AIML122)
        # AIML111: Unvalidated training data sources
        self._check_unvalidated_training_data(node)
        
        # AIML112: Missing data sanitization
        self._check_missing_data_sanitization(node)
        
        # AIML113: PII leakage in training datasets
        self._check_pii_in_training_data(node)
        
        # AIML114: Copyright-infringing data inclusion
        self._check_copyright_infringing_data(node)
        
        # AIML115: Data poisoning detection (label flipping)
        self._check_label_flipping_detection(node)
        
        # AIML116: Backdoor injection in datasets
        self._check_backdoor_in_dataset(node)
        
        # AIML117: Trigger pattern insertion
        self._check_trigger_pattern_insertion(node)
        
        # AIML118: Data augmentation attacks
        self._check_data_augmentation_attacks(node)
        
        # AIML119: Synthetic data vulnerabilities
        self._check_synthetic_data_vulnerabilities(node)
        
        # AIML120: Web scraping data risks
        self._check_web_scraping_data_risks(node)
        
        # AIML121: User-generated content risks
        self._check_user_generated_content_risks(node)
        
        # AIML122: Missing data provenance tracking
        self._check_missing_data_provenance(node)
        
        # Phase 1.3.2: Training Process Security (10 checks - AIML123-AIML132)
        # AIML123: Gradient manipulation attacks
        self._check_gradient_manipulation(node)
        
        # AIML124: Learning rate manipulation
        self._check_learning_rate_manipulation(node)
        
        # AIML125: Optimizer state poisoning
        self._check_optimizer_state_poisoning(node)
        
        # AIML126: Checkpoint tampering during training
        self._check_checkpoint_tampering_training(node)
        
        # AIML127: Early stopping bypass
        self._check_early_stopping_bypass(node)
        
        # AIML128: Validation set poisoning
        self._check_validation_set_poisoning(node)
        
        # AIML129: TensorBoard logging injection
        self._check_tensorboard_logging_injection(node)
        
        # AIML130: Experiment tracking manipulation
        self._check_experiment_tracking_manipulation(node)
        
        # AIML131: Distributed training node compromise
        self._check_distributed_training_node_compromise(node)
        
        # AIML132: Parameter server vulnerabilities
        self._check_parameter_server_vulnerabilities(node)
        
        # Phase 1.3.3: Fine-Tuning Risks (8 checks - AIML133-AIML140)
        # AIML133: Base model poisoning
        self._check_base_model_poisoning(node)
        
        # AIML134: Fine-tuning data injection
        self._check_fine_tuning_data_injection(node)
        
        # AIML135: Catastrophic forgetting exploitation
        self._check_catastrophic_forgetting_exploitation(node)
        
        # AIML136: PEFT attacks
        self._check_peft_attacks(node)
        
        # AIML137: LoRA poisoning
        self._check_lora_poisoning(node)
        
        # AIML138: Adapter injection
        self._check_adapter_injection(node)
        
        # AIML139: Prompt tuning manipulation
        self._check_prompt_tuning_manipulation(node)
        
        # AIML140: Instruction fine-tuning risks
        self._check_instruction_fine_tuning_risks(node)
        
        # Phase 1.4: Adversarial ML & Model Robustness (20 checks)
        # Phase 1.4.1: Adversarial Input Detection (10 checks - AIML141-AIML150)
        # AIML141: Missing input adversarial defense
        self._check_missing_adversarial_defense(node)
        
        # AIML142: No FGSM protection
        self._check_no_fgsm_protection(node)
        
        # AIML143: PGD vulnerability
        self._check_pgd_vulnerability(node)
        
        # AIML144: C&W attack surface
        self._check_cw_attack_surface(node)
        
        # AIML145: DeepFool susceptibility
        self._check_deepfool_susceptibility(node)
        
        # AIML146: Universal adversarial perturbations
        self._check_universal_adversarial_perturbations(node)
        
        # AIML147: Black-box attack vulnerability
        self._check_black_box_attack_vulnerability(node)
        
        # AIML148: Transfer attack risks
        self._check_transfer_attack_risks(node)
        
        # AIML149: Physical adversarial examples
        self._check_physical_adversarial_examples(node)
        
        # AIML150: Adversarial patch detection missing
        self._check_adversarial_patch_detection_missing(node)
        
        # Phase 1.4.2: Model Robustness (10 checks - AIML151-AIML160)
        # AIML151: Missing adversarial training
        self._check_missing_adversarial_training(node)
        
        # AIML152: No certified defenses
        self._check_no_certified_defenses(node)
        
        # AIML153: Input gradient masking
        self._check_input_gradient_masking(node)
        
        # AIML154: Defensive distillation gaps
        self._check_defensive_distillation_gaps(node)
        
        # AIML155: Ensemble defenses missing
        self._check_ensemble_defenses_missing(node)
        
        # AIML156: Randomization defense gaps
        self._check_randomization_defense_gaps(node)
        
        # AIML157: Input transformation missing
        self._check_input_transformation_missing(node)
        
        # AIML158: Detection mechanism missing
        self._check_detection_mechanism_missing(node)
        
        # AIML159: Rejection option missing
        self._check_rejection_option_missing(node)
        
        # AIML160: Robustness testing absent
        self._check_robustness_testing_absent(node)
        
        # Phase 2.1: Feature Engineering & Preprocessing (30 checks)
        # Phase 2.1.1: Data Preprocessing Security (15 checks - AIML161-AIML175)
        # AIML161: Missing input validation in preprocessing
        self._check_missing_preprocessing_validation(node)
        
        # AIML162: Normalization bypass attacks
        self._check_normalization_bypass(node)
        
        # AIML163: Feature scaling manipulation
        self._check_feature_scaling_manipulation(node)
        
        # AIML164: Missing value injection
        self._check_missing_value_injection(node)
        
        # AIML165: Encoding injection
        self._check_encoding_injection(node)
        
        # AIML166: Feature extraction vulnerabilities
        self._check_feature_extraction_vulnerabilities(node)
        
        # AIML167: Dimensionality reduction poisoning
        self._check_dimensionality_reduction_poisoning(node)
        
        # AIML168: Feature selection manipulation
        self._check_feature_selection_manipulation(node)
        
        # AIML169: Missing outlier detection
        self._check_missing_outlier_detection(node)
        
        # AIML170: Data leakage in preprocessing
        self._check_data_leakage_preprocessing(node)
        
        # AIML171: Test/train contamination
        self._check_test_train_contamination(node)
        
        # AIML172: Feature store injection
        self._check_feature_store_injection(node)
        
        # AIML173: Pipeline versioning gaps
        self._check_pipeline_versioning_gaps(node)
        
        # AIML174: Preprocessing state tampering
        self._check_preprocessing_state_tampering(node)
        
        # AIML175: Transformation order vulnerabilities
        self._check_transformation_order_vulnerabilities(node)
        
        # Phase 2.1.2: Feature Store Security (15 checks - AIML176-AIML190)
        # AIML176: Feast feature store injection
        self._check_feast_feature_store_injection(node)
        
        # AIML177: Missing feature validation
        self._check_missing_feature_validation(node)
        
        # AIML178: Feature drift without detection
        self._check_feature_drift_without_detection(node)
        
        # AIML179: Feature serving vulnerabilities
        self._check_feature_serving_vulnerabilities(node)
        
        # AIML180: Offline/online feature skew
        self._check_offline_online_feature_skew(node)
        
        # AIML181: Feature metadata tampering
        self._check_feature_metadata_tampering(node)
        
        # AIML182: Feature lineage missing
        self._check_feature_lineage_missing(node)
        
        # AIML183: Access control gaps
        self._check_feature_access_control_gaps(node)
        
        # AIML184: Feature deletion/corruption
        self._check_feature_deletion_corruption(node)
        
        # AIML185: Version control weaknesses
        self._check_feature_version_control_weaknesses(node)
        
        # AIML186: Feature freshness attacks
        self._check_feature_freshness_attacks(node)
        
        # AIML187: Batch vs real-time inconsistencies
        self._check_batch_realtime_inconsistencies(node)
        
        # AIML188: Feature engineering code injection
        self._check_feature_engineering_code_injection(node)
        
        # AIML189: Schema evolution attacks
        self._check_schema_evolution_attacks(node)
        
        # AIML190: Feature importance manipulation
        self._check_feature_importance_manipulation(node)
        
        # Phase 2.2: Model Training Infrastructure (35 checks)
        # Phase 2.2.1: Distributed Training Security (15 checks - AIML191-AIML205)
        # AIML191: Parameter server vulnerabilities
        self._check_parameter_server_vulnerabilities(node)
        
        # AIML192: Gradient aggregation poisoning
        self._check_gradient_aggregation_poisoning(node)
        
        # AIML193: Byzantine worker attacks
        self._check_byzantine_worker_attacks(node)
        
        # AIML194: All-Reduce manipulation
        self._check_all_reduce_manipulation(node)
        
        # AIML195: Ring-All-Reduce injection
        self._check_ring_all_reduce_injection(node)
        
        # AIML196: Horovod security gaps
        self._check_horovod_security_gaps(node)
        
        # AIML197: DeepSpeed vulnerabilities
        self._check_deepspeed_vulnerabilities(node)
        
        # AIML198: FSDP (Fully Sharded Data Parallel) risks
        self._check_fsdp_risks(node)
        
        # AIML199: ZeRO optimizer state attacks
        self._check_zero_optimizer_state_attacks(node)
        
        # AIML200: Model parallel partition poisoning
        self._check_model_parallel_partition_poisoning(node)
        
        # AIML201: Pipeline parallel injection
        self._check_pipeline_parallel_injection(node)
        
        # AIML202: Tensor parallel tampering
        self._check_tensor_parallel_tampering(node)
        
        # AIML203: Mixed precision training risks
        self._check_mixed_precision_training_risks(node)
        
        # AIML204: Communication backend vulnerabilities
        self._check_communication_backend_vulnerabilities(node)
        
        # AIML205: Collective operation manipulation
        self._check_collective_operation_manipulation(node)
        
        # Phase 2.2.2: GPU & Accelerator Security (10 checks - AIML206-AIML215)
        # AIML206: GPU memory leakage
        self._check_gpu_memory_leakage_aiml206(node)
        
        # AIML207: CUDA kernel injection
        self._check_cuda_kernel_injection(node)
        
        # AIML208: ROCm vulnerabilities
        self._check_rocm_vulnerabilities(node)
        
        # AIML209: TPU security gaps
        self._check_tpu_security_gaps(node)
        
        # AIML210: NPU/IPU risks
        self._check_npu_ipu_risks(node)
        
        # AIML211: Multi-GPU synchronization attacks
        self._check_multi_gpu_synchronization_attacks(node)
        
        # AIML212: Device placement manipulation
        self._check_device_placement_manipulation(node)
        
        # AIML213: CUDA graph poisoning
        self._check_cuda_graph_poisoning(node)
        
        # AIML214: Kernel launch parameter tampering
        self._check_kernel_launch_parameter_tampering(node)
        
        # AIML215: GPU memory exhaustion attacks
        self._check_gpu_memory_exhaustion_attacks(node)
        
        # Phase 2.2.3: Experiment Tracking Security (10 checks - AIML216-AIML225)
        # AIML216: MLflow injection attacks
        self._check_mlflow_injection_attacks(node)
        
        # AIML217: Weights & Biases credential leakage
        self._check_wandb_credential_leakage(node)
        
        # AIML218: Comet.ml experiment tampering
        self._check_cometml_experiment_tampering(node)
        
        # AIML219: TensorBoard remote code execution
        self._check_tensorboard_rce(node)
        
        # AIML220: Neptune.ai model manipulation
        self._check_neptuneai_model_manipulation(node)
        
        # AIML221: Experiment metadata injection
        self._check_experiment_metadata_injection(node)
        
        # AIML222: Metric tampering
        self._check_metric_tampering(node)
        
        # AIML223: Artifact poisoning
        self._check_artifact_poisoning(node)
        
        # AIML224: Run comparison manipulation
        self._check_run_comparison_manipulation(node)
        
        # AIML225: Hyperparameter logging risks
        self._check_hyperparameter_logging_risks(node)
        
        # Phase 2.3: Model Deployment & Serving (35 checks)
        # Phase 2.3.1: Model Serving Vulnerabilities (15 checks - AIML226-AIML240)
        # AIML226: TorchServe vulnerabilities
        self._check_torchserve_vulnerabilities(node)
        
        # AIML227: TensorFlow Serving injection
        self._check_tensorflow_serving_injection(node)
        
        # AIML228: ONNX Runtime risks
        self._check_onnx_runtime_risks(node)
        
        # AIML229: Triton Inference Server gaps
        self._check_triton_inference_server_gaps(node)
        
        # AIML230: BentoML security issues
        self._check_bentoml_security_issues(node)
        
        # AIML231: Ray Serve vulnerabilities
        self._check_ray_serve_vulnerabilities(node)
        
        # AIML232: Seldon Core risks
        self._check_seldon_core_risks(node)
        
        # AIML233: KServe weaknesses
        self._check_kserve_weaknesses(node)
        
        # AIML234: Model batching attacks
        self._check_model_batching_attacks(node)
        
        # AIML235: Dynamic batching poisoning
        self._check_dynamic_batching_poisoning(node)
        
        # AIML236: Model versioning bypass
        self._check_model_versioning_bypass(node)
        
        # AIML237: A/B testing manipulation
        self._check_ab_testing_manipulation(node)
        
        # AIML238: Canary deployment risks
        self._check_canary_deployment_risks(node)
        
        # AIML239: Blue-green deployment gaps
        self._check_blue_green_deployment_gaps(node)
        
        # AIML240: Shadow deployment leakage
        self._check_shadow_deployment_leakage(node)
        
        # Phase 2.3.2: API & Endpoint Security (12 checks - AIML241-AIML252)
        # AIML241: Missing authentication on inference API
        self._check_missing_authentication_inference_api(node)
        
        # AIML242: Model endpoint enumeration
        self._check_model_endpoint_enumeration(node)
        
        # AIML243: Batch inference injection
        self._check_batch_inference_injection(node)
        
        # AIML244: Streaming inference attacks
        self._check_streaming_inference_attacks(node)
        
        # AIML245: Model cache poisoning
        self._check_model_cache_poisoning(node)
        
        # AIML246: Prediction logging risks (PII)
        self._check_prediction_logging_risks(node)
        
        # AIML247: Model warm-up vulnerabilities
        self._check_model_warmup_vulnerabilities(node)
        
        # AIML248: Health check information disclosure
        self._check_health_check_information_disclosure(node)
        
        # AIML249: Metrics endpoint exposure
        self._check_metrics_endpoint_exposure(node)
        
        # AIML250: Model metadata leakage
        self._check_model_metadata_leakage(node)
        
        # AIML251: Feature flag manipulation
        self._check_feature_flag_manipulation(node)
        
        # AIML252: Circuit breaker bypass
        self._check_circuit_breaker_bypass(node)
        
        # Phase 2.3.3: Edge & Mobile Deployment (8 checks - AIML253-AIML260)
        # AIML253: TFLite model tampering
        self._check_tflite_model_tampering(node)
        
        # AIML254: Core ML injection
        self._check_coreml_injection(node)
        
        # AIML255: ONNX mobile risks
        self._check_onnx_mobile_risks(node)
        
        # AIML256: Quantized model vulnerabilities
        self._check_quantized_model_vulnerabilities(node)
        
        # AIML257: Model pruning attacks
        self._check_model_pruning_attacks(node)
        
        # AIML258: Knowledge distillation risks
        self._check_knowledge_distillation_risks(node)
        
        # AIML259: On-device training weaknesses
        self._check_on_device_training_weaknesses(node)
        
        # AIML260: Federated learning gaps
        self._check_federated_learning_gaps(node)
        
        # Phase 2.4: Model Monitoring & Observability (20 checks)
        # Phase 2.4.1: Drift Detection Security (10 checks - AIML261-AIML270)
        # AIML261: Data drift detection bypass
        self._check_data_drift_detection_bypass(node)
        
        # AIML262: Concept drift manipulation
        self._check_concept_drift_manipulation(node)
        
        # AIML263: Model performance degradation hiding
        self._check_model_performance_degradation_hiding(node)
        
        # AIML264: Prediction distribution poisoning
        self._check_prediction_distribution_poisoning(node)
        
        # AIML265: Monitoring pipeline injection
        self._check_monitoring_pipeline_injection(node)
        
        # AIML266: Alert threshold manipulation
        self._check_alert_threshold_manipulation(node)
        
        # AIML267: Logging framework vulnerabilities
        self._check_logging_framework_vulnerabilities(node)
        
        # AIML268: Missing drift detection
        self._check_missing_drift_detection(node)
        
        # AIML269: Statistical test manipulation
        self._check_statistical_test_manipulation(node)
        
        # AIML270: Ground truth poisoning
        self._check_ground_truth_poisoning(node)
        
        # Phase 2.4.2: Explainability & Interpretability (10 checks - AIML271-AIML280)
        # AIML271: SHAP value manipulation
        self._check_shap_value_manipulation(node)
        
        # AIML272: LIME explanation poisoning
        self._check_lime_explanation_poisoning(node)
        
        # AIML273: Feature importance injection
        self._check_feature_importance_injection(node)
        
        # AIML274: Saliency map tampering
        self._check_saliency_map_tampering(node)
        
        # AIML275: Attention weight manipulation
        self._check_attention_weight_manipulation(node)
        
        # AIML276: Counterfactual explanation attacks
        self._check_counterfactual_explanation_attacks(node)
        
        # AIML277: Model card injection
        self._check_model_card_injection(node)
        
        # AIML278: Explanation dashboard vulnerabilities
        self._check_explanation_dashboard_vulnerabilities(node)
        
        # AIML279: Fairness metric manipulation
        self._check_fairness_metric_manipulation(node)
        
        # AIML280: Bias detection bypass
        self._check_bias_detection_bypass(node)
        
        # Phase 3: Specialized AI/ML Frameworks (100 checks - AIML281-AIML380)
        # Phase 3.1: Computer Vision Security (35 checks)
        # Phase 3.1.1: Image Processing Vulnerabilities (15 checks - AIML281-AIML295)
        # AIML281: OpenCV injection attacks
        self._check_opencv_injection_attacks(node)
        
        # AIML282: PIL/Pillow buffer overflows
        self._check_pillow_buffer_overflows(node)
        
        # AIML283: Image augmentation poisoning
        self._check_image_augmentation_poisoning(node)
        
        # AIML284: EXIF metadata injection
        self._check_exif_metadata_injection(node)
        
        # AIML285: Adversarial patch attacks
        self._check_adversarial_patch_attacks(node)
        
        # AIML286: Texture synthesis manipulation
        self._check_texture_synthesis_manipulation(node)
        
        # AIML287: Style transfer poisoning
        self._check_style_transfer_poisoning(node)
        
        # AIML288: Super-resolution attacks
        self._check_super_resolution_attacks(node)
        
        # AIML289: Image segmentation manipulation
        self._check_image_segmentation_manipulation(node)
        
        # AIML290: Object detection bypass
        self._check_object_detection_bypass(node)
        
        # AIML291: Facial recognition spoofing
        self._check_facial_recognition_spoofing(node)
        
        # AIML292: OCR injection attacks
        self._check_ocr_injection_attacks(node)
        
        # AIML293: Image captioning poisoning
        self._check_image_captioning_poisoning(node)
        
        # AIML294: Visual question answering attacks
        self._check_visual_question_answering_attacks(node)
        
        # AIML295: Video frame injection
        self._check_video_frame_injection(node)
        
        # Phase 3.1.2: Vision Transformers (10 checks - AIML296-AIML305)
        # AIML296: Patch embedding manipulation
        self._check_patch_embedding_manipulation(node)
        
        # AIML297: Position encoding injection
        self._check_position_encoding_injection(node)
        
        # AIML298: Attention mechanism attacks
        self._check_attention_mechanism_attacks(node)
        
        # AIML299: Vision-language model risks (CLIP)
        self._check_vision_language_model_risks(node)
        
        # AIML300: Diffusion model injection (Stable Diffusion)
        self._check_diffusion_model_injection(node)
        
        # AIML301: Text-to-image prompt injection
        self._check_text_to_image_prompt_injection(node)
        
        # AIML302: Image-to-image manipulation
        self._check_image_to_image_manipulation(node)
        
        # AIML303: Inpainting attacks
        self._check_inpainting_attacks(node)
        
        # AIML304: Outpainting vulnerabilities
        self._check_outpainting_vulnerabilities(node)
        
        # AIML305: Multimodal fusion risks
        self._check_multimodal_fusion_risks(node)
        
        # Phase 3.1.3: CNN & Architecture Security (10 checks - AIML306-AIML315)
        # AIML306: ResNet skip connection attacks
        self._check_resnet_skip_connection_attacks(node)
        
        # AIML307: DenseNet feature concatenation
        self._check_densenet_feature_concatenation(node)
        
        # AIML308: EfficientNet scaling manipulation
        self._check_efficientnet_scaling_manipulation(node)
        
        # AIML309: MobileNet depthwise convolution risks
        self._check_mobilenet_depthwise_convolution_risks(node)
        
        # AIML310: SqueezeNet fire module injection
        self._check_squeezenet_fire_module_injection(node)
        
        # AIML311: Neural architecture search poisoning
        self._check_neural_architecture_search_poisoning(node)
        
        # AIML312: Activation function vulnerabilities
        self._check_activation_function_vulnerabilities(node)
        
        # AIML313: Pooling layer manipulation
        self._check_pooling_layer_manipulation(node)
        
        # AIML314: Dropout bypass techniques
        self._check_dropout_bypass_techniques(node)
        
        # AIML315: Batch normalization attacks
        self._check_batch_normalization_attacks(node)
        
        # Phase 3.2: Natural Language Processing Security (35 checks - AIML316-AIML350)
        # Phase 3.2.1: Text Processing Security (15 checks - AIML316-AIML330)
        # AIML316: Tokenization injection
        self._check_tokenization_injection(node)
        
        # AIML317: Subword tokenization bypass
        self._check_subword_tokenization_bypass(node)
        
        # AIML318: BPE manipulation
        self._check_bpe_manipulation(node)
        
        # AIML319: WordPiece attack vectors
        self._check_wordpiece_attack_vectors(node)
        
        # AIML320: SentencePiece vulnerabilities
        self._check_sentencepiece_vulnerabilities(node)
        
        # AIML321: Text normalization bypass
        self._check_text_normalization_bypass(node)
        
        # AIML322: Stop word removal manipulation
        self._check_stopword_removal_manipulation(node)
        
        # AIML323: Stemming/lemmatization attacks
        self._check_stemming_lemmatization_attacks(node)
        
        # AIML324: Named entity recognition injection
        self._check_ner_injection(node)
        
        # AIML325: POS tagging manipulation
        self._check_pos_tagging_manipulation(node)
        
        # AIML326: Dependency parsing poisoning
        self._check_dependency_parsing_poisoning(node)
        
        # AIML327: Sentiment analysis bias
        self._check_sentiment_analysis_bias(node)
        
        # AIML328: Text classification backdoors
        self._check_text_classification_backdoors(node)
        
        # AIML329: Sequence labeling attacks
        self._check_sequence_labeling_attacks(node)
        
        # AIML330: Coreference resolution manipulation
        self._check_coreference_resolution_manipulation(node)
        
        # Phase 3.2.2: Transformer Architectures (12 checks - AIML331-AIML342)
        # AIML331: BERT fine-tuning injection
        self._check_bert_finetuning_injection(node)
        
        # AIML332: GPT prompt engineering attacks
        self._check_gpt_prompt_engineering_attacks(node)
        
        # AIML333: T5 encoder-decoder manipulation
        self._check_t5_encoder_decoder_manipulation(node)
        
        # AIML334: BART denoising poisoning
        self._check_bart_denoising_poisoning(node)
        
        # AIML335: RoBERTa masked language modeling
        self._check_roberta_masked_lm(node)
        
        # AIML336: ELECTRA discriminator/generator attacks
        self._check_electra_attacks(node)
        
        # AIML337: XLNet permutation language modeling
        self._check_xlnet_permutation_lm(node)
        
        # AIML338: ALBERT parameter sharing risks
        self._check_albert_parameter_sharing_risks(node)
        
        # AIML339: DistilBERT knowledge distillation
        self._check_distilbert_knowledge_distillation(node)
        
        # AIML340: DeBERTa disentangled attention
        self._check_deberta_disentangled_attention(node)
        
        # AIML341: Longformer sliding window attacks
        self._check_longformer_sliding_window_attacks(node)
        
        # AIML342: BigBird sparse attention manipulation
        self._check_bigbird_sparse_attention_manipulation(node)
        
        # Phase 3.2.3: Embeddings & Representations (8 checks - AIML343-AIML350)
        # AIML343: Word2Vec poisoning
        self._check_word2vec_poisoning(node)
        
        # AIML344: GloVe embedding manipulation
        self._check_glove_embedding_manipulation(node)
        
        # AIML345: FastText subword attacks
        self._check_fasttext_subword_attacks(node)
        
        # AIML346: ELMo contextualized embedding injection
        self._check_elmo_contextualized_embedding_injection(node)
        
        # AIML347: Sentence-BERT manipulation
        self._check_sentence_bert_manipulation(node)
        
        # AIML348: Universal Sentence Encoder risks
        self._check_universal_sentence_encoder_risks(node)
        
        # AIML349: Doc2Vec document poisoning
        self._check_doc2vec_document_poisoning(node)
        
        # AIML350: Graph embedding attacks
        self._check_graph_embedding_attacks(node)
        
        # Phase 3.3: Reinforcement Learning Security (20 checks - AIML351-370)
        
        # Phase 3.3.1: RL Algorithm Vulnerabilities (12 checks - AIML351-362)
        
        # AIML351: Q-learning poisoning
        self._check_q_learning_poisoning(node)
        
        # AIML352: DQN replay buffer manipulation
        self._check_dqn_replay_buffer_manipulation(node)
        
        # AIML353: Policy gradient attacks
        self._check_policy_gradient_attacks(node)
        
        # AIML354: Actor-critic tampering
        self._check_actor_critic_tampering(node)
        
        # AIML355: PPO (Proximal Policy Optimization) injection
        self._check_ppo_injection(node)
        
        # AIML356: A3C (Asynchronous Actor-Critic) risks
        self._check_a3c_risks(node)
        
        # AIML357: DDPG (Deep Deterministic Policy Gradient) attacks
        self._check_ddpg_attacks(node)
        
        # AIML358: SAC (Soft Actor-Critic) vulnerabilities
        self._check_sac_vulnerabilities(node)
        
        # AIML359: TD3 (Twin Delayed DDPG) manipulation
        self._check_td3_manipulation(node)
        
        # AIML360: TRPO (Trust Region Policy Optimization) bypass
        self._check_trpo_bypass(node)
        
        # AIML361: Reward function poisoning
        self._check_reward_function_poisoning(node)
        
        # AIML362: Reward shaping attacks
        self._check_reward_shaping_attacks(node)
        
        # Phase 3.3.2: RL Environment Security (8 checks - AIML363-370)
        
        # AIML363: OpenAI Gym environment injection
        self._check_gym_environment_injection(node)
        
        # AIML364: Gymnasium API manipulation
        self._check_gymnasium_api_manipulation(node)
        
        # AIML365: Custom environment backdoors
        self._check_custom_environment_backdoors(node)
        
        # AIML366: State space poisoning
        self._check_state_space_poisoning(node)
        
        # AIML367: Action space tampering
        self._check_action_space_tampering(node)
        
        # AIML368: Observation function attacks
        self._check_observation_function_attacks(node)
        
        # AIML369: Reward function manipulation
        self._check_reward_function_manipulation(node)
        
        # AIML370: Multi-agent RL vulnerabilities
        self._check_multiagent_rl_vulnerabilities(node)
        
        # Phase 3.4: Specialized ML Libraries (10 checks - AIML371-380)
        
        # Phase 3.4.1: AutoML & Hyperparameter Tuning (5 checks - AIML371-375)
        
        # AIML371: Optuna trial manipulation
        self._check_optuna_trial_manipulation(node)
        
        # AIML372: Ray Tune search space poisoning
        self._check_ray_tune_search_space_poisoning(node)
        
        # AIML373: Hyperopt objective injection
        self._check_hyperopt_objective_injection(node)
        
        # AIML374: Auto-sklearn pipeline tampering
        self._check_autosklearn_pipeline_tampering(node)
        
        # AIML375: AutoKeras architecture injection
        self._check_autokeras_architecture_injection(node)
        
        # Phase 3.4.2: Graph Neural Networks (5 checks - AIML376-380)
        
        # AIML376: PyTorch Geometric injection
        self._check_pytorch_geometric_injection(node)
        
        # AIML377: DGL (Deep Graph Library) manipulation
        self._check_dgl_manipulation(node)
        
        # AIML378: Graph structure poisoning
        self._check_graph_structure_poisoning(node)
        
        # AIML379: Node feature injection
        self._check_node_feature_injection(node)
        
        # AIML380: Message passing attacks
        self._check_message_passing_attacks(node)
        
        # Phase 4: AI/ML Supply Chain & Infrastructure Security (80 checks - AIML381-460)
        # Phase 4.1: Jupyter & Notebook Security (25 checks - AIML381-405)
        
        # Phase 4.1.1: Notebook Execution Risks (12 checks - AIML381-392)
        
        # AIML381: Untrusted notebook execution
        self._check_untrusted_notebook_execution(node)
        
        # AIML382: Output cell code injection
        self._check_output_cell_code_injection(node)
        
        # AIML383: Matplotlib backend exploitation
        self._check_matplotlib_backend_exploitation(node)
        
        # AIML384: IPython magic command injection
        self._check_ipython_magic_injection(node)
        
        # AIML385: Kernel manipulation attacks
        self._check_kernel_manipulation_attacks(node)
        
        # AIML386: Widget state poisoning
        self._check_widget_state_poisoning(node)
        
        # AIML387: Display object injection
        self._check_display_object_injection(node)
        
        # AIML388: Markdown cell XSS
        self._check_markdown_cell_xss(node)
        
        # AIML389: LaTeX injection in outputs
        self._check_latex_injection(node)
        
        # AIML390: SVG code execution
        self._check_svg_code_execution(node)
        
        # AIML391: HTML display risks
        self._check_html_display_risks(node)
        
        # AIML392: JavaScript execution in notebooks
        self._check_javascript_execution_notebooks(node)
        
        # Phase 4.1.2: Collaboration & Sharing Risks (8 checks - AIML393-400)
        
        # AIML393: nbviewer security gaps
        self._check_nbviewer_security_gaps(node)
        
        # AIML394: Binder configuration injection
        self._check_binder_config_injection(node)
        
        # AIML395: JupyterHub authentication bypass
        self._check_jupyterhub_auth_bypass(node)
        
        # AIML396: Notebook sharing credential leakage
        self._check_notebook_credential_leakage(node)
        
        # AIML397: Git integration risks
        self._check_git_ipynb_risks(node)
        
        # AIML398: Colab notebook sharing vulnerabilities
        self._check_colab_sharing_vulnerabilities(node)
        
        # AIML399: Kaggle notebook injection
        self._check_kaggle_notebook_injection(node)
        
        # AIML400: Paperspace Gradient risks
        self._check_paperspace_gradient_risks(node)
        
        # Phase 4.1.3: ML-Specific Notebook Risks (5 checks - AIML401-405)
        
        # AIML401: Model checkpoints in notebooks
        self._check_model_checkpoints_notebooks(node)
        
        # AIML402: Credentials in notebook metadata
        self._check_credentials_notebook_metadata(node)
        
        # AIML403: Large model loading (DoS)
        self._check_large_model_loading_dos(node)
        
        # AIML404: GPU memory exhaustion in notebooks
        self._check_gpu_memory_exhaustion_notebooks(node)
        
        # AIML405: Dataset downloading vulnerabilities
        self._check_dataset_download_vulnerabilities(node)
        
        # Phase 4.2: Dataset & Data Pipeline Security (25 checks - AIML406-430)
        
        # Phase 4.2.1: Dataset Repositories (10 checks - AIML406-415)
        
        # AIML406: Hugging Face Datasets poisoning
        self._check_huggingface_datasets_poisoning(node)
        
        # AIML407: Kaggle dataset injection
        self._check_kaggle_dataset_injection(node)
        
        # AIML408: TensorFlow Datasets manipulation
        self._check_tensorflow_datasets_manipulation(node)
        
        # AIML409: Torchvision datasets tampering
        self._check_torchvision_datasets_tampering(node)
        
        # AIML410: Papers with Code dataset risks
        self._check_paperswithcode_dataset_risks(node)
        
        # AIML411: Google Dataset Search injection
        self._check_google_dataset_search_injection(node)
        
        # AIML412: UCI ML Repository vulnerabilities
        self._check_uci_ml_repository_vulnerabilities(node)
        
        # AIML413: Common Crawl poisoning
        self._check_common_crawl_poisoning(node)
        
        # AIML414: ImageNet distribution attacks
        self._check_imagenet_distribution_attacks(node)
        
        # AIML415: Custom dataset loader risks
        self._check_custom_dataset_loader_risks(node)
        
        # Phase 4.2.2: Data Loading & Preprocessing (10 checks - AIML416-425)
        
        # AIML416: PyTorch DataLoader injection
        self._check_pytorch_dataloader_injection(node)
        
        # AIML417: TensorFlow tf.data manipulation
        self._check_tensorflow_tfdata_manipulation(node)
        
        # AIML418: Pandas data loading risks
        self._check_pandas_data_loading_risks(node)
        
        # AIML419: NumPy data loading vulnerabilities
        self._check_numpy_data_loading_vulnerabilities(node)
        
        # AIML420: H5PY file injection
        self._check_h5py_file_injection(node)
        
        # AIML421: Zarr array poisoning
        self._check_zarr_array_poisoning(node)
        
        # AIML422: Parquet file tampering
        self._check_parquet_file_tampering(node)
        
        # AIML423: Arrow data format risks
        self._check_arrow_data_format_risks(node)
        
        # AIML424: Data augmentation library vulnerabilities
        self._check_augmentation_library_vulnerabilities(node)
        
        # AIML425: Albumentations injection
        self._check_albumentations_injection(node)
        
        # Phase 4.2.3: Data Versioning & Tracking (5 checks - AIML426-430)
        
        # AIML426: DVC injection
        self._check_dvc_injection(node)
        
        # AIML427: LakeFS tampering
        self._check_lakefs_tampering(node)
        
        # AIML428: Delta Lake poisoning
        self._check_delta_lake_poisoning(node)
        
        # AIML429: Iceberg table manipulation
        self._check_iceberg_table_manipulation(node)
        
        # AIML430: Hudi versioning risks
        self._check_hudi_versioning_risks(node)
        
        # Phase 4.3: Model Registry & Versioning (20 checks - AIML431-450)
        # Phase 4.3.1: Model Registry Security (12 checks - AIML431-442)
        # AIML431: MLflow Model Registry injection
        self._check_mlflow_registry_injection(node)
        
        # AIML432: Weights & Biases artifact tampering
        self._check_wandb_artifact_tampering(node)
        
        # AIML433: Neptune.ai model poisoning
        self._check_neptune_model_poisoning(node)
        
        # AIML434: Comet.ml registry manipulation
        self._check_comet_registry_manipulation(node)
        
        # AIML435: AWS SageMaker model registry risks
        self._check_sagemaker_registry_risks(node)
        
        # AIML436: Azure ML model registry gaps
        self._check_azure_ml_registry_gaps(node)
        
        # AIML437: Google Vertex AI model registry
        self._check_vertex_ai_registry(node)
        
        # AIML438: Custom registry vulnerabilities
        self._check_custom_registry_vulnerabilities(node)
        
        # AIML439: Model metadata injection
        self._check_model_metadata_injection(node)
        
        # AIML440: Model versioning bypass
        self._check_model_versioning_bypass(node)
        
        # AIML441: Model lineage tampering
        self._check_model_lineage_tampering(node)
        
        # AIML442: Model approval workflow gaps
        self._check_model_approval_workflow_gaps(node)
        
        # Phase 4.3.2: Model Packaging & Distribution (8 checks - AIML443-450)
        # AIML443: Docker image model injection
        self._check_docker_image_model_injection(node)
        
        # AIML444: Model as a service (MaaS) risks
        self._check_maas_risks(node)
        
        # AIML445: Model marketplace vulnerabilities
        self._check_model_marketplace_vulnerabilities(node)
        
        # AIML446: Model license bypass
        self._check_model_license_bypass(node)
        
        # AIML447: Model watermark removal
        self._check_model_watermark_removal(node)
        
        # AIML448: Model fingerprinting attacks
        self._check_model_fingerprinting_attacks(node)
        
        # AIML449: Model compression tampering
        self._check_model_compression_tampering(node)
        
        # AIML450: Model conversion vulnerabilities
        self._check_model_conversion_vulnerabilities(node)
        
        # Phase 4.4: Cloud & Infrastructure Security (10 checks - AIML451-460)
        # Phase 4.4.1: Cloud ML Services (10 checks - AIML451-460)
        # AIML451: AWS SageMaker notebook injection
        self._check_sagemaker_notebook_injection(node)
        
        # AIML452: Azure ML workspace tampering
        self._check_azure_ml_workspace_tampering(node)
        
        # AIML453: Google Vertex AI pipeline manipulation
        self._check_vertex_ai_pipeline_manipulation(node)
        
        # AIML454: Databricks ML runtime risks
        self._check_databricks_ml_runtime_risks(node)
        
        # AIML455: Snowflake ML vulnerabilities
        self._check_snowflake_ml_vulnerabilities(node)
        
        # AIML456: BigQuery ML injection
        self._check_bigquery_ml_injection(node)
        
        # AIML457: Redshift ML tampering
        self._check_redshift_ml_tampering(node)
        
        # AIML458: Lambda ML inference risks
        self._check_lambda_ml_inference_risks(node)
        
        # AIML459: Cloud Functions ML serving gaps
        self._check_cloud_functions_ml_serving_gaps(node)
        
        # AIML460: Serverless ML vulnerabilities
        self._check_serverless_ml_vulnerabilities(node)
        
        # Phase 5.1: Generative AI Security (20 checks - AIML461-480)
        # Phase 5.1.1: Text Generation Security (10 checks - AIML461-470)
        # AIML461: Prompt leaking attacks
        self._check_prompt_leaking_attacks(node)
        
        # AIML462: Training data extraction
        self._check_training_data_extraction(node)
        
        # AIML463: Memorization exploitation
        self._check_memorization_exploitation(node)
        
        # AIML464: Copyright infringement detection
        self._check_copyright_infringement(node)
        
        # AIML465: Generated code security (Copilot-style)
        self._check_generated_code_security(node)
        
        # AIML466: Jailbreak detection
        self._check_jailbreak_detection(node)
        
        # AIML467: Toxicity generation risks
        self._check_toxicity_generation(node)
        
        # AIML468: Bias amplification detection
        self._check_bias_amplification(node)
        
        # AIML469: Hallucination exploitation
        self._check_hallucination_exploitation(node)
        
        # AIML470: Output filtering bypass
        self._check_output_filtering_bypass(node)
        
        # Phase 5.1.2: Image/Video Generation (10 checks - AIML471-480)
        # AIML471: Stable Diffusion prompt injection
        self._check_stable_diffusion_prompt_injection(node)
        
        # AIML472: DALL-E manipulation
        self._check_dalle_manipulation(node)
        
        # AIML473: Midjourney prompt engineering
        self._check_midjourney_prompt_engineering(node)
        
        # AIML474: GAN mode collapse exploitation
        self._check_gan_mode_collapse(node)
        
        # AIML475: VAE latent space manipulation
        self._check_vae_latent_manipulation(node)
        
        # AIML476: Diffusion model backdoors
        self._check_diffusion_model_backdoors(node)
        
        # AIML477: Video generation injection (Runway, Gen-2)
        self._check_video_generation_injection(node)
        
        # AIML478: 3D generation vulnerabilities (Point-E, Shap-E)
        self._check_3d_generation_vulnerabilities(node)
        
        # AIML479: Music generation risks (Jukebox, MusicLM)
        self._check_music_generation_risks(node)
        
        # AIML480: Audio generation injection (AudioLM, Whisper)
        self._check_audio_generation_injection(node)
        
        # Phase 5.2: Multimodal & Fusion Models (15 checks - AIML481-495)
        # Phase 5.2.1: Vision-Language Models (8 checks - AIML481-488)
        # AIML481: CLIP contrastive learning poisoning
        self._check_clip_contrastive_poisoning(node)
        
        # AIML482: ALIGN multimodal injection
        self._check_align_multimodal_injection(node)
        
        # AIML483: Flamingo few-shot manipulation
        self._check_flamingo_few_shot_manipulation(node)
        
        # AIML484: BLIP-2 query injection
        self._check_blip2_query_injection(node)
        
        # AIML485: GPT-4 Vision prompt attacks
        self._check_gpt4_vision_prompt_attacks(node)
        
        # AIML486: LLaVA instruction tuning risks
        self._check_llava_instruction_tuning_risks(node)
        
        # AIML487: MiniGPT-4 alignment bypass
        self._check_minigpt4_alignment_bypass(node)
        
        # AIML488: CoCa caption poisoning
        self._check_coca_caption_poisoning(node)
        
        # Phase 5.2.2: Audio-Visual & Cross-Modal (7 checks - AIML489-495)
        # AIML489: Audio-text alignment poisoning
        self._check_audio_text_alignment_poisoning(node)
        
        # AIML490: Video-text retrieval manipulation
        self._check_video_text_retrieval_manipulation(node)
        
        # AIML491: Speech-to-text injection
        self._check_speech_to_text_injection(node)
        
        # AIML492: Text-to-speech vulnerabilities
        self._check_text_to_speech_vulnerabilities(node)
        
        # AIML493: Visual grounding attacks
        self._check_visual_grounding_attacks(node)
        
        # AIML494: Embodied AI risks (robotics)
        self._check_embodied_ai_risks(node)
        
        # AIML495: Sensor fusion manipulation
        self._check_sensor_fusion_manipulation(node)
        
        # Phase 5.3: Federated & Privacy-Preserving ML (AIML496-510)
        # AIML496: Federated averaging poisoning
        self._check_federated_averaging_poisoning(node)
        
        # AIML497: Client selection manipulation
        self._check_client_selection_manipulation(node)
        
        # AIML498: Model aggregation attacks
        self._check_model_aggregation_attacks(node)
        
        # AIML499: Byzantine client detection bypass
        self._check_byzantine_client_detection_bypass(node)
        
        # AIML500: Privacy budget exploitation
        self._check_privacy_budget_exploitation(node)
        
        # AIML501: Differential privacy bypass
        self._check_differential_privacy_bypass(node)
        
        # AIML502: Secure aggregation vulnerabilities
        self._check_secure_aggregation_vulnerabilities(node)
        
        # AIML503: Homomorphic encryption weaknesses
        self._check_homomorphic_encryption_weaknesses(node)
        
        # AIML504: Trusted execution environment gaps
        self._check_trusted_execution_environment_gaps(node)
        
        # AIML505: Split learning injection
        self._check_split_learning_injection(node)
        
        # AIML506: Differential privacy parameter manipulation
        self._check_differential_privacy_parameter_manipulation(node)
        
        # AIML507: SMPC risks
        self._check_smpc_risks(node)
        
        # AIML508: Trusted execution environment bypass
        self._check_trusted_execution_environment_bypass(node)
        
        # AIML509: Encrypted inference vulnerabilities
        self._check_encrypted_inference_vulnerabilities(node)
        
        # AIML510: Zero-knowledge proof gaps
        self._check_zero_knowledge_proof_gaps(node)
        
        # AIML007: Insecure model serialization
        self._check_insecure_serialization(node)
        
        # AIML008: Missing input validation
        self._check_missing_input_validation(node)
        
        # AIML009: GPU memory leakage
        self._check_gpu_memory_leakage(node)
        
        self.generic_visit(node)

    def visit_Assign(self, node: ast.Assign) -> None:
        """Check for AI/ML security issues in assignments."""
        # AIML001: Check for prompt injection in assignments (f-strings, .format())
        if self.has_llm_framework:
            for target in node.targets:
                if isinstance(target, ast.Name):
                    var_name = target.id
                    # Check if this looks like a prompt variable
                    if "prompt" in var_name.lower() or "message" in var_name.lower() or "text" in var_name.lower() or "query" in var_name.lower() or "content" in var_name.lower() or "msg" in var_name.lower() or "input" in var_name.lower() or "user" in var_name.lower():
                        # Check if using f-string
                        if isinstance(node.value, ast.JoinedStr):
                            self.violations.append(
                                RuleViolation(
                                    rule_id="AIML001",
                                    category=RuleCategory.SECURITY,
                                    severity=RuleSeverity.CRITICAL,
                                    message="Potential prompt injection: F-string used for LLM prompt with user input",
                                    line_number=node.lineno,
                                    column=node.col_offset,
                                    end_line_number=getattr(node, "end_lineno", node.lineno),
                                    end_column=getattr(node, "end_col_offset", node.col_offset),
                                    file_path=str(self.file_path),
                                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                                    fix_applicability=FixApplicability.SAFE,
                                    fix_data=None,
                                    owasp_id="LLM01",
                                    cwe_id="CWE-94",
                                    source_tool="pyguard",
                                )
                            )
                        # Check if using .format()
                        elif isinstance(node.value, ast.Call):
                            if isinstance(node.value.func, ast.Attribute):
                                if node.value.func.attr == "format":
                                    self.violations.append(
                                        RuleViolation(
                                            rule_id="AIML001",
                                            category=RuleCategory.SECURITY,
                                            severity=RuleSeverity.CRITICAL,
                                            message="Potential prompt injection: .format() used for LLM prompt with user input",
                                            line_number=node.lineno,
                                            column=node.col_offset,
                                            end_line_number=getattr(node, "end_lineno", node.lineno),
                                            end_column=getattr(node, "end_col_offset", node.col_offset),
                                            file_path=str(self.file_path),
                                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                                            fix_applicability=FixApplicability.SAFE,
                                            fix_data=None,
                                            owasp_id="LLM01",
                                            cwe_id="CWE-94",
                                            source_tool="pyguard",
                                        )
                                    )
                        # AIML011-AIML022: Check for various prompt injection patterns in string constants
                        elif isinstance(node.value, ast.Constant) and isinstance(node.value.value, str):
                            self._check_string_for_prompt_override(node.value.value, node)
                            self._check_string_for_unicode_injection(node.value.value, node)
                            self._check_string_for_role_confusion(node.value.value, node)
                            self._check_string_for_instruction_concatenation(node.value.value, node)
                            self._check_string_for_multilanguage_injection(node.value.value, node)
                            self._check_string_for_markdown_injection(node.value.value, node)
                            self._check_string_for_payload_injection(node.value.value, node)
                            self._check_string_for_sql_comment_injection(node.value.value, node)
                            self._check_string_for_escape_sequence_injection(node.value.value, node)
                            self._check_string_for_token_stuffing(node.value.value, node)
                            self._check_string_for_recursive_prompt_injection(node.value.value, node)
                            self._check_string_for_base64_injection(node.value.value, node)
                            self._check_string_for_rot13_obfuscation(node.value.value, node)
                            self._check_string_for_invisible_char_injection(node.value.value, node)
                            self._check_string_for_bidi_override(node.value.value, node)
                            self._check_string_for_template_literal_injection(node.value.value, node)
                            self._check_string_for_variable_substitution(node.value.value, node)
                            self._check_string_for_context_window_overflow(node.value.value, node)
                            self._check_string_for_attention_manipulation(node.value.value, node)
        
        # AIML002: Model inversion risks
        self._check_model_inversion(node)
        
        # AIML003: Training data poisoning
        self._check_training_data_poisoning(node)
        
        self.generic_visit(node)

    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:
        """Check function definitions for AI/ML security issues."""
        # AIML005: Model extraction vulnerabilities
        self._check_model_extraction(node)
        
        # AIML006: AI bias detection
        self._check_ai_bias(node)
        
        # AIML010: Federated learning privacy
        self._check_federated_learning(node)
        
        self.generic_visit(node)

    def _check_prompt_injection(self, node: ast.Call) -> None:
        """AIML001: Detect prompt injection vulnerabilities in LLM applications."""
        if not self.has_llm_framework:
            return
            
        # Check for string formatting with user input in prompts
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["create", "complete", "generate", "chat"]:
                for arg in node.args:
                    if self._contains_user_input(arg):
                        violation = RuleViolation(
                            rule_id="AIML001",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.CRITICAL,
                            message="Potential prompt injection: User input concatenated directly into LLM prompt",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="LLM01",
                            cwe_id="CWE-94",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_system_prompt_override(self, node: ast.Call) -> None:
        """AIML011: Detect system prompt override attempts (delimiter injection)."""
        if not self.has_llm_framework:
            return
        
        # Check for dangerous delimiter patterns in function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_prompt_override(arg.value, node)
                    
        # Also check for delimiter injection in keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_prompt_override(keyword.value.value, node)
    
    def _check_string_for_prompt_override(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string contains prompt override patterns."""
        if not self.has_llm_framework:
            return
            
        # Check for dangerous delimiter patterns in string literals
        dangerous_patterns = [
            "ignore previous instructions",
            "ignore above",
            "ignore all previous",
            "new system message",
            "system:",
            "assistant:",
            "you are now",
            "forget everything",
            "disregard previous",
        ]
        
        lower_val = text.lower()
        if any(pattern in lower_val for pattern in dangerous_patterns):
            violation = RuleViolation(
                rule_id="AIML011",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.CRITICAL,
                message="System prompt override attempt detected: delimiter injection pattern",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="LLM01",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
            
    def _check_unicode_injection(self, node: ast.Call) -> None:
        """AIML012: Detect Unicode/homoglyph injection in prompts."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_unicode_injection(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_unicode_injection(keyword.value.value, node)
    
    def _check_string_for_unicode_injection(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string contains Unicode injection patterns."""
        if not self.has_llm_framework:
            return
            
        # Check for Unicode/homoglyph patterns
        suspicious_unicode_ranges = [
            (0x200B, 0x200F),  # Zero-width characters
            (0x202A, 0x202E),  # Bi-directional text overrides
            (0xFEFF, 0xFEFF),  # Zero-width no-break space
        ]
        
        def contains_suspicious_unicode(text: str) -> bool:
            """Check if text contains suspicious Unicode characters."""
            for char in text:
                code_point = ord(char)
                for start, end in suspicious_unicode_ranges:
                    if start <= code_point <= end:
                        return True
            return False
        
        if contains_suspicious_unicode(text):
            violation = RuleViolation(
                rule_id="AIML012",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.HIGH,
                message="Unicode injection detected: suspicious zero-width or bi-directional characters",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="LLM01",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_role_confusion(self, node: ast.Call) -> None:
        """AIML013: Detect role confusion attacks (DAN mode, jailbreaks)."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_role_confusion(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_role_confusion(keyword.value.value, node)
    
    def _check_string_for_role_confusion(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string contains role confusion patterns."""
        if not self.has_llm_framework:
            return
        
        # Role confusion patterns - DAN mode, jailbreaks
        role_confusion_patterns = [
            "dan mode",
            "do anything now",
            "developer mode",
            "jailbreak",
            "unrestricted mode",
            "act as if",
            "pretend you are",
            "simulate being",
            "roleplay as",
            "you are now a",
            "switch to",
            "enable dev mode",
            "unlock",
        ]
        
        lower_text = text.lower()
        if any(pattern in lower_text for pattern in role_confusion_patterns):
            violation = RuleViolation(
                rule_id="AIML013",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.CRITICAL,
                message="Role confusion attack detected: DAN mode or jailbreak attempt",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="LLM01",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_instruction_concatenation(self, node: ast.Call) -> None:
        """AIML014: Detect instruction concatenation bypasses."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_instruction_concatenation(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_instruction_concatenation(keyword.value.value, node)
    
    def _check_string_for_instruction_concatenation(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string contains instruction concatenation patterns."""
        if not self.has_llm_framework:
            return
        
        # Instruction concatenation patterns - multiple newlines or delimiter sequences
        concatenation_indicators = [
            "\n\n\n",  # Multiple newlines
            "---",     # Delimiter sequence
            "###",     # Markdown header delimiters
            "===",     # Another delimiter
            "***",     # Another delimiter
        ]
        
        # Also check for instruction keywords after delimiters
        instruction_keywords = ["new instruction:", "new task:", "new prompt:", "override:", "update:"]
        
        has_delimiter = any(delimiter in text for delimiter in concatenation_indicators)
        has_instruction = any(keyword in text.lower() for keyword in instruction_keywords)
        
        if has_delimiter and has_instruction:
            violation = RuleViolation(
                rule_id="AIML014",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.HIGH,
                message="Instruction concatenation bypass detected: delimiter followed by instruction keywords",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="LLM01",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_multilanguage_injection(self, node: ast.Call) -> None:
        """AIML015: Detect multi-language prompt injection (non-English)."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_multilanguage_injection(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_multilanguage_injection(keyword.value.value, node)
    
    def _check_string_for_multilanguage_injection(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string contains multi-language injection patterns."""
        if not self.has_llm_framework:
            return
        
        # Common prompt injection phrases in other languages
        multilang_patterns = [
            # Chinese (Simplified)
            "å¿½ç•¥ä»¥ä¸Š",  # ignore above
            "å¿½ç•¥ä¹‹å‰",  # ignore previous
            "æ–°æŒ‡ä»¤",    # new instruction
            # Spanish (case-insensitive)
            ("ignorar instrucciones anteriores", True),
            ("nueva instrucciÃ³n", True),
            # French (case-insensitive)
            ("ignorer les instructions prÃ©cÃ©dentes", True),
            ("nouvelle instruction", True),
            # German (case-insensitive)
            ("vorherige anweisungen ignorieren", True),
            ("neue anweisung", True),
            # Russian (Cyrillic)
            "Ð¸Ð³Ð½Ð¾Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ðµ",
            "Ð½Ð¾Ð²Ð°Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ",
            # Japanese
            "ä»¥å‰ã®æŒ‡ç¤ºã‚’ç„¡è¦–",
            "æ–°ã—ã„æŒ‡ç¤º",
            # Korean
            "ì´ì „ ì§€ì‹œ ë¬´ì‹œ",
            "ìƒˆë¡œìš´ ì§€ì‹œ",
        ]
        
        lower_text = text.lower()
        for pattern in multilang_patterns:
            # Handle tuple patterns (pattern, case_insensitive)
            if isinstance(pattern, tuple):
                pattern_text, case_insensitive = pattern
                if case_insensitive:
                    if pattern_text.lower() in lower_text:
                        matched = True
                        break
                else:
                    if pattern_text in text:
                        matched = True
                        break
            else:
                # Direct string match (for non-Latin scripts)
                if pattern in text:
                    matched = True
                    break
        else:
            matched = False
        
        if matched:
            violation = RuleViolation(
                rule_id="AIML015",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.HIGH,
                message="Multi-language prompt injection detected: non-English prompt manipulation attempt",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="LLM01",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_markdown_injection(self, node: ast.Call) -> None:
        """AIML016: Detect Markdown injection in prompts."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_markdown_injection(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_markdown_injection(keyword.value.value, node)
    
    def _check_string_for_markdown_injection(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string contains Markdown injection patterns."""
        if not self.has_llm_framework:
            return
        
        # Markdown patterns that could be used for injection
        markdown_injection_patterns = [
            "](javascript:",  # Markdown link with javascript protocol
            "](data:",        # Markdown link with data protocol
            "](file:",        # Markdown link with file protocol
            "![",             # Image embedding
            "<script",        # Script tag in markdown
            "<iframe",        # Iframe tag in markdown
            "```javascript",  # JavaScript code block
            "```html",        # HTML code block
        ]
        
        lower_text = text.lower()
        if any(pattern in lower_text for pattern in markdown_injection_patterns):
            violation = RuleViolation(
                rule_id="AIML016",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.HIGH,
                message="Markdown injection detected: potentially malicious markdown constructs in prompt",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="LLM01",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_payload_injection(self, node: ast.Call) -> None:
        """AIML017: Detect XML/JSON payload injection."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_payload_injection(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_payload_injection(keyword.value.value, node)
    
    def _check_string_for_payload_injection(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string contains XML/JSON payload injection patterns."""
        if not self.has_llm_framework:
            return
        
        # XML/JSON payload injection patterns
        payload_patterns = [
            "<system>",       # XML system tag
            "</system>",      # XML system closing tag
            "<instruction>",  # XML instruction tag
            '"role":',        # JSON role field
            '"system":',      # JSON system field
            '{"role": "system"',  # JSON system role
            "<assistant>",    # XML assistant tag
            '"content":',     # JSON content field
        ]
        
        lower_text = text.lower()
        
        # Count suspicious patterns
        suspicious_count = sum(1 for pattern in payload_patterns if pattern in lower_text)
        
        # Flag if multiple payload indicators present
        if suspicious_count >= 2:
            violation = RuleViolation(
                rule_id="AIML017",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.HIGH,
                message="XML/JSON payload injection detected: structured payload manipulation in prompt",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="LLM01",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_sql_comment_injection(self, node: ast.Call) -> None:
        """AIML018: Detect SQL-style comment injection in prompts."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_sql_comment_injection(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_sql_comment_injection(keyword.value.value, node)
    
    def _check_string_for_sql_comment_injection(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string contains SQL-style comment injection patterns."""
        if not self.has_llm_framework:
            return
        
        # SQL-style comment patterns used for prompt injection
        sql_comment_patterns = [
            "-- ",          # SQL single-line comment
            "--\n",         # SQL comment with newline
            "--\r",         # SQL comment with carriage return
            "/* ",          # SQL multi-line comment start
            " */",          # SQL multi-line comment end
            "#ignore",      # Hash comment with injection keyword
            "-- ignore",    # SQL comment with ignore
            "/* ignore",    # Multi-line comment with ignore
        ]
        
        # Also check for patterns that combine comments with instructions
        has_comment = any(pattern in text for pattern in sql_comment_patterns)
        has_instruction_after_comment = False
        
        if has_comment:
            # Check if there's an instruction keyword after the comment
            instruction_keywords = ["ignore", "bypass", "override", "new", "system", "admin"]
            lower_text = text.lower()
            has_instruction_after_comment = any(keyword in lower_text for keyword in instruction_keywords)
        
        if has_comment and has_instruction_after_comment:
            violation = RuleViolation(
                rule_id="AIML018",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.HIGH,
                message="SQL-style comment injection detected: comment syntax used for prompt manipulation",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="LLM01",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_escape_sequence_injection(self, node: ast.Call) -> None:
        """AIML019: Detect escape sequence injection in prompts."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_escape_sequence_injection(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_escape_sequence_injection(keyword.value.value, node)
    
    def _check_string_for_escape_sequence_injection(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string contains escape sequence injection patterns."""
        if not self.has_llm_framework:
            return
        
        # Escape sequence patterns that could be used for injection
        # Looking for patterns like actual newlines (Python has already processed escape sequences)
        # Check for multiple consecutive newlines
        has_multiple_newlines = "\n\n\n" in text
        has_multiple_crlf = "\r\n\r\n" in text
        has_null_byte = "\x00" in text
        has_escape_char = "\x1b" in text or "\033" in text
        
        # Count suspicious patterns
        suspicious_count = sum([
            has_multiple_newlines,
            has_multiple_crlf,
            has_null_byte,
            has_escape_char
        ])
        
        # Also check for these patterns followed by instruction keywords
        has_escape_with_instruction = False
        if suspicious_count > 0:
            instruction_keywords = ["new", "system", "ignore", "override", "instruction"]
            lower_text = text.lower()
            has_escape_with_instruction = any(keyword in lower_text for keyword in instruction_keywords)
        
        if suspicious_count >= 1 and has_escape_with_instruction:
            violation = RuleViolation(
                rule_id="AIML019",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.HIGH,
                message="Escape sequence injection detected: suspicious escape sequences in prompt",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="LLM01",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_token_stuffing(self, node: ast.Call) -> None:
        """AIML020: Detect token stuffing attacks (context window exhaustion)."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments for very long strings
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_token_stuffing(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_token_stuffing(keyword.value.value, node)
    
    def _check_string_for_token_stuffing(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string is attempting token stuffing."""
        if not self.has_llm_framework:
            return
        
        # Token stuffing indicators:
        # 1. Very long strings (>8000 chars is suspicious)
        # 2. Highly repetitive content
        text_length = len(text)
        
        if text_length > 8000:
            # Check for repetitiveness (same pattern repeated many times)
            # Simple heuristic: if the first 100 chars appear multiple times, it's repetitive
            if text_length > 200:
                sample = text[:100]
                occurrences = text.count(sample)
                if occurrences > 10:  # Very repetitive
                    violation = RuleViolation(
                        rule_id="AIML020",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message=f"Token stuffing attack detected: very long ({text_length} chars) repetitive prompt",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM01",
                        cwe_id="CWE-400",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_recursive_prompt_injection(self, node: ast.Call) -> None:
        """AIML021: Detect recursive prompt injection (prompts containing prompts)."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_recursive_prompt_injection(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_recursive_prompt_injection(keyword.value.value, node)
    
    def _check_string_for_recursive_prompt_injection(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string contains recursive prompt injection patterns."""
        if not self.has_llm_framework:
            return
        
        # Recursive prompt injection patterns - prompts within prompts
        recursive_patterns = [
            "prompt:",
            "user:",
            "assistant:",
            "system:",
            "generate a prompt",
            "create a prompt",
            "write a prompt",
            "respond to:",
            "answer:",
        ]
        
        lower_text = text.lower()
        pattern_count = sum(1 for pattern in recursive_patterns if pattern in lower_text)
        
        # If multiple prompt-related keywords, likely recursive
        if pattern_count >= 2:
            violation = RuleViolation(
                rule_id="AIML021",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.MEDIUM,
                message="Recursive prompt injection detected: prompt contains nested prompt instructions",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="LLM01",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_base64_injection(self, node: ast.Call) -> None:
        """AIML022: Detect Base64 encoded injection attempts."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_base64_injection(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_base64_injection(keyword.value.value, node)
    
    def _check_string_for_base64_injection(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string contains Base64 encoded injection attempts."""
        if not self.has_llm_framework:
            return
        
        # Base64 pattern detection
        # Look for Base64-like strings (alphanumeric + / and + with = padding)
        import re
        base64_pattern = r'[A-Za-z0-9+/]{20,}={0,2}'
        matches = re.findall(base64_pattern, text)
        
        if matches:
            # Try to decode and check for malicious patterns
            try:
                import base64
                for match in matches:
                    try:
                        decoded = base64.b64decode(match).decode('utf-8', errors='ignore')
                        # Check if decoded text contains injection keywords
                        malicious_keywords = ["ignore", "system", "bypass", "override", "admin", "jailbreak"]
                        if any(keyword in decoded.lower() for keyword in malicious_keywords):
                            violation = RuleViolation(
                                rule_id="AIML022",
                                category=RuleCategory.SECURITY,
                                severity=RuleSeverity.HIGH,
                                message="Base64 encoded injection detected: encoded malicious content in prompt",
                                line_number=node.lineno,
                                column=node.col_offset,
                                end_line_number=getattr(node, "end_lineno", node.lineno),
                                end_column=getattr(node, "end_col_offset", node.col_offset),
                                file_path=str(self.file_path),
                                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                                fix_applicability=FixApplicability.SAFE,
                                fix_data=None,
                                owasp_id="LLM01",
                                cwe_id="CWE-94",
                                source_tool="pyguard",
                            )
                            self.violations.append(violation)
                            break
                    except Exception:
                        # Invalid Base64, skip
                        continue
            except ImportError:
                # base64 module not available, skip
                pass

    def _check_rot13_obfuscation(self, node: ast.Call) -> None:
        """AIML023: Detect ROT13/Caesar cipher obfuscation in prompts."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_rot13_obfuscation(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_rot13_obfuscation(keyword.value.value, node)
    
    def _check_string_for_rot13_obfuscation(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string contains ROT13 or Caesar cipher obfuscation."""
        if not self.has_llm_framework:
            return
        
        # Check for ROT13 encoded text by looking for patterns
        # ROT13 characteristic: high concentration of unusual letter patterns
        # Simple heuristic: check if decoding with ROT13 produces more common English words
        import codecs
        try:
            # Try ROT13 decode
            decoded = codecs.decode(text, 'rot_13')
            
            # Check if decoded text contains malicious keywords
            malicious_keywords = ["ignore", "system", "bypass", "override", "admin", "jailbreak", "instruction"]
            lower_decoded = decoded.lower()
            
            # Also check original text doesn't contain these keywords (to avoid false positives)
            lower_text = text.lower()
            has_keywords_in_decoded = any(keyword in lower_decoded for keyword in malicious_keywords)
            has_keywords_in_original = any(keyword in lower_text for keyword in malicious_keywords)
            
            # If decoded has keywords but original doesn't, likely ROT13 obfuscation
            if has_keywords_in_decoded and not has_keywords_in_original and len(text) > 10:
                violation = RuleViolation(
                    rule_id="AIML023",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="ROT13/Caesar cipher obfuscation detected: encoded malicious content in prompt",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
        except Exception:
            # ROT13 decode failed, skip
            pass
    
    def _check_invisible_char_injection(self, node: ast.Call) -> None:
        """AIML024: Detect invisible character injection (zero-width spaces)."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_invisible_char_injection(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_invisible_char_injection(keyword.value.value, node)
    
    def _check_string_for_invisible_char_injection(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string contains invisible character injection."""
        if not self.has_llm_framework:
            return
        
        # Invisible/zero-width characters that can be used for injection
        invisible_chars = [
            '\u200b',  # Zero-width space
            '\u200c',  # Zero-width non-joiner
            '\u200d',  # Zero-width joiner
            '\u2060',  # Word joiner
            '\ufeff',  # Zero-width no-break space (BOM)
            '\u180e',  # Mongolian vowel separator
            '\u00ad',  # Soft hyphen
        ]
        
        # Count invisible characters
        invisible_count = sum(text.count(char) for char in invisible_chars)
        
        if invisible_count > 0:
            # Check if there are also instruction keywords (suggests malicious intent)
            malicious_keywords = ["ignore", "system", "bypass", "override", "instruction"]
            has_keywords = any(keyword in text.lower() for keyword in malicious_keywords)
            
            if has_keywords or invisible_count >= 3:
                violation = RuleViolation(
                    rule_id="AIML024",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message=f"Invisible character injection detected: {invisible_count} zero-width characters in prompt",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_bidi_override(self, node: ast.Call) -> None:
        """AIML025: Detect right-to-left override attacks (Unicode bidi)."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_bidi_override(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_bidi_override(keyword.value.value, node)
    
    def _check_string_for_bidi_override(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string contains Unicode bidi override attacks."""
        if not self.has_llm_framework:
            return
        
        # Unicode bidirectional text control characters
        bidi_chars = [
            '\u202a',  # Left-to-right embedding
            '\u202b',  # Right-to-left embedding
            '\u202c',  # Pop directional formatting
            '\u202d',  # Left-to-right override
            '\u202e',  # Right-to-left override
            '\u2066',  # Left-to-right isolate
            '\u2067',  # Right-to-left isolate
            '\u2068',  # First strong isolate
            '\u2069',  # Pop directional isolate
        ]
        
        # Count bidi control characters
        bidi_count = sum(text.count(char) for char in bidi_chars)
        
        if bidi_count > 0:
            violation = RuleViolation(
                rule_id="AIML025",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.HIGH,
                message=f"Unicode bidirectional override detected: {bidi_count} bidi control characters in prompt",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="LLM01",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_template_literal_injection(self, node: ast.Call) -> None:
        """AIML026: Detect prompt template literal injection."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_template_literal_injection(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_template_literal_injection(keyword.value.value, node)
    
    def _check_string_for_template_literal_injection(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string contains template literal injection patterns."""
        if not self.has_llm_framework:
            return
        
        # Template literal injection patterns
        template_patterns = [
            "${",          # JavaScript template literal
            "{{",          # Jinja2/Handlebars template
            "{%",          # Jinja2 control
            "<%",          # ERB/EJS template
            "[[",          # Custom template syntax
            "<<",          # Heredoc-style templates
        ]
        
        # Check for template patterns
        has_template = any(pattern in text for pattern in template_patterns)
        
        if has_template:
            # Check if there are also injection keywords
            injection_keywords = ["eval", "exec", "import", "system", "process", "require"]
            has_injection = any(keyword in text.lower() for keyword in injection_keywords)
            
            if has_injection:
                violation = RuleViolation(
                    rule_id="AIML026",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Template literal injection detected: template syntax with dangerous code execution patterns",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_fstring_injection(self, node: ast.Call) -> None:
        """AIML027: Detect F-string injection in prompts."""
        if not self.has_llm_framework:
            return
        
        # Check for f-string usage in arguments
        for arg in node.args:
            if isinstance(arg, ast.JoinedStr):
                # This is an f-string
                violation = RuleViolation(
                    rule_id="AIML027",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="F-string injection in prompt: unvalidated user input in f-string can lead to injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
        
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.JoinedStr):
                violation = RuleViolation(
                    rule_id="AIML027",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="F-string injection in prompt: unvalidated user input in f-string can lead to injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_variable_substitution(self, node: ast.Call) -> None:
        """AIML028: Detect variable substitution attacks."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_variable_substitution(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_variable_substitution(keyword.value.value, node)
    
    def _check_string_for_variable_substitution(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string contains variable substitution attacks."""
        if not self.has_llm_framework:
            return
        
        # Variable substitution patterns that could be exploited
        substitution_patterns = [
            "$(",          # Shell command substitution
            "`",           # Backtick command substitution
            "${env:",      # Environment variable access
            "${var:",      # Variable interpolation
            "$(whoami)",   # Common attack pattern
            "${PATH}",     # Environment variable
            "$USER",       # Environment variable
        ]
        
        # Check for substitution patterns
        has_substitution = any(pattern in text for pattern in substitution_patterns)
        
        if has_substitution:
            violation = RuleViolation(
                rule_id="AIML028",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.HIGH,
                message="Variable substitution attack detected: shell/environment variable substitution in prompt",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="LLM01",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_context_window_overflow(self, node: ast.Call) -> None:
        """AIML029: Detect context window overflow attempts."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_context_window_overflow(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_context_window_overflow(keyword.value.value, node)
    
    def _check_string_for_context_window_overflow(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string attempts context window overflow."""
        if not self.has_llm_framework:
            return
        
        # Context window overflow indicators:
        # 1. Extremely long text (>32000 chars is suspicious for most LLMs)
        # 2. Repetitive patterns designed to fill context
        text_length = len(text)
        
        if text_length > 32000:
            violation = RuleViolation(
                rule_id="AIML029",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.MEDIUM,
                message=f"Context window overflow detected: extremely long prompt ({text_length} chars)",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="LLM01",
                cwe_id="CWE-400",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_attention_manipulation(self, node: ast.Call) -> None:
        """AIML030: Detect attention mechanism manipulation attempts."""
        if not self.has_llm_framework:
            return
        
        # Check function call arguments
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                self._check_string_for_attention_manipulation(arg.value, node)
                    
        # Check keyword arguments
        for keyword in node.keywords:
            if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                self._check_string_for_attention_manipulation(keyword.value.value, node)
    
    def _check_string_for_attention_manipulation(self, text: str, node: ast.AST) -> None:
        """Helper to check if a string attempts to manipulate attention mechanisms."""
        if not self.has_llm_framework:
            return
        
        # Attention manipulation patterns
        attention_patterns = [
            "pay attention to",
            "focus on",
            "most important",
            "critical:",
            "priority:",
            "emphasis:",
            "highlight:",
            "**important**",
            "!!!",
            "URGENT",
            "CRITICAL",
        ]
        
        # Check for attention manipulation combined with instruction changes
        has_attention = any(pattern in text.lower() for pattern in [p.lower() for p in attention_patterns])
        
        if has_attention:
            # Check if combined with instruction keywords
            instruction_keywords = ["ignore", "override", "bypass", "forget", "new instruction", "disregard"]
            has_instruction = any(keyword in text.lower() for keyword in instruction_keywords)
            
            if has_instruction:
                violation = RuleViolation(
                    rule_id="AIML030",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Attention manipulation detected: emphasis markers combined with instruction override attempts",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_url_based_injection(self, node: ast.Call) -> None:
        """AIML031: Detect URL-based injection (fetched web content)."""
        if not self.has_llm_framework:
            return
        
        # Check for requests/urllib usage before LLM calls (indicates external content fetching)
        if isinstance(node.func, ast.Attribute):
            # Look for patterns like: content = requests.get(url).text; llm.generate(content)
            if node.func.attr in ["get", "post", "fetch", "retrieve", "download"]:
                # This is a potential external content fetch
                # Flag if used in LLM context
                violation = RuleViolation(
                    rule_id="AIML031",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="URL-based injection risk: External content fetched without sanitization before LLM processing",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_document_poisoning(self, node: ast.Call) -> None:
        """AIML032: Detect document poisoning (PDF, DOCX injection)."""
        if not self.has_llm_framework:
            return
        
        # Check for document parsing libraries
        if isinstance(node.func, ast.Attribute):
            doc_parsers = ["extract_text", "read_pdf", "parse_docx", "load_document", "parse_document"]
            if node.func.attr in doc_parsers:
                violation = RuleViolation(
                    rule_id="AIML032",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Document poisoning risk: Document content parsed without validation before LLM processing",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_image_injection(self, node: ast.Call) -> None:
        """AIML033: Detect image-based prompt injection (OCR manipulation)."""
        if not self.has_llm_framework:
            return
        
        # Check for OCR/image processing
        if isinstance(node.func, ast.Attribute):
            image_processors = ["image_to_text", "ocr", "read_text", "extract_text_from_image", "vision"]
            if node.func.attr in image_processors:
                violation = RuleViolation(
                    rule_id="AIML033",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Image-based injection risk: OCR/image text extracted without validation before LLM processing",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_api_response_injection(self, node: ast.Call) -> None:
        """AIML034: Detect API response injection (3rd party data)."""
        if not self.has_llm_framework:
            return
        
        # Check for API response parsing (json(), .text, etc.)
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["json", "text", "content"]:
                # Check if this is from a response object
                if isinstance(node.func.value, ast.Name):
                    if "response" in node.func.value.id.lower() or "resp" in node.func.value.id.lower():
                        violation = RuleViolation(
                            rule_id="AIML034",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.HIGH,
                            message="API response injection risk: Third-party API data used without sanitization",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="LLM01",
                            cwe_id="CWE-94",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_database_injection(self, node: ast.Call) -> None:
        """AIML035: Detect database content injection."""
        if not self.has_llm_framework:
            return
        
        # Check for database query executions
        if isinstance(node.func, ast.Attribute):
            db_methods = ["execute", "fetchall", "fetchone", "query", "find", "find_one"]
            if node.func.attr in db_methods:
                violation = RuleViolation(
                    rule_id="AIML035",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Database injection risk: Database content used without validation in LLM prompts",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_file_upload_injection(self, node: ast.Call) -> None:
        """AIML036: Detect file upload injection vectors."""
        if not self.has_llm_framework:
            return
        
        # Check for file read/upload operations
        if isinstance(node.func, ast.Attribute):
            file_ops = ["read", "read_text", "readlines", "load", "upload"]
            if node.func.attr in file_ops:
                # Check if this is a file operation
                if isinstance(node.func.value, ast.Name):
                    if "file" in node.func.value.id.lower() or "upload" in node.func.value.id.lower():
                        violation = RuleViolation(
                            rule_id="AIML036",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.HIGH,
                            message="File upload injection risk: Uploaded file content used without validation",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="LLM01",
                            cwe_id="CWE-94",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_email_injection(self, node: ast.Call) -> None:
        """AIML037: Detect email content injection."""
        if not self.has_llm_framework:
            return
        
        # Check for email parsing
        if isinstance(node.func, ast.Attribute):
            email_methods = ["get_body", "get_content", "parse_email", "read_email"]
            if node.func.attr in email_methods:
                violation = RuleViolation(
                    rule_id="AIML037",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Email injection risk: Email content used without sanitization in LLM prompts",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_social_scraping_injection(self, node: ast.Call) -> None:
        """AIML038: Detect social media scraping injection."""
        if not self.has_llm_framework:
            return
        
        # Check for social media APIs
        if isinstance(node.func, ast.Attribute):
            social_methods = ["get_tweets", "get_posts", "scrape", "fetch_timeline", "get_comments"]
            if node.func.attr in social_methods:
                violation = RuleViolation(
                    rule_id="AIML038",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Social scraping injection risk: Social media content used without validation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_rag_poisoning(self, node: ast.Call) -> None:
        """AIML039: Detect RAG poisoning (retrieval augmented generation)."""
        if not self.has_llm_framework:
            return
        
        # Check for RAG operations
        if isinstance(node.func, ast.Attribute):
            rag_methods = ["retrieve", "search", "query", "get_context", "get_relevant_docs"]
            if node.func.attr in rag_methods:
                # Check if this is a vector/document search
                if isinstance(node.func.value, ast.Name):
                    if any(keyword in node.func.value.id.lower() for keyword in ["vector", "index", "retriev", "rag", "embed"]):
                        violation = RuleViolation(
                            rule_id="AIML039",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.HIGH,
                            message="RAG poisoning risk: Retrieved content used without validation in prompts",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="LLM01",
                            cwe_id="CWE-94",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_vector_db_injection(self, node: ast.Call) -> None:
        """AIML040: Detect vector database injection."""
        if not self.has_llm_framework:
            return
        
        # Check for vector database operations
        if isinstance(node.func, ast.Attribute):
            vector_methods = ["similarity_search", "query", "search", "find_similar"]
            if node.func.attr in vector_methods:
                violation = RuleViolation(
                    rule_id="AIML040",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Vector database injection risk: Vector search results used without validation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_knowledge_base_tampering(self, node: ast.Call) -> None:
        """AIML041: Detect knowledge base tampering."""
        if not self.has_llm_framework:
            return
        
        # Check for knowledge base access
        if isinstance(node.func, ast.Attribute):
            kb_methods = ["get_knowledge", "query_kb", "search_kb", "get_facts"]
            if node.func.attr in kb_methods:
                violation = RuleViolation(
                    rule_id="AIML041",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Knowledge base tampering risk: KB content used without integrity verification",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_citation_manipulation(self, node: ast.Call) -> None:
        """AIML042: Detect citation manipulation."""
        if not self.has_llm_framework:
            return
        
        # Check for citation/reference extraction
        if isinstance(node.func, ast.Attribute):
            citation_methods = ["get_citations", "get_references", "get_sources", "extract_citations"]
            if node.func.attr in citation_methods:
                violation = RuleViolation(
                    rule_id="AIML042",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="Citation manipulation risk: Citation data used without verification",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_search_poisoning(self, node: ast.Call) -> None:
        """AIML043: Detect search result poisoning."""
        if not self.has_llm_framework:
            return
        
        # Check for web/search results
        if isinstance(node.func, ast.Attribute):
            search_methods = ["search", "google_search", "web_search", "get_results"]
            if node.func.attr in search_methods:
                violation = RuleViolation(
                    rule_id="AIML043",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Search result poisoning risk: Search results used without validation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_user_profile_injection(self, node: ast.Call) -> None:
        """AIML044: Detect user profile injection."""
        if not self.has_llm_framework:
            return
        
        # Check for user profile access
        if isinstance(node.func, ast.Attribute):
            profile_methods = ["get_profile", "get_user_data", "get_user_info", "load_profile"]
            if node.func.attr in profile_methods:
                violation = RuleViolation(
                    rule_id="AIML044",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="User profile injection risk: User-controlled profile data used in prompts",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_conversation_history_injection(self, node: ast.Call) -> None:
        """AIML045: Detect conversation history injection."""
        if not self.has_llm_framework:
            return
        
        # Check for conversation history access
        if isinstance(node.func, ast.Attribute):
            history_methods = ["get_history", "load_history", "get_messages", "get_conversation"]
            if node.func.attr in history_methods:
                violation = RuleViolation(
                    rule_id="AIML045",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Conversation history injection risk: Chat history used without sanitization",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    # Phase 1.1.3: LLM API Security (15 checks - AIML046-AIML060)
    
    def _check_missing_rate_limiting(self, node: ast.Call) -> None:
        """AIML046: Detect missing rate limiting on LLM API calls."""
        if not self.has_llm_framework:
            return
        
        # Check for LLM API calls without rate limiting
        llm_api_calls = [
            "create", "completion", "chat", "complete",
            "ChatCompletion", "Completion", "generate"
        ]
        
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in llm_api_calls:
                # Simple heuristic: Check if there's no rate limiting decorator or call
                # This is a simplified check - in reality, would need more context
                violation = RuleViolation(
                    rule_id="AIML046",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Missing rate limiting on LLM API call - potential DoS risk",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="LLM04",
                    cwe_id="CWE-770",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_unvalidated_llm_parameters(self, node: ast.Call) -> None:
        """AIML047: Detect unvalidated temperature/top_p parameters."""
        if not self.has_llm_framework:
            return
        
        # Check for temperature and top_p parameters that could be manipulated
        for keyword in node.keywords:
            if keyword.arg in ["temperature", "top_p", "top_k"]:
                # Check if value comes from user input (not a constant)
                if not isinstance(keyword.value, ast.Constant):
                    violation = RuleViolation(
                        rule_id="AIML047",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message=f"Unvalidated {keyword.arg} parameter - could enable model manipulation",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM04",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_max_tokens_manipulation(self, node: ast.Call) -> None:
        """AIML048: Detect max_tokens manipulation (DoS)."""
        if not self.has_llm_framework:
            return
        
        # Check for max_tokens parameter
        for keyword in node.keywords:
            if keyword.arg in ["max_tokens", "max_length", "max_new_tokens"]:
                # Check if value is not validated or comes from user input
                if not isinstance(keyword.value, ast.Constant):
                    violation = RuleViolation(
                        rule_id="AIML048",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Max tokens parameter from user input - DoS risk from excessive token generation",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM04",
                        cwe_id="CWE-400",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_streaming_response_injection(self, node: ast.Call) -> None:
        """AIML049: Detect streaming response injection."""
        if not self.has_llm_framework:
            return
        
        # Check for streaming API calls
        for keyword in node.keywords:
            if keyword.arg == "stream":
                if isinstance(keyword.value, ast.Constant) and keyword.value.value is True:
                    violation = RuleViolation(
                        rule_id="AIML049",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Streaming response without validation - injection risk in streamed chunks",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM01",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_function_calling_injection(self, node: ast.Call) -> None:
        """AIML050: Detect function calling injection."""
        if not self.has_llm_framework:
            return
        
        # Check for function calling parameters
        for keyword in node.keywords:
            if keyword.arg in ["functions", "tools", "function_call"]:
                violation = RuleViolation(
                    rule_id="AIML050",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Function calling enabled - validate function parameters to prevent injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_tool_use_tampering(self, node: ast.Call) -> None:
        """AIML051: Detect tool use parameter tampering."""
        if not self.has_llm_framework:
            return
        
        # Check for tool use configurations
        for keyword in node.keywords:
            if keyword.arg in ["tool_choice", "tools"]:
                # Check if tool parameters could be user-controlled
                if not isinstance(keyword.value, ast.Constant):
                    violation = RuleViolation(
                        rule_id="AIML051",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Tool use parameters from user input - tampering risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM01",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_system_message_manipulation(self, node: ast.Call) -> None:
        """AIML052: Detect system message manipulation via API."""
        if not self.has_llm_framework:
            return
        
        # Check for messages parameter with system role
        for keyword in node.keywords:
            if keyword.arg == "messages":
                # Check if it's a list/dict that could contain user-controlled system messages
                if isinstance(keyword.value, (ast.List, ast.Dict, ast.Name)):
                    violation = RuleViolation(
                        rule_id="AIML052",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.CRITICAL,
                        message="System message construction - ensure user input cannot modify system role",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM01",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_model_selection_bypass(self, node: ast.Call) -> None:
        """AIML053: Detect model selection bypass."""
        if not self.has_llm_framework:
            return
        
        # Check for model parameter that could be user-controlled
        for keyword in node.keywords:
            if keyword.arg in ["model", "engine", "model_name"]:
                if not isinstance(keyword.value, ast.Constant):
                    violation = RuleViolation(
                        rule_id="AIML053",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Model selection from user input - bypass risk and cost implications",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM04",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_api_key_exposure(self, node: ast.Call) -> None:
        """AIML054: Detect API key exposure in client code."""
        if not self.has_llm_framework:
            return
        
        # Check for hardcoded API keys
        for keyword in node.keywords:
            if keyword.arg in ["api_key", "api_token", "auth_token", "key"]:
                if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                    # Check if it looks like an API key (common patterns)
                    key_value = keyword.value.value
                    if len(key_value) > 10 and not key_value.startswith("$"):  # Not env var
                        violation = RuleViolation(
                            rule_id="AIML054",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.CRITICAL,
                            message="API key hardcoded in client code - use environment variables instead",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="LLM10",
                            cwe_id="CWE-798",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_hardcoded_model_names(self, node: ast.Call) -> None:
        """AIML055: Detect hardcoded model names (version lock-in)."""
        if not self.has_llm_framework:
            return
        
        # Check for hardcoded model names without version pinning
        for keyword in node.keywords:
            if keyword.arg in ["model", "engine"]:
                if isinstance(keyword.value, ast.Constant) and isinstance(keyword.value.value, str):
                    model_name = keyword.value.value
                    # Check if it's a model name without version
                    common_models = ["gpt-3.5-turbo", "gpt-4", "claude", "llama"]
                    if any(m in model_name.lower() for m in common_models):
                        violation = RuleViolation(
                            rule_id="AIML055",
                            category=RuleCategory.CONVENTION,
                            severity=RuleSeverity.LOW,
                            message="Hardcoded model name - consider using configuration for flexibility",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="LLM04",
                            cwe_id="CWE-1188",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_missing_timeout(self, node: ast.Call) -> None:
        """AIML056: Detect missing timeout configurations."""
        if not self.has_llm_framework:
            return
        
        # Check for LLM API calls
        llm_api_calls = [
            "create", "completion", "chat", "complete",
            "ChatCompletion", "Completion", "generate"
        ]
        
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in llm_api_calls:
                # Check if timeout is specified
                has_timeout = any(kw.arg == "timeout" for kw in node.keywords)
                if not has_timeout:
                    violation = RuleViolation(
                        rule_id="AIML056",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Missing timeout on LLM API call - DoS risk from hanging requests",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM04",
                        cwe_id="CWE-400",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_unhandled_api_errors(self, node: ast.Call) -> None:
        """AIML057: Detect unhandled API errors (info disclosure)."""
        if not self.has_llm_framework:
            return
        
        # This check would ideally look at try/except blocks around API calls
        # For now, we flag API calls that might need error handling
        llm_api_calls = [
            "create", "completion", "chat", "complete",
            "ChatCompletion", "Completion", "generate"
        ]
        
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in llm_api_calls:
                # Simple heuristic: flag for manual review
                violation = RuleViolation(
                    rule_id="AIML057",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="LLM API call - ensure error handling prevents information disclosure",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="LLM04",
                    cwe_id="CWE-209",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_token_counting_bypass(self, node: ast.Call) -> None:
        """AIML058: Detect token counting bypass."""
        if not self.has_llm_framework:
            return
        
        # Check for API calls without token counting
        llm_api_calls = [
            "create", "completion", "chat", "complete",
            "ChatCompletion", "Completion", "generate"
        ]
        
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in llm_api_calls:
                # Simple heuristic: warn about token counting
                violation = RuleViolation(
                    rule_id="AIML058",
                    category=RuleCategory.CONVENTION,
                    severity=RuleSeverity.LOW,
                    message="LLM API call - implement token counting to prevent context overflow",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="LLM04",
                    cwe_id="CWE-400",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_cost_overflow(self, node: ast.Call) -> None:
        """AIML059: Detect cost overflow attacks."""
        if not self.has_llm_framework:
            return
        
        # Check for expensive operations without cost limits
        for keyword in node.keywords:
            if keyword.arg in ["n", "num_completions", "best_of"]:
                # Check if value could be user-controlled
                if not isinstance(keyword.value, ast.Constant):
                    violation = RuleViolation(
                        rule_id="AIML059",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Completion count from user input - cost overflow risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM04",
                        cwe_id="CWE-400",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_conversation_state_injection(self, node: ast.Call) -> None:
        """AIML060: Detect multi-turn conversation state injection."""
        if not self.has_llm_framework:
            return
        
        # Check for state management in multi-turn conversations
        state_keywords = ["session_id", "conversation_id", "state", "context"]
        
        for keyword in node.keywords:
            if keyword.arg in state_keywords:
                # Check if state could be user-controlled
                if not isinstance(keyword.value, ast.Constant):
                    violation = RuleViolation(
                        rule_id="AIML060",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Conversation state from user input - injection risk in multi-turn interactions",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM01",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    # Phase 1.1.4: Output Validation & Filtering (10 checks - AIML061-AIML070)
    
    def _check_missing_output_sanitization(self, node: ast.Call) -> None:
        """AIML061: Detect missing output sanitization on LLM responses."""
        if not self.has_llm_framework:
            return
        
        # Check for LLM API calls that produce responses
        func_name = ""
        if isinstance(node.func, ast.Attribute):
            func_name = node.func.attr
        elif isinstance(node.func, ast.Name):
            func_name = node.func.id
        
        # Check for common LLM response methods
        llm_response_methods = ["create", "complete", "chat", "generate", "invoke", "call"]
        if any(method in func_name.lower() for method in llm_response_methods):
            # Flag if output is not being sanitized (heuristic check)
            violation = RuleViolation(
                rule_id="AIML061",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.HIGH,
                message="Missing output sanitization - ensure LLM response is validated before use",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="LLM02",
                cwe_id="CWE-20",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_code_execution_in_response(self, node: ast.Call) -> None:
        """AIML062: Detect code execution risks in LLM responses."""
        if not self.has_llm_framework:
            return
        
        # Check for dangerous execution functions that might use LLM output
        dangerous_funcs = ["exec", "eval", "compile", "__import__"]
        
        func_name = ""
        if isinstance(node.func, ast.Name):
            func_name = node.func.id
        elif isinstance(node.func, ast.Attribute):
            func_name = node.func.attr
        
        if func_name in dangerous_funcs:
            # Check if any argument looks like it could come from LLM
            for arg in node.args:
                if isinstance(arg, (ast.Name, ast.Attribute, ast.Subscript)):
                    violation = RuleViolation(
                        rule_id="AIML062",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.CRITICAL,
                        message="Code execution on LLM response - extreme arbitrary code execution risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM02",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
                    break
    
    def _check_sql_injection_in_generated(self, node: ast.Call) -> None:
        """AIML063: Detect SQL injection risks via LLM-generated queries."""
        if not self.has_llm_framework:
            return
        
        # Check for SQL execution functions
        sql_funcs = ["execute", "executemany", "raw", "exec_driver_sql"]
        
        func_name = ""
        if isinstance(node.func, ast.Attribute):
            func_name = node.func.attr
        
        if func_name in sql_funcs:
            # Check if argument could be from LLM-generated content
            for arg in node.args:
                if not isinstance(arg, ast.Constant):
                    violation = RuleViolation(
                        rule_id="AIML063",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.CRITICAL,
                        message="SQL execution with dynamic query - SQL injection risk if using LLM-generated content",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM02",
                        cwe_id="CWE-89",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
                    break
    
    def _check_xss_in_generated_html(self, node: ast.Call) -> None:
        """AIML064: Detect XSS risks via LLM-generated HTML."""
        if not self.has_llm_framework:
            return
        
        # Check for HTML rendering functions
        html_funcs = ["render", "render_template", "render_to_string", "mark_safe", "Markup"]
        
        func_name = ""
        if isinstance(node.func, ast.Name):
            func_name = node.func.id
        elif isinstance(node.func, ast.Attribute):
            func_name = node.func.attr
        
        if any(html_func in func_name for html_func in html_funcs):
            # Check if rendering dynamic content
            for arg in node.args:
                if not isinstance(arg, ast.Constant):
                    violation = RuleViolation(
                        rule_id="AIML064",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="HTML rendering with dynamic content - XSS risk if using LLM-generated HTML",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM02",
                        cwe_id="CWE-79",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
                    break
    
    def _check_command_injection_in_generated(self, node: ast.Call) -> None:
        """AIML065: Detect command injection risks via LLM-generated shell scripts."""
        if not self.has_llm_framework:
            return
        
        # Check for shell command execution
        shell_funcs = ["system", "popen", "run", "call", "check_output", "Popen"]
        
        func_name = ""
        if isinstance(node.func, ast.Name):
            func_name = node.func.id
        elif isinstance(node.func, ast.Attribute):
            func_name = node.func.attr
        
        if func_name in shell_funcs:
            # Check if using dynamic commands
            for arg in node.args:
                if not isinstance(arg, ast.Constant):
                    violation = RuleViolation(
                        rule_id="AIML065",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.CRITICAL,
                        message="Shell command with dynamic input - command injection risk if using LLM-generated scripts",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM02",
                        cwe_id="CWE-78",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
                    break
    
    def _check_path_traversal_in_generated(self, node: ast.Call) -> None:
        """AIML066: Detect path traversal risks in LLM-generated file paths."""
        if not self.has_llm_framework:
            return
        
        # Check for file operations
        file_funcs = ["open", "read", "write", "remove", "unlink", "rmdir"]
        
        func_name = ""
        if isinstance(node.func, ast.Name):
            func_name = node.func.id
        elif isinstance(node.func, ast.Attribute):
            func_name = node.func.attr
        
        if func_name in file_funcs:
            # Check if using dynamic file paths
            for arg in node.args:
                if not isinstance(arg, ast.Constant):
                    violation = RuleViolation(
                        rule_id="AIML066",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="File operation with dynamic path - path traversal risk if using LLM-generated paths",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM02",
                        cwe_id="CWE-22",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
                    break
    
    def _check_arbitrary_file_access_in_generated(self, node: ast.Call) -> None:
        """AIML067: Detect arbitrary file access risks via LLM-generated code."""
        if not self.has_llm_framework:
            return
        
        # Check for file system access patterns
        import_funcs = ["__import__", "importlib.import_module"]
        
        func_name = ""
        if isinstance(node.func, ast.Name):
            func_name = node.func.id
        elif isinstance(node.func, ast.Attribute):
            func_str = ""
            if hasattr(node.func.value, 'id'):
                func_str = f"{node.func.value.id}.{node.func.attr}"
            else:
                func_str = node.func.attr
            func_name = func_str
        
        if any(imp in func_name for imp in import_funcs):
            # Check if importing dynamic modules
            for arg in node.args:
                if not isinstance(arg, ast.Constant):
                    violation = RuleViolation(
                        rule_id="AIML067",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.CRITICAL,
                        message="Dynamic module import - arbitrary file access risk if using LLM-generated code",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM02",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
                    break
    
    def _check_sensitive_data_leakage(self, node: ast.Call) -> None:
        """AIML068: Detect sensitive data leakage in LLM responses."""
        if not self.has_llm_framework:
            return
        
        # Check for logging or output of LLM responses
        log_funcs = ["print", "log", "debug", "info", "warning", "error", "write"]
        
        func_name = ""
        if isinstance(node.func, ast.Name):
            func_name = node.func.id
        elif isinstance(node.func, ast.Attribute):
            func_name = node.func.attr
        
        if func_name in log_funcs:
            # Check if logging dynamic content that could be LLM response
            for arg in node.args:
                if isinstance(arg, (ast.Name, ast.Attribute, ast.Subscript)):
                    # Look for response-like variable names
                    var_name = ""
                    if isinstance(arg, ast.Name):
                        var_name = arg.id
                    
                    if any(term in var_name.lower() for term in ["response", "completion", "result", "output", "answer"]):
                        violation = RuleViolation(
                            rule_id="AIML068",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.MEDIUM,
                            message="Logging LLM response - sensitive data leakage risk",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="LLM06",
                            cwe_id="CWE-532",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
                        break
    
    def _check_pii_disclosure(self, node: ast.Call) -> None:
        """AIML069: Detect PII disclosure risk from training data."""
        if not self.has_llm_framework:
            return
        
        # Check for LLM API calls that might expose training data
        func_name = ""
        if isinstance(node.func, ast.Attribute):
            func_name = node.func.attr
        elif isinstance(node.func, ast.Name):
            func_name = node.func.id
        
        # Check for completion/chat calls
        llm_funcs = ["create", "complete", "chat", "generate"]
        if any(func in func_name.lower() for func in llm_funcs):
            # Check for high temperature (more likely to leak training data)
            for keyword in node.keywords:
                if keyword.arg == "temperature":
                    if isinstance(keyword.value, ast.Constant):
                        if keyword.value.value > 1.5:
                            violation = RuleViolation(
                                rule_id="AIML069",
                                category=RuleCategory.SECURITY,
                                severity=RuleSeverity.MEDIUM,
                                message="High temperature setting - increased PII disclosure risk from training data",
                                line_number=node.lineno,
                                column=node.col_offset,
                                end_line_number=getattr(node, "end_lineno", node.lineno),
                                end_column=getattr(node, "end_col_offset", node.col_offset),
                                file_path=str(self.file_path),
                                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                                fix_applicability=FixApplicability.SAFE,
                                fix_data=None,
                                owasp_id="LLM06",
                                cwe_id="CWE-359",
                                source_tool="pyguard",
                            )
                            self.violations.append(violation)
    
    def _check_copyright_violation_risk(self, node: ast.Call) -> None:
        """AIML070: Detect copyright violation risks from memorized content."""
        if not self.has_llm_framework:
            return
        
        # Check for LLM API calls that might generate copyrighted content
        func_name = ""
        if isinstance(node.func, ast.Attribute):
            func_name = node.func.attr
        elif isinstance(node.func, ast.Name):
            func_name = node.func.id
        
        # Check for completion/chat calls with long outputs
        llm_funcs = ["create", "complete", "chat", "generate"]
        if any(func in func_name.lower() for func in llm_funcs):
            # Check for high max_tokens (more likely to generate long copyrighted text)
            for keyword in node.keywords:
                if keyword.arg in ["max_tokens", "max_length"]:
                    if isinstance(keyword.value, ast.Constant):
                        if keyword.value.value > 2000:
                            violation = RuleViolation(
                                rule_id="AIML070",
                                category=RuleCategory.CONVENTION,
                                severity=RuleSeverity.LOW,
                                message="High max_tokens setting - copyright violation risk from memorized content",
                                line_number=node.lineno,
                                column=node.col_offset,
                                end_line_number=getattr(node, "end_lineno", node.lineno),
                                end_column=getattr(node, "end_col_offset", node.col_offset),
                                file_path=str(self.file_path),
                                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                                fix_applicability=FixApplicability.MANUAL,
                                fix_data=None,
                                owasp_id="LLM06",
                                cwe_id="CWE-1059",
                                source_tool="pyguard",
                            )
                            self.violations.append(violation)
    
    # Phase 1.2: Model Serialization & Loading (40 checks)
    # Phase 1.2.1: PyTorch Model Security (15 checks - AIML071-AIML085)
    
    def _check_torch_load_unsafe(self, node: ast.Call) -> None:
        """AIML071: Detect torch.load() without weights_only=True."""
        if not self.has_pytorch:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr == "load":
            # Check if this is torch.load
            if hasattr(node.func.value, 'id') and node.func.value.id == "torch":
                # Check for weights_only parameter
                has_weights_only = any(
                    kw.arg == "weights_only" and isinstance(kw.value, ast.Constant) and kw.value.value is True
                    for kw in node.keywords
                )
                
                if not has_weights_only:
                    violation = RuleViolation(
                        rule_id="AIML071",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.CRITICAL,
                        message="torch.load() without weights_only=True - arbitrary code execution risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML05",
                        cwe_id="CWE-502",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_torch_pickle_unsafe(self, node: ast.Call) -> None:
        """AIML072: Detect unsafe pickle in torch.save/load."""
        if not self.has_pytorch:
            return
        
        # Check for pickle.load with torch models
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["save", "load"]:
            if hasattr(node.func.value, 'id') and node.func.value.id in ["torch", "pickle"]:
                violation = RuleViolation(
                    rule_id="AIML072",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Unsafe pickle usage with PyTorch - use safetensors or weights_only=True",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-502",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_missing_model_integrity(self, node: ast.Call) -> None:
        """AIML073: Detect missing model integrity verification."""
        if not (self.has_pytorch or self.has_tensorflow):
            return
        
        # Check for model loading without checksum verification
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["load", "load_model", "from_pretrained"]:
            # Look for missing hash/checksum parameters
            has_verification = any(
                kw.arg in ["sha256", "checksum", "hash", "revision", "etag"]
                for kw in node.keywords
            )
            
            if not has_verification:
                violation = RuleViolation(
                    rule_id="AIML073",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Model loading without integrity verification - use checksums or revisions",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-494",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_untrusted_model_url(self, node: ast.Call) -> None:
        """AIML074: Detect untrusted model URL loading."""
        if not (self.has_pytorch or self.has_tensorflow or self.has_transformers):
            return
        
        # Check for URL-based model loading
        for arg in node.args:
            if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                if arg.value.startswith(("http://", "https://", "ftp://")):
                    violation = RuleViolation(
                        rule_id="AIML074",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Loading model from URL - supply chain attack risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML05",
                        cwe_id="CWE-494",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
                    break
    
    def _check_state_dict_poisoning(self, node: ast.Call) -> None:
        """AIML075: Detect model poisoning in state_dict."""
        if not self.has_pytorch:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr == "load_state_dict":
            violation = RuleViolation(
                rule_id="AIML075",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.MEDIUM,
                message="load_state_dict() - validate state dict to prevent model poisoning",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML05",
                cwe_id="CWE-345",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_custom_module_injection(self, node: ast.Call) -> None:
        """AIML076: Detect custom layer/module injection."""
        if not (self.has_pytorch or self.has_tensorflow):
            return
        
        # Check for custom modules/layers being loaded
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["register_module", "add_module", "register_buffer"]:
            violation = RuleViolation(
                rule_id="AIML076",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.HIGH,
                message="Custom module registration - validate to prevent code injection",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML05",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_torch_jit_unsafe(self, node: ast.Call) -> None:
        """AIML077: Detect unsafe torch.jit.load()."""
        if not self.has_pytorch:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr == "load":
            # Check if this is torch.jit.load
            if (hasattr(node.func.value, 'attr') and node.func.value.attr == "jit" and
                hasattr(node.func.value.value, 'id') and node.func.value.value.id == "torch"):
                violation = RuleViolation(
                    rule_id="AIML077",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.CRITICAL,
                    message="torch.jit.load() - arbitrary code execution risk via TorchScript",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-502",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_torchscript_deserialization(self, node: ast.Call) -> None:
        """AIML078: Detect TorchScript deserialization risks."""
        if not self.has_pytorch:
            return
        
        # Check for TorchScript operations
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["script", "trace", "load"]:
            if hasattr(node.func.value, 'attr') and node.func.value.attr == "jit":
                violation = RuleViolation(
                    rule_id="AIML078",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="TorchScript deserialization - validate input to prevent attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-502",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_onnx_tampering(self, node: ast.Call) -> None:
        """AIML079: Detect ONNX model tampering."""
        if not (self.has_pytorch or self.has_tensorflow):
            return
        
        # Check for ONNX model loading
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["load", "load_model"]:
            for arg in node.args:
                if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                    if arg.value.endswith(".onnx"):
                        violation = RuleViolation(
                            rule_id="AIML079",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.MEDIUM,
                            message="ONNX model loading - verify model integrity to prevent tampering",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="ML05",
                            cwe_id="CWE-494",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
                        break
    
    def _check_model_metadata_injection(self, node: ast.Call) -> None:
        """AIML080: Detect model metadata injection."""
        if not (self.has_pytorch or self.has_tensorflow or self.has_transformers):
            return
        
        # Check for metadata loading
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["load_config", "config", "metadata"]:
            violation = RuleViolation(
                rule_id="AIML080",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.MEDIUM,
                message="Model metadata loading - validate to prevent injection attacks",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML05",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_missing_gpu_limits(self, node: ast.Call) -> None:
        """AIML081: Detect missing GPU memory limits."""
        if not (self.has_pytorch or self.has_tensorflow):
            return
        
        # Check for GPU operations without memory limits
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["cuda", "to"]:
            # Check if device is being set to GPU without memory limits
            has_memory_limit = any(
                kw.arg in ["max_memory", "device_map", "max_split_size_mb"]
                for kw in node.keywords
            )
            
            if not has_memory_limit:
                violation = RuleViolation(
                    rule_id="AIML081",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="GPU usage without memory limits - resource exhaustion risk",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-400",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_tensor_size_attacks(self, node: ast.Call) -> None:
        """AIML082: Detect tensor size attacks (memory exhaustion)."""
        if not (self.has_pytorch or self.has_tensorflow):
            return
        
        # Check for tensor operations with user-controlled sizes
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["zeros", "ones", "empty", "randn", "rand"]:
            # Check if size is from user input (not a constant)
            for arg in node.args:
                if not isinstance(arg, ast.Constant):
                    violation = RuleViolation(
                        rule_id="AIML082",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Tensor creation with dynamic size - memory exhaustion risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML09",
                        cwe_id="CWE-400",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
                    break
    
    def _check_quantization_vulnerabilities(self, node: ast.Call) -> None:
        """AIML083: Detect quantization vulnerabilities."""
        if not (self.has_pytorch or self.has_tensorflow):
            return
        
        # Check for quantization operations
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["quantize", "quantize_dynamic", "quantize_per_tensor"]:
            violation = RuleViolation(
                rule_id="AIML083",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.LOW,
                message="Model quantization - validate to prevent accuracy degradation attacks",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML05",
                cwe_id="CWE-345",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_mixed_precision_attacks(self, node: ast.Call) -> None:
        """AIML084: Detect mixed precision attacks."""
        if not (self.has_pytorch or self.has_tensorflow):
            return
        
        # Check for mixed precision training/inference
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["autocast", "amp"]:
            violation = RuleViolation(
                rule_id="AIML084",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.LOW,
                message="Mixed precision mode - validate precision settings to prevent attacks",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML05",
                cwe_id="CWE-345",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_model_zoo_trust(self, node: ast.Call) -> None:
        """AIML085: Detect model zoo trust verification."""
        if not (self.has_pytorch or self.has_tensorflow):
            return
        
        # Check for model zoo downloads
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["load_state_dict_from_url", "hub.load"]:
            violation = RuleViolation(
                rule_id="AIML085",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.MEDIUM,
                message="Model zoo download - verify model source and integrity",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="ML05",
                cwe_id="CWE-494",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    # Phase 1.2.2: TensorFlow/Keras Security (15 checks - AIML086-AIML100)
    
    def _check_savedmodel_unsafe(self, node: ast.Call) -> None:
        """AIML086: Detect SavedModel arbitrary code execution."""
        if not self.has_tensorflow:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["load_model", "load"]:
            # Check for TensorFlow SavedModel loading
            for arg in node.args:
                if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                    violation = RuleViolation(
                        rule_id="AIML086",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.CRITICAL,
                        message="TensorFlow SavedModel loading - arbitrary code execution risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML05",
                        cwe_id="CWE-502",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
                    break
    
    def _check_hdf5_deserialization(self, node: ast.Call) -> None:
        """AIML087: Detect HDF5 deserialization attacks."""
        if not self.has_tensorflow:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr == "load":
            # Check for .h5 or .hdf5 files
            for arg in node.args:
                if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                    if arg.value.endswith((".h5", ".hdf5", ".keras")):
                        violation = RuleViolation(
                            rule_id="AIML087",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.HIGH,
                            message="HDF5/Keras model loading - deserialization attack risk",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="ML05",
                            cwe_id="CWE-502",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
                        break
    
    def _check_keras_custom_object_injection(self, node: ast.Call) -> None:
        """AIML088: Detect custom object injection in model.load."""
        if not self.has_tensorflow:
            return
        
        # Check for custom_objects parameter
        for keyword in node.keywords:
            if keyword.arg == "custom_objects":
                violation = RuleViolation(
                    rule_id="AIML088",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Custom objects in Keras model - validate to prevent code injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_tf_hub_trust(self, node: ast.Call) -> None:
        """AIML089: Detect TensorFlow Hub model trust."""
        if not self.has_tensorflow:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["load", "KerasLayer"]:
            # Check if loading from TF Hub
            for arg in node.args:
                if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                    if "tfhub.dev" in arg.value or "hub" in arg.value:
                        violation = RuleViolation(
                            rule_id="AIML089",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.MEDIUM,
                            message="TensorFlow Hub model loading - verify model source and integrity",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="ML05",
                            cwe_id="CWE-494",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
                        break
    
    def _check_graph_execution_injection(self, node: ast.Call) -> None:
        """AIML090: Detect graph execution injection."""
        if not self.has_tensorflow:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["graph", "GraphDef", "import_graph_def"]:
            violation = RuleViolation(
                rule_id="AIML090",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.HIGH,
                message="TensorFlow graph operation - validate to prevent injection",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML05",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_checkpoint_poisoning(self, node: ast.Call) -> None:
        """AIML091: Detect checkpoint poisoning."""
        if not (self.has_tensorflow or self.has_pytorch):
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["restore", "load_checkpoint", "from_checkpoint"]:
            violation = RuleViolation(
                rule_id="AIML091",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.MEDIUM,
                message="Checkpoint loading - verify integrity to prevent poisoning",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="ML05",
                cwe_id="CWE-345",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_keras_lambda_injection(self, node: ast.Call) -> None:
        """AIML092: Detect Keras Lambda layer code injection."""
        if not self.has_tensorflow:
            return
        
        if isinstance(node.func, ast.Name) and node.func.id == "Lambda":
            violation = RuleViolation(
                rule_id="AIML092",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.HIGH,
                message="Keras Lambda layer - code injection risk with untrusted input",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML05",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_custom_metric_tampering(self, node: ast.Call) -> None:
        """AIML093: Detect custom metric/loss function tampering."""
        if not self.has_tensorflow:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["compile"]:
            # Check for custom metrics or loss functions
            for keyword in node.keywords:
                if keyword.arg in ["metrics", "loss"]:
                    if not isinstance(keyword.value, (ast.Constant, ast.List)):
                        violation = RuleViolation(
                            rule_id="AIML093",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.MEDIUM,
                            message="Custom metrics/loss functions - validate to prevent tampering",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.MANUAL,
                            fix_data=None,
                            owasp_id="ML05",
                            cwe_id="CWE-345",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
                        break
    
    def _check_tflite_manipulation(self, node: ast.Call) -> None:
        """AIML094: Detect TF Lite model manipulation."""
        if not self.has_tensorflow:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["Interpreter", "load_model"]:
            # Check for .tflite files
            for arg in node.args:
                if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                    if arg.value.endswith(".tflite"):
                        violation = RuleViolation(
                            rule_id="AIML094",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.MEDIUM,
                            message="TF Lite model loading - verify integrity to prevent manipulation",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="ML05",
                            cwe_id="CWE-494",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
                        break
    
    def _check_tensorboard_injection(self, node: ast.Call) -> None:
        """AIML095: Detect TensorBoard log injection."""
        if not self.has_tensorflow:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["SummaryWriter", "FileWriter"]:
            violation = RuleViolation(
                rule_id="AIML095",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.LOW,
                message="TensorBoard logging - sanitize data to prevent log injection",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML05",
                cwe_id="CWE-117",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_tf_serving_vulnerabilities(self, node: ast.Call) -> None:
        """AIML096: Detect model serving vulnerabilities (TF Serving)."""
        if not self.has_tensorflow:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["export_saved_model", "export"]:
            violation = RuleViolation(
                rule_id="AIML096",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.MEDIUM,
                message="Model export for serving - ensure proper access controls",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML09",
                cwe_id="CWE-285",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_graphdef_manipulation(self, node: ast.Call) -> None:
        """AIML097: Detect GraphDef manipulation."""
        if not self.has_tensorflow:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["ParseFromString", "import_graph_def"]:
            violation = RuleViolation(
                rule_id="AIML097",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.HIGH,
                message="GraphDef parsing - validate to prevent manipulation",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML05",
                cwe_id="CWE-502",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_operation_injection(self, node: ast.Call) -> None:
        """AIML098: Detect operation injection attacks."""
        if not self.has_tensorflow:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["add_op", "register_op"]:
            violation = RuleViolation(
                rule_id="AIML098",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.HIGH,
                message="Operation registration - validate to prevent injection",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML05",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_resource_exhaustion_model(self, node: ast.Call) -> None:
        """AIML099: Detect resource exhaustion via model architecture."""
        if not (self.has_tensorflow or self.has_pytorch):
            return
        
        # Check for operations that could cause resource exhaustion
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["Sequential", "Model"]:
            violation = RuleViolation(
                rule_id="AIML099",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.MEDIUM,
                message="Model architecture creation - validate complexity to prevent DoS",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML09",
                cwe_id="CWE-400",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_tfrecord_poisoning(self, node: ast.Call) -> None:
        """AIML100: Detect TFRecord poisoning."""
        if not self.has_tensorflow:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["TFRecordReader", "TFRecordDataset"]:
            violation = RuleViolation(
                rule_id="AIML100",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.MEDIUM,
                message="TFRecord loading - validate data to prevent poisoning",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML05",
                cwe_id="CWE-345",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    # Phase 1.2.3: Hugging Face & Transformers (10 checks - AIML101-AIML110)
    
    def _check_from_pretrained_trust(self, node: ast.Call) -> None:
        """AIML101: Detect from_pretrained() trust issues."""
        if not self.has_transformers:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr == "from_pretrained":
            # Check for trust_remote_code parameter
            has_trust_param = any(
                kw.arg == "trust_remote_code" and isinstance(kw.value, ast.Constant) and kw.value.value is False
                for kw in node.keywords
            )
            
            if not has_trust_param:
                violation = RuleViolation(
                    rule_id="AIML101",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.CRITICAL,
                    message="from_pretrained() without trust_remote_code=False - arbitrary code execution risk",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_model_card_credentials(self, node: ast.Call) -> None:
        """AIML102: Detect model card credential leakage."""
        if not self.has_transformers:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["push_to_hub", "create_model_card"]:
            # Check for token parameter
            for keyword in node.keywords:
                if keyword.arg in ["token", "use_auth_token"]:
                    if isinstance(keyword.value, ast.Constant):
                        violation = RuleViolation(
                            rule_id="AIML102",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.HIGH,
                            message="Hardcoded token in model card - credential leakage risk",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="ML05",
                            cwe_id="CWE-798",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
                        break
    
    def _check_tokenizer_vulnerabilities(self, node: ast.Call) -> None:
        """AIML103: Detect tokenizer vulnerabilities."""
        if not self.has_transformers:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["tokenize", "encode", "batch_encode"]:
            # Check for truncation/max_length parameters
            has_limits = any(
                kw.arg in ["max_length", "truncation"]
                for kw in node.keywords
            )
            
            if not has_limits:
                violation = RuleViolation(
                    rule_id="AIML103",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Tokenizer without limits - DoS risk from long inputs",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-400",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_pipeline_injection(self, node: ast.Call) -> None:
        """AIML104: Detect pipeline injection attacks."""
        if not self.has_transformers:
            return
        
        if isinstance(node.func, ast.Name) and node.func.id == "pipeline":
            violation = RuleViolation(
                rule_id="AIML104",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.MEDIUM,
                message="Transformers pipeline - validate task and model to prevent injection",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML05",
                cwe_id="CWE-94",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_hf_dataset_poisoning(self, node: ast.Call) -> None:
        """AIML105: Detect dataset poisoning (Hugging Face Datasets)."""
        if not self.has_transformers:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["load_dataset", "load_from_disk"]:
            violation = RuleViolation(
                rule_id="AIML105",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.MEDIUM,
                message="Dataset loading - validate source to prevent poisoning",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML05",
                cwe_id="CWE-345",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_missing_model_signature(self, node: ast.Call) -> None:
        """AIML106: Detect missing model signature verification."""
        if not self.has_transformers:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr == "from_pretrained":
            # Check for revision or commit hash
            has_version = any(
                kw.arg in ["revision", "commit_hash"]
                for kw in node.keywords
            )
            
            if not has_version:
                violation = RuleViolation(
                    rule_id="AIML106",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Model loading without version pinning - supply chain attack risk",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-494",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_arbitrary_file_in_config(self, node: ast.Call) -> None:
        """AIML107: Detect arbitrary file loading in model config."""
        if not self.has_transformers:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["from_json_file", "from_dict"]:
            violation = RuleViolation(
                rule_id="AIML107",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.HIGH,
                message="Config loading from file - arbitrary file access risk",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML05",
                cwe_id="CWE-22",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_space_app_injection(self, node: ast.Call) -> None:
        """AIML108: Detect Space app injection (Gradio/Streamlit)."""
        if not self.has_transformers:
            return
        
        # Check for Gradio/Streamlit interfaces
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["Interface", "launch"]:
            violation = RuleViolation(
                rule_id="AIML108",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.MEDIUM,
                message="Gradio/Streamlit interface - validate inputs to prevent injection",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML09",
                cwe_id="CWE-20",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_model_repo_tampering(self, node: ast.Call) -> None:
        """AIML109: Detect model repository tampering."""
        if not self.has_transformers:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr in ["clone_from", "Repository"]:
            violation = RuleViolation(
                rule_id="AIML109",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.MEDIUM,
                message="Model repository cloning - verify source to prevent tampering",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="ML05",
                cwe_id="CWE-494",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_private_model_access(self, node: ast.Call) -> None:
        """AIML110: Detect private model access control."""
        if not self.has_transformers:
            return
        
        if isinstance(node.func, ast.Attribute) and node.func.attr == "from_pretrained":
            # Check for private models without proper authentication
            for arg in node.args:
                if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                    # Look for model names that might be private
                    if "/" in arg.value:  # org/model format
                        has_auth = any(
                            kw.arg in ["token", "use_auth_token"]
                            for kw in node.keywords
                        )
                        
                        if not has_auth:
                            violation = RuleViolation(
                                rule_id="AIML110",
                                category=RuleCategory.SECURITY,
                                severity=RuleSeverity.LOW,
                                message="Model loading without authentication - consider access controls",
                                line_number=node.lineno,
                                column=node.col_offset,
                                end_line_number=getattr(node, "end_lineno", node.lineno),
                                end_column=getattr(node, "end_col_offset", node.col_offset),
                                file_path=str(self.file_path),
                                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                                fix_applicability=FixApplicability.MANUAL,
                                fix_data=None,
                                owasp_id="ML05",
                                cwe_id="CWE-285",
                                source_tool="pyguard",
                            )
                            self.violations.append(violation)
                        break

    # Phase 1.3: Training & Fine-Tuning Security (30 checks)
    # Phase 1.3.1: Training Data Security (12 checks - AIML111-AIML122)
    
    def _check_unvalidated_training_data(self, node: ast.Call) -> None:
        """AIML111: Detect unvalidated training data sources."""
        if not self.has_ml_framework:
            return
        
        # Check for data loading from URLs or files without validation
        data_loaders = ["load_dataset", "read_csv", "from_csv", "ImageFolder", "DataLoader"]
        
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if func_name in data_loaders:
                # Check if validation is mentioned anywhere
                has_validation = any(
                    kw.arg in ["trust", "verify", "validate", "safe_mode"]
                    for kw in node.keywords
                )
                
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML111",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Training data from unvalidated source - data poisoning risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_missing_data_sanitization(self, node: ast.Call) -> None:
        """AIML112: Detect missing data sanitization."""
        if not self.has_ml_framework:
            return
        
        # Check for text/string data loading without sanitization
        text_loaders = ["read_text", "load_dataset", "read_csv", "from_pandas"]
        
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if func_name in text_loaders:
                has_sanitization = any(
                    kw.arg in ["clean", "sanitize", "strip", "preprocess"]
                    for kw in node.keywords
                )
                
                if not has_sanitization:
                    violation = RuleViolation(
                        rule_id="AIML112",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Training data without sanitization - injection risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_pii_in_training_data(self, node: ast.Call) -> None:
        """AIML113: Detect PII leakage in training datasets."""
        if not self.has_ml_framework:
            return
        
        # Check for training with user data or sensitive sources
        sensitive_sources = ["user_data", "customers", "patients", "employees", "personal"]
        
        for arg in node.args:
            if isinstance(arg, (ast.Name, ast.Constant)):
                value = arg.id if isinstance(arg, ast.Name) else str(arg.value).lower()
                if any(source in value.lower() for source in sensitive_sources):
                    violation = RuleViolation(
                        rule_id="AIML113",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="PII in training data - privacy violation risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML06",
                        cwe_id="CWE-359",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
                    break

    def _check_copyright_infringing_data(self, node: ast.Call) -> None:
        """AIML114: Detect copyright-infringing data inclusion."""
        if not self.has_ml_framework:
            return
        
        # Check for web scraping without copyright consideration
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if func_name in ["scrape", "crawl", "fetch_web", "download"]:
                violation = RuleViolation(
                    rule_id="AIML114",
                    category=RuleCategory.CONVENTION,
                    severity=RuleSeverity.MEDIUM,
                    message="Training data may include copyrighted content - legal risk",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-1059",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_label_flipping_detection(self, node: ast.Call) -> None:
        """AIML115: Detect label flipping attacks."""
        if not self.has_ml_framework:
            return
        
        # Check for training without label validation
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["fit", "train", "fit_transform"]:
                has_validation = any(
                    kw.arg in ["validate_labels", "check_labels", "verify_labels"]
                    for kw in node.keywords
                )
                
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML115",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Training without label validation - label flipping attack risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_backdoor_in_dataset(self, node: ast.Call) -> None:
        """AIML116: Detect backdoor injection in datasets."""
        if not self.has_ml_framework:
            return
        
        # Check for dataset loading without backdoor detection
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if func_name in ["load_dataset", "ImageFolder", "DataLoader"]:
                has_detection = any(
                    kw.arg in ["scan_backdoor", "detect_triggers", "verify_clean"]
                    for kw in node.keywords
                )
                
                if not has_detection:
                    violation = RuleViolation(
                        rule_id="AIML116",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.CRITICAL,
                        message="Dataset without backdoor detection - hidden trigger risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-912",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_trigger_pattern_insertion(self, node: ast.Call) -> None:
        """AIML117: Detect trigger pattern insertion."""
        if not self.has_ml_framework:
            return
        
        # Check for data augmentation without trigger validation
        augmentation_funcs = ["RandomCrop", "RandomFlip", "ColorJitter", "augment", "transform"]
        
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if func_name in augmentation_funcs:
                violation = RuleViolation(
                    rule_id="AIML117",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Data augmentation without validation - trigger pattern risk",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_data_augmentation_attacks(self, node: ast.Call) -> None:
        """AIML118: Detect data augmentation attacks."""
        if not self.has_ml_framework:
            return
        
        # Check for complex augmentation pipelines
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if "Compose" in func_name or "Pipeline" in func_name:
                violation = RuleViolation(
                    rule_id="AIML118",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Data augmentation - validate transformations to prevent poisoning",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_synthetic_data_vulnerabilities(self, node: ast.Call) -> None:
        """AIML119: Detect synthetic data vulnerabilities."""
        if not self.has_ml_framework:
            return
        
        # Check for synthetic data generation
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if "generate" in func_name.lower() or "synthesize" in func_name.lower():
                violation = RuleViolation(
                    rule_id="AIML119",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Synthetic data generation - validate quality and safety",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_web_scraping_data_risks(self, node: ast.Call) -> None:
        """AIML120: Detect web scraping data risks."""
        if not self.has_ml_framework:
            return
        
        # Check for web scraping functions
        scraping_funcs = ["scrape", "crawl", "BeautifulSoup", "requests.get", "urllib"]
        
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if any(func in func_name for func in scraping_funcs):
                violation = RuleViolation(
                    rule_id="AIML120",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Web scraped data - validate and sanitize to prevent poisoning",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_user_generated_content_risks(self, node: ast.Call) -> None:
        """AIML121: Detect user-generated content risks."""
        if not self.has_ml_framework:
            return
        
        # Check for loading user content for training
        for arg in node.args:
            if isinstance(arg, (ast.Name, ast.Constant)):
                value = arg.id if isinstance(arg, ast.Name) else str(arg.value).lower()
                if "user" in value or "upload" in value or "submission" in value:
                    violation = RuleViolation(
                        rule_id="AIML121",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="User-generated training data - validate to prevent poisoning",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
                    break

    def _check_missing_data_provenance(self, node: ast.Call) -> None:
        """AIML122: Detect missing data provenance tracking."""
        if not self.has_ml_framework:
            return
        
        # Check for data loading without provenance tracking
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if func_name in ["load_dataset", "read_csv", "from_csv"]:
                has_provenance = any(
                    kw.arg in ["source", "origin", "metadata", "provenance"]
                    for kw in node.keywords
                )
                
                if not has_provenance:
                    violation = RuleViolation(
                        rule_id="AIML122",
                        category=RuleCategory.CONVENTION,
                        severity=RuleSeverity.MEDIUM,
                        message="Training data without provenance - unable to verify integrity",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-778",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    # Phase 1.3.2: Training Process Security (10 checks - AIML123-AIML132)
    
    def _check_gradient_manipulation(self, node: ast.Call) -> None:
        """AIML123: Detect gradient manipulation attacks."""
        if not self.has_ml_framework:
            return
        
        # Check for gradient computation without validation
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["backward", "grad", "gradient"]:
                violation = RuleViolation(
                    rule_id="AIML123",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Gradient computation - validate to prevent manipulation attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_learning_rate_manipulation(self, node: ast.Call) -> None:
        """AIML124: Detect learning rate manipulation."""
        if not self.has_ml_framework:
            return
        
        # Check for dynamic learning rate changes without validation
        for keyword in node.keywords:
            if keyword.arg == "lr" or keyword.arg == "learning_rate":
                if isinstance(keyword.value, ast.Name):  # Variable, not constant
                    violation = RuleViolation(
                        rule_id="AIML124",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Dynamic learning rate without validation - manipulation risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
                    break

    def _check_optimizer_state_poisoning(self, node: ast.Call) -> None:
        """AIML125: Detect optimizer state poisoning."""
        if not self.has_ml_framework:
            return
        
        # Check for optimizer state loading
        if isinstance(node.func, ast.Attribute):
            if "load_state_dict" in node.func.attr and "optimizer" in self.lines[node.lineno - 1].lower():
                violation = RuleViolation(
                    rule_id="AIML125",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Optimizer state loading - verify integrity to prevent poisoning",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_checkpoint_tampering_training(self, node: ast.Call) -> None:
        """AIML126: Detect checkpoint tampering during training."""
        if not self.has_ml_framework:
            return
        
        # Check for checkpoint saving without integrity protection
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if "save" in func_name and "checkpoint" in self.lines[node.lineno - 1].lower():
                has_hash = any(
                    kw.arg in ["checksum", "hash", "integrity"]
                    for kw in node.keywords
                )
                
                if not has_hash:
                    violation = RuleViolation(
                        rule_id="AIML126",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Checkpoint saving without integrity checks - tampering risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML05",
                        cwe_id="CWE-494",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_early_stopping_bypass(self, node: ast.Call) -> None:
        """AIML127: Detect early stopping bypass."""
        if not self.has_ml_framework:
            return
        
        # Check for early stopping without validation monitoring
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if "EarlyStopping" in func_name:
                violation = RuleViolation(
                    rule_id="AIML127",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="Early stopping without validation monitoring - bypass risk",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_validation_set_poisoning(self, node: ast.Call) -> None:
        """AIML128: Detect validation set poisoning."""
        if not self.has_ml_framework:
            return
        
        # Check for validation data loading
        for keyword in node.keywords:
            if keyword.arg in ["validation_data", "val_data", "valid_data"]:
                violation = RuleViolation(
                    rule_id="AIML128",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Validation data from untrusted source - poisoning risk",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
                break

    def _check_tensorboard_logging_injection(self, node: ast.Call) -> None:
        """AIML129: Detect TensorBoard logging injection."""
        if not self.has_ml_framework:
            return
        
        # Check for TensorBoard logging
        if isinstance(node.func, ast.Attribute):
            if "SummaryWriter" in str(node.func) or "add_scalar" in node.func.attr:
                violation = RuleViolation(
                    rule_id="AIML129",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="TensorBoard logging - sanitize data to prevent injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-117",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_experiment_tracking_manipulation(self, node: ast.Call) -> None:
        """AIML130: Detect experiment tracking manipulation."""
        if not self.has_ml_framework:
            return
        
        # Check for experiment tracking without validation
        tracking_libs = ["mlflow", "wandb", "neptune", "comet"]
        
        if isinstance(node.func, ast.Attribute):
            if any(lib in str(node.func).lower() for lib in tracking_libs):
                if "log" in node.func.attr:
                    violation = RuleViolation(
                        rule_id="AIML130",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Experiment tracking - validate metrics to prevent manipulation",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_distributed_training_node_compromise(self, node: ast.Call) -> None:
        """AIML131: Detect distributed training node compromise."""
        if not self.has_ml_framework:
            return
        
        # Check for distributed training without secure communication
        distributed_funcs = ["DistributedDataParallel", "init_process_group", "all_reduce"]
        
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if any(func in func_name for func in distributed_funcs):
                violation = RuleViolation(
                    rule_id="AIML131",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.CRITICAL,
                    message="Distributed training - secure communication between nodes",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-300",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_parameter_server_vulnerabilities(self, node: ast.Call) -> None:
        """AIML132: Detect parameter server vulnerabilities."""
        if not self.has_ml_framework:
            return
        
        # Check for parameter server usage
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if "ParameterServer" in func_name or "parameter_server" in func_name:
                violation = RuleViolation(
                    rule_id="AIML132",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Parameter server - implement authentication and encryption",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-306",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    # Phase 1.3.3: Fine-Tuning Risks (8 checks - AIML133-AIML140)
    
    def _check_base_model_poisoning(self, node: ast.Call) -> None:
        """AIML133: Detect base model poisoning."""
        if not self.has_transformers:
            return
        
        # Check for fine-tuning from untrusted base model
        if isinstance(node.func, ast.Attribute):
            if "from_pretrained" in node.func.attr:
                has_verification = any(
                    kw.arg in ["revision", "trust_remote_code"]
                    for kw in node.keywords
                    if kw.arg == "trust_remote_code" and isinstance(kw.value, ast.Constant) and kw.value.value is False
                )
                
                if not has_verification:
                    violation = RuleViolation(
                        rule_id="AIML133",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Fine-tuning from untrusted base model - poisoning risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML05",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_fine_tuning_data_injection(self, node: ast.Call) -> None:
        """AIML134: Detect fine-tuning data injection."""
        if not self.has_ml_framework:
            return
        
        # Check for fine-tuning with unvalidated data
        if isinstance(node.func, ast.Attribute):
            if "train" in node.func.attr or "fit" in node.func.attr:
                # Look for fine-tuning context
                line_text = self.lines[node.lineno - 1].lower()
                if "fine" in line_text or "finetune" in line_text:
                    violation = RuleViolation(
                        rule_id="AIML134",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Fine-tuning data - validate to prevent injection attacks",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_catastrophic_forgetting_exploitation(self, node: ast.Call) -> None:
        """AIML135: Detect catastrophic forgetting exploitation."""
        if not self.has_ml_framework:
            return
        
        # Check for fine-tuning without forgetting protection
        if isinstance(node.func, ast.Attribute):
            if "train" in node.func.attr and "fine" in self.lines[node.lineno - 1].lower():
                has_protection = any(
                    kw.arg in ["preserve_weights", "elastic_weight_consolidation", "ewc"]
                    for kw in node.keywords
                )
                
                if not has_protection:
                    violation = RuleViolation(
                        rule_id="AIML135",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Fine-tuning without forgetting protection - exploitation risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_peft_attacks(self, node: ast.Call) -> None:
        """AIML136: Detect PEFT attacks."""
        if not self.has_transformers:
            return
        
        # Check for PEFT without validation
        peft_funcs = ["get_peft_model", "PeftModel", "prepare_model_for_kbit_training"]
        
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if any(func in func_name for func in peft_funcs):
                violation = RuleViolation(
                    rule_id="AIML136",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="PEFT without validation - parameter tampering risk",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_lora_poisoning(self, node: ast.Call) -> None:
        """AIML137: Detect LoRA poisoning."""
        if not self.has_transformers:
            return
        
        # Check for LoRA adapter loading
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if "LoraConfig" in func_name or "lora" in func_name.lower():
                violation = RuleViolation(
                    rule_id="AIML137",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="LoRA adapter - verify source to prevent poisoning",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_adapter_injection(self, node: ast.Call) -> None:
        """AIML138: Detect adapter injection."""
        if not self.has_transformers:
            return
        
        # Check for adapter loading
        adapter_funcs = ["add_adapter", "load_adapter", "set_adapter"]
        
        if isinstance(node.func, ast.Attribute):
            if any(func in node.func.attr for func in adapter_funcs):
                violation = RuleViolation(
                    rule_id="AIML138",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Adapter loading - validate to prevent malicious injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_prompt_tuning_manipulation(self, node: ast.Call) -> None:
        """AIML139: Detect prompt tuning manipulation."""
        if not self.has_transformers:
            return
        
        # Check for prompt tuning
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if "PromptTuning" in func_name or "prompt_tuning" in func_name.lower():
                violation = RuleViolation(
                    rule_id="AIML139",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Prompt tuning - validate prompts to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_instruction_fine_tuning_risks(self, node: ast.Call) -> None:
        """AIML140: Detect instruction fine-tuning risks."""
        if not self.has_transformers:
            return
        
        # Check for instruction fine-tuning
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "instruction" in line_text and ("train" in line_text or "fine" in line_text):
            violation = RuleViolation(
                rule_id="AIML140",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.MEDIUM,
                message="Instruction fine-tuning - validate data to prevent jailbreaks",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="LLM01",
                cwe_id="CWE-345",
                source_tool="pyguard",
            )
            self.violations.append(violation)

    # Phase 1.4: Adversarial ML & Model Robustness (20 checks)
    # Phase 1.4.1: Adversarial Input Detection (10 checks - AIML141-AIML150)
    
    def _check_missing_adversarial_defense(self, node: ast.Call) -> None:
        """AIML141: Detect missing adversarial defense."""
        if not self.has_ml_framework:
            return
        
        # Check for model inference without adversarial defense
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["predict", "forward", "__call__", "infer"]:
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                # Check for adversarial defense keywords
                has_defense = any(
                    keyword in line_text
                    for keyword in ["adversarial", "robust", "defense", "certify"]
                )
                
                if not has_defense:
                    violation = RuleViolation(
                        rule_id="AIML141",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Model inference without adversarial defense - attack vulnerability",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML04",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_no_fgsm_protection(self, node: ast.Call) -> None:
        """AIML142: Detect lack of FGSM protection."""
        if not self.has_ml_framework:
            return
        
        # Check for training without FGSM adversarial examples
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["fit", "train"]:
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "fgsm" not in line_text and "adversarial" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML142",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Model vulnerable to FGSM attacks - add adversarial training",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML04",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_pgd_vulnerability(self, node: ast.Call) -> None:
        """AIML143: Detect PGD vulnerability."""
        if not self.has_ml_framework:
            return
        
        # Similar to FGSM check, looking for PGD protection
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["fit", "train"]:
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "pgd" not in line_text and "projected" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML143",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Model vulnerable to PGD attacks - implement robust training",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML04",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_cw_attack_surface(self, node: ast.Call) -> None:
        """AIML144: Detect C&W attack surface."""
        if not self.has_ml_framework:
            return
        
        # Check for defense against C&W attacks
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if "distillation" in func_name.lower() or "defensive" in str(node).lower():
                violation = RuleViolation(
                    rule_id="AIML144",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Model vulnerable to C&W attacks - add defensive distillation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML04",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_deepfool_susceptibility(self, node: ast.Call) -> None:
        """AIML145: Detect DeepFool susceptibility."""
        if not self.has_ml_framework:
            return
        
        # Check for perturbation validation
        if isinstance(node.func, ast.Attribute):
            if node.func.attr == "predict":
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "perturbation" not in line_text and "validate" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML145",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Model vulnerable to DeepFool attacks - validate input perturbations",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML04",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_universal_adversarial_perturbations(self, node: ast.Call) -> None:
        """AIML146: Detect universal adversarial perturbations vulnerability."""
        if not self.has_ml_framework:
            return
        
        # Check for input validation against universal perturbations
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["predict", "forward"]:
                violation = RuleViolation(
                    rule_id="AIML146",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Model vulnerable to universal perturbations - add input validation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML04",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_black_box_attack_vulnerability(self, node: ast.Call) -> None:
        """AIML147: Detect black-box attack vulnerability."""
        if not self.has_ml_framework:
            return
        
        # Check for API endpoints exposing inference
        if isinstance(node.func, ast.Attribute):
            if "api" in str(node.func).lower() and "predict" in node.func.attr:
                violation = RuleViolation(
                    rule_id="AIML147",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Model API exposes inference - black-box attack risk",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML04",
                    cwe_id="CWE-200",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_transfer_attack_risks(self, node: ast.Call) -> None:
        """AIML148: Detect transfer attack risks."""
        if not self.has_ml_framework:
            return
        
        # Check for models similar to public architectures
        public_archs = ["resnet", "vgg", "inception", "mobilenet", "efficientnet"]
        
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = str(node.func).lower()
            
            if any(arch in func_name for arch in public_archs):
                violation = RuleViolation(
                    rule_id="AIML148",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Model architecture similar to public models - transfer attack risk",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML04",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_physical_adversarial_examples(self, node: ast.Call) -> None:
        """AIML149: Detect physical adversarial examples vulnerability."""
        if not self.has_ml_framework:
            return
        
        # Check for vision models without physical robustness
        vision_funcs = ["detect", "classify", "segment", "recognize"]
        
        if isinstance(node.func, ast.Attribute):
            if any(func in node.func.attr.lower() for func in vision_funcs):
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "camera" in line_text or "video" in line_text or "real" in line_text:
                    violation = RuleViolation(
                        rule_id="AIML149",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Vision model without physical robustness - real-world attack risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML04",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_adversarial_patch_detection_missing(self, node: ast.Call) -> None:
        """AIML150: Detect missing adversarial patch detection."""
        if not self.has_ml_framework:
            return
        
        # Check for object detection without patch detection
        if isinstance(node.func, ast.Attribute):
            if "detect" in node.func.attr.lower():
                violation = RuleViolation(
                    rule_id="AIML150",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Object detection without patch detection - adversarial sticker risk",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML04",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    # Phase 1.4.2: Model Robustness (10 checks - AIML151-AIML160)
    
    def _check_missing_adversarial_training(self, node: ast.Call) -> None:
        """AIML151: Detect missing adversarial training."""
        if not self.has_ml_framework:
            return
        
        # Check for training without adversarial examples
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["fit", "train"]:
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "adversarial" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML151",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Model trained without adversarial examples - weak robustness",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML04",
                        cwe_id="CWE-693",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_no_certified_defenses(self, node: ast.Call) -> None:
        """AIML152: Detect lack of certified defenses."""
        if not self.has_ml_framework:
            return
        
        # Check for certified defense mechanisms
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if "certify" not in func_name.lower() and "provable" not in str(node).lower():
                if isinstance(node.func, ast.Attribute) and node.func.attr in ["predict", "forward"]:
                    violation = RuleViolation(
                        rule_id="AIML152",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Model lacks certified robustness guarantees",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML04",
                        cwe_id="CWE-693",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_input_gradient_masking(self, node: ast.Call) -> None:
        """AIML153: Detect input gradient masking."""
        if not self.has_ml_framework:
            return
        
        # Check for gradient masking techniques
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "gradient" in line_text and ("mask" in line_text or "obfuscate" in line_text):
            violation = RuleViolation(
                rule_id="AIML153",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.LOW,
                message="Model uses gradient masking - false sense of security",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML04",
                cwe_id="CWE-693",
                source_tool="pyguard",
            )
            self.violations.append(violation)

    def _check_defensive_distillation_gaps(self, node: ast.Call) -> None:
        """AIML154: Detect defensive distillation gaps."""
        if not self.has_ml_framework:
            return
        
        # Check for defensive distillation usage
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if "distillation" in func_name.lower():
                violation = RuleViolation(
                    rule_id="AIML154",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Defensive distillation incomplete - C&W vulnerability",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML04",
                    cwe_id="CWE-693",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_ensemble_defenses_missing(self, node: ast.Call) -> None:
        """AIML155: Detect missing ensemble defenses."""
        if not self.has_ml_framework:
            return
        
        # Check for single model inference (no ensemble)
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["predict", "forward"]:
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "ensemble" not in line_text and "voting" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML155",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Single model inference - consider ensemble for robustness",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML04",
                        cwe_id="CWE-693",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_randomization_defense_gaps(self, node: ast.Call) -> None:
        """AIML156: Detect randomization defense gaps."""
        if not self.has_ml_framework:
            return
        
        # Check for randomization-only defense
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "random" in line_text and "defense" in line_text:
            violation = RuleViolation(
                rule_id="AIML156",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.LOW,
                message="Randomization defense weak - can be circumvented",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML04",
                cwe_id="CWE-693",
                source_tool="pyguard",
            )
            self.violations.append(violation)

    def _check_input_transformation_missing(self, node: ast.Call) -> None:
        """AIML157: Detect missing input transformation."""
        if not self.has_ml_framework:
            return
        
        # Check for preprocessing/transformation layers
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["predict", "forward"]:
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "transform" not in line_text and "preprocess" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML157",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="No input preprocessing defenses - add transformation layers",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML04",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_detection_mechanism_missing(self, node: ast.Call) -> None:
        """AIML158: Detect missing detection mechanism."""
        if not self.has_ml_framework:
            return
        
        # Check for adversarial detection layer
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["predict", "forward"]:
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "detect" not in line_text or "adversarial" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML158",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="No adversarial example detector - add detection layer",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML04",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_rejection_option_missing(self, node: ast.Call) -> None:
        """AIML159: Detect missing rejection option."""
        if not self.has_ml_framework:
            return
        
        # Check for confidence-based rejection
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["predict", "forward"]:
                has_rejection = any(
                    kw.arg in ["confidence_threshold", "reject_threshold", "min_confidence"]
                    for kw in node.keywords
                )
                
                if not has_rejection:
                    violation = RuleViolation(
                        rule_id="AIML159",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Model lacks confidence-based rejection - add uncertainty quantification",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML04",
                        cwe_id="CWE-754",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_robustness_testing_absent(self, node: ast.Call) -> None:
        """AIML160: Detect absence of robustness testing."""
        if not self.has_ml_framework:
            return
        
        # Check for robustness testing
        test_funcs = ["test", "evaluate", "benchmark"]
        
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if any(test in func_name.lower() for test in test_funcs):
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "adversarial" not in line_text and "robust" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML160",
                        category=RuleCategory.CONVENTION,
                        severity=RuleSeverity.MEDIUM,
                        message="No adversarial robustness testing - add evaluation suite",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML04",
                        cwe_id="CWE-1059",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    # Phase 2.1: Feature Engineering & Preprocessing (30 checks)
    # Phase 2.1.1: Data Preprocessing Security (15 checks - AIML161-AIML175)
    
    def _check_missing_preprocessing_validation(self, node: ast.Call) -> None:
        """AIML161: Detect missing input validation in preprocessing."""
        if not self.has_ml_framework:
            return
        
        # Check for preprocessing without validation
        preprocessing_funcs = ["fit_transform", "transform", "scale", "normalize", "encode"]
        
        if isinstance(node.func, ast.Attribute):
            if any(func in node.func.attr.lower() for func in preprocessing_funcs):
                # Check if validation is present
                has_validation = any(
                    kw.arg in ["validate", "check_input", "force_all_finite"]
                    for kw in node.keywords
                )
                
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML161",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Preprocessing without input validation - add validation checks",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_normalization_bypass(self, node: ast.Call) -> None:
        """AIML162: Detect normalization bypass attacks."""
        if not self.has_ml_framework:
            return
        
        # Check for normalization without bounds checking
        if isinstance(node.func, ast.Attribute):
            if "normalize" in node.func.attr.lower() or "standard" in node.func.attr.lower():
                # Check for clip/bound parameters
                has_bounds = any(
                    kw.arg in ["clip", "min", "max", "clip_values"]
                    for kw in node.keywords
                )
                
                if not has_bounds:
                    violation = RuleViolation(
                        rule_id="AIML162",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Normalization without bounds checking - bypass attack risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_feature_scaling_manipulation(self, node: ast.Call) -> None:
        """AIML163: Detect feature scaling manipulation."""
        if not self.has_ml_framework:
            return
        
        # Check for scaling operations
        if isinstance(node.func, ast.Attribute):
            if "scaler" in node.func.attr.lower() or "scale" in node.func.attr.lower():
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                # Check if scaling parameters are hardcoded
                if "user" in line_text or "input" in line_text or "request" in line_text:
                    violation = RuleViolation(
                        rule_id="AIML163",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Feature scaling with user input - manipulation risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_missing_value_injection(self, node: ast.Call) -> None:
        """AIML164: Detect missing value injection."""
        if not self.has_ml_framework:
            return
        
        # Check for imputation without validation
        if isinstance(node.func, ast.Attribute):
            if "impute" in node.func.attr.lower() or "fillna" in node.func.attr.lower():
                # Check for strategy validation
                has_strategy_validation = any(
                    kw.arg in ["strategy", "method"]
                    for kw in node.keywords
                )
                
                if has_strategy_validation:
                    for kw in node.keywords:
                        if kw.arg in ["strategy", "method"] and not isinstance(kw.value, ast.Constant):
                            violation = RuleViolation(
                                rule_id="AIML164",
                                category=RuleCategory.SECURITY,
                                severity=RuleSeverity.MEDIUM,
                                message="Missing value imputation with dynamic strategy - injection risk",
                                line_number=node.lineno,
                                column=node.col_offset,
                                end_line_number=getattr(node, "end_lineno", node.lineno),
                                end_column=getattr(node, "end_col_offset", node.col_offset),
                                file_path=str(self.file_path),
                                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                                fix_applicability=FixApplicability.SAFE,
                                fix_data=None,
                                owasp_id="ML03",
                                cwe_id="CWE-20",
                                source_tool="pyguard",
                            )
                            self.violations.append(violation)
    
    def _check_encoding_injection(self, node: ast.Call) -> None:
        """AIML165: Detect encoding injection in categorical features."""
        if not self.has_ml_framework:
            return
        
        # Check for encoding operations
        if isinstance(node.func, ast.Attribute):
            if "encode" in node.func.attr.lower() and "categorical" in str(node).lower():
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "validate" not in line_text and "sanitize" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML165",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Categorical encoding without validation - injection risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_feature_extraction_vulnerabilities(self, node: ast.Call) -> None:
        """AIML166: Detect feature extraction vulnerabilities."""
        if not self.has_ml_framework:
            return
        
        # Check for feature extraction
        if isinstance(node.func, ast.Attribute):
            if "extract" in node.func.attr.lower() or "feature" in node.func.attr.lower():
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "sanitize" not in line_text and "validate" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML166",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Feature extraction without validation - vulnerability risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_dimensionality_reduction_poisoning(self, node: ast.Call) -> None:
        """AIML167: Detect dimensionality reduction poisoning."""
        if not self.has_ml_framework:
            return
        
        # Check for dimensionality reduction
        if isinstance(node.func, ast.Attribute):
            dim_red_methods = ["pca", "tsne", "umap", "lda", "svd", "nmf"]
            if any(method in node.func.attr.lower() for method in dim_red_methods):
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "validate" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML167",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Dimensionality reduction without validation - poisoning risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_feature_selection_manipulation(self, node: ast.Call) -> None:
        """AIML168: Detect feature selection manipulation."""
        if not self.has_ml_framework:
            return
        
        # Check for feature selection
        if isinstance(node.func, ast.Attribute):
            if "selectk" in node.func.attr.lower() or "feature_selection" in str(node).lower():
                # Check if selection criteria is hardcoded
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "user" in line_text or "input" in line_text:
                    violation = RuleViolation(
                        rule_id="AIML168",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Feature selection with user input - manipulation risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_missing_outlier_detection(self, node: ast.Call) -> None:
        """AIML169: Detect missing outlier detection."""
        if not self.has_ml_framework:
            return
        
        # Check for preprocessing without outlier detection
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["fit", "fit_transform"]:
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "outlier" not in line_text and "anomaly" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML169",
                        category=RuleCategory.CONVENTION,
                        severity=RuleSeverity.LOW,
                        message="Preprocessing without outlier detection - consider adding anomaly detection",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_data_leakage_preprocessing(self, node: ast.Call) -> None:
        """AIML170: Detect data leakage in preprocessing."""
        if not self.has_ml_framework:
            return
        
        # Check for fit_transform on entire dataset
        if isinstance(node.func, ast.Attribute):
            if node.func.attr == "fit_transform":
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "test" in line_text or "validation" in line_text:
                    violation = RuleViolation(
                        rule_id="AIML170",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Data leakage - fitting preprocessing on test/validation data",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-200",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_test_train_contamination(self, node: ast.Call) -> None:
        """AIML171: Detect test/train contamination."""
        if not self.has_ml_framework:
            return
        
        # Check for train_test_split usage
        if isinstance(node.func, ast.Attribute):
            if "split" in node.func.attr.lower():
                # Check if shuffle is disabled
                has_shuffle = any(
                    kw.arg == "shuffle" and isinstance(kw.value, ast.Constant) and kw.value.value
                    for kw in node.keywords
                )
                
                has_random_state = any(
                    kw.arg == "random_state"
                    for kw in node.keywords
                )
                
                if not has_random_state:
                    violation = RuleViolation(
                        rule_id="AIML171",
                        category=RuleCategory.CONVENTION,
                        severity=RuleSeverity.MEDIUM,
                        message="Train/test split without random_state - reproducibility risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-330",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_feature_store_injection(self, node: ast.Call) -> None:
        """AIML172: Detect feature store injection."""
        if not self.has_ml_framework:
            return
        
        # Check for feature store operations
        if isinstance(node.func, ast.Attribute):
            if "feature" in node.func.attr.lower() and "get" in node.func.attr.lower():
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "validate" not in line_text and "sanitize" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML172",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Feature store access without validation - injection risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_pipeline_versioning_gaps(self, node: ast.Call) -> None:
        """AIML173: Detect pipeline versioning gaps."""
        if not self.has_ml_framework:
            return
        
        # Check for pipeline save/load without version
        if isinstance(node.func, ast.Attribute):
            if "pipeline" in str(node).lower() and node.func.attr in ["save", "dump", "pickle"]:
                # Check if version is specified
                has_version = any(
                    kw.arg in ["version", "metadata"]
                    for kw in node.keywords
                )
                
                if not has_version:
                    violation = RuleViolation(
                        rule_id="AIML173",
                        category=RuleCategory.CONVENTION,
                        severity=RuleSeverity.LOW,
                        message="Pipeline saved without version metadata - tracking risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-778",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_preprocessing_state_tampering(self, node: ast.Call) -> None:
        """AIML174: Detect preprocessing state tampering."""
        if not self.has_ml_framework:
            return
        
        # Check for preprocessor state loading
        if isinstance(node.func, ast.Attribute):
            if "load" in node.func.attr.lower() and ("scaler" in str(node).lower() or "encoder" in str(node).lower()):
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "verify" not in line_text and "checksum" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML174",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Preprocessing state loaded without integrity check - tampering risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML05",
                        cwe_id="CWE-494",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_transformation_order_vulnerabilities(self, node: ast.Call) -> None:
        """AIML175: Detect transformation order vulnerabilities."""
        if not self.has_ml_framework:
            return
        
        # Check for make_pipeline usage
        if isinstance(node.func, (ast.Name, ast.Attribute)):
            func_name = node.func.id if isinstance(node.func, ast.Name) else node.func.attr
            
            if "pipeline" in func_name.lower():
                # Pipeline order matters for security
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "comment" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML175",
                        category=RuleCategory.CONVENTION,
                        severity=RuleSeverity.LOW,
                        message="Pipeline transformation order - document to prevent vulnerabilities",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-693",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    # Phase 2.1.2: Feature Store Security (15 checks - AIML176-AIML190)
    
    def _check_feast_feature_store_injection(self, node: ast.Call) -> None:
        """AIML176: Detect Feast feature store injection."""
        if not self.has_ml_framework:
            return
        
        # Check for Feast feature store operations
        if isinstance(node.func, ast.Attribute):
            if "feast" in str(node).lower() or "feature_store" in node.func.attr.lower():
                # Check for SQL/NoSQL injection in feature queries
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if any(keyword in line_text for keyword in ["user", "input", "request", "param"]):
                    violation = RuleViolation(
                        rule_id="AIML176",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Feast feature store query with user input - injection risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-89",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_missing_feature_validation(self, node: ast.Call) -> None:
        """AIML177: Detect missing feature validation."""
        if not self.has_ml_framework:
            return
        
        # Check for feature retrieval without validation
        if isinstance(node.func, ast.Attribute):
            if "get_features" in node.func.attr.lower() or "fetch_features" in node.func.attr.lower():
                # Check if validation is present
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "validate" not in line_text and "check" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML177",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Feature retrieval without validation - integrity risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_feature_drift_without_detection(self, node: ast.Call) -> None:
        """AIML178: Detect feature drift without detection."""
        if not self.has_ml_framework:
            return
        
        # Check for feature serving without drift monitoring
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["predict", "transform", "get_features"]:
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "drift" not in line_text and "monitor" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML178",
                        category=RuleCategory.CONVENTION,
                        severity=RuleSeverity.LOW,
                        message="Feature usage without drift detection - add monitoring",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML06",
                        cwe_id="CWE-754",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_feature_serving_vulnerabilities(self, node: ast.Call) -> None:
        """AIML179: Detect feature serving vulnerabilities."""
        if not self.has_ml_framework:
            return
        
        # Check for feature serving endpoints
        if isinstance(node.func, ast.Attribute):
            if "serve" in node.func.attr.lower() and "feature" in str(node).lower():
                # Check for authentication
                has_auth = any(
                    kw.arg in ["auth", "authentication", "token", "api_key"]
                    for kw in node.keywords
                )
                
                if not has_auth:
                    violation = RuleViolation(
                        rule_id="AIML179",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Feature serving without authentication - access control risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="A01",
                        cwe_id="CWE-306",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_offline_online_feature_skew(self, node: ast.Call) -> None:
        """AIML180: Detect offline/online feature skew."""
        if not self.has_ml_framework:
            return
        
        # Check for online feature serving
        if isinstance(node.func, ast.Attribute):
            if "online" in node.func.attr.lower() or "real_time" in str(node).lower():
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "consistency" not in line_text and "validate" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML180",
                        category=RuleCategory.CONVENTION,
                        severity=RuleSeverity.MEDIUM,
                        message="Online feature serving - validate consistency with offline features",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-754",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_feature_metadata_tampering(self, node: ast.Call) -> None:
        """AIML181: Detect feature metadata tampering."""
        if not self.has_ml_framework:
            return
        
        # Check for metadata updates
        if isinstance(node.func, ast.Attribute):
            if "metadata" in node.func.attr.lower() and "update" in node.func.attr.lower():
                # Check for integrity verification
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "verify" not in line_text and "checksum" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML181",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Feature metadata update without integrity check - tampering risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML05",
                        cwe_id="CWE-494",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_feature_lineage_missing(self, node: ast.Call) -> None:
        """AIML182: Detect missing feature lineage."""
        if not self.has_ml_framework:
            return
        
        # Check for feature creation without lineage tracking
        if isinstance(node.func, ast.Attribute):
            if "create_feature" in node.func.attr.lower() or "register_feature" in node.func.attr.lower():
                # Check for lineage metadata
                has_lineage = any(
                    kw.arg in ["lineage", "source", "provenance", "metadata"]
                    for kw in node.keywords
                )
                
                if not has_lineage:
                    violation = RuleViolation(
                        rule_id="AIML182",
                        category=RuleCategory.CONVENTION,
                        severity=RuleSeverity.LOW,
                        message="Feature creation without lineage tracking - add provenance metadata",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-778",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_feature_access_control_gaps(self, node: ast.Call) -> None:
        """AIML183: Detect feature access control gaps."""
        if not self.has_ml_framework:
            return
        
        # Check for feature access without authorization
        if isinstance(node.func, ast.Attribute):
            if "get_features" in node.func.attr.lower() or "access" in node.func.attr.lower():
                # Check for authorization
                has_auth = any(
                    kw.arg in ["user", "role", "permissions", "acl"]
                    for kw in node.keywords
                )
                
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if not has_auth and "auth" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML183",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Feature access without authorization - add access control",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="A01",
                        cwe_id="CWE-862",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_feature_deletion_corruption(self, node: ast.Call) -> None:
        """AIML184: Detect feature deletion/corruption risks."""
        if not self.has_ml_framework:
            return
        
        # Check for feature deletion operations
        if isinstance(node.func, ast.Attribute):
            if "delete" in node.func.attr.lower() and "feature" in str(node).lower():
                # Check for soft delete or backup
                has_backup = any(
                    kw.arg in ["backup", "archive", "soft_delete"]
                    for kw in node.keywords
                )
                
                if not has_backup:
                    violation = RuleViolation(
                        rule_id="AIML184",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Feature deletion without backup - data loss risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-404",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_feature_version_control_weaknesses(self, node: ast.Call) -> None:
        """AIML185: Detect feature version control weaknesses."""
        if not self.has_ml_framework:
            return
        
        # Check for feature updates without versioning
        if isinstance(node.func, ast.Attribute):
            if "update_feature" in node.func.attr.lower() or "modify_feature" in node.func.attr.lower():
                # Check for version parameter
                has_version = any(
                    kw.arg in ["version", "revision"]
                    for kw in node.keywords
                )
                
                if not has_version:
                    violation = RuleViolation(
                        rule_id="AIML185",
                        category=RuleCategory.CONVENTION,
                        severity=RuleSeverity.LOW,
                        message="Feature update without version control - tracking risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-778",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_feature_freshness_attacks(self, node: ast.Call) -> None:
        """AIML186: Detect feature freshness attacks."""
        if not self.has_ml_framework:
            return
        
        # Check for feature serving without freshness validation
        if isinstance(node.func, ast.Attribute):
            if "get_features" in node.func.attr.lower() or "serve" in node.func.attr.lower():
                # Check for timestamp/ttl validation
                has_freshness = any(
                    kw.arg in ["ttl", "max_age", "timestamp", "freshness"]
                    for kw in node.keywords
                )
                
                if not has_freshness:
                    violation = RuleViolation(
                        rule_id="AIML186",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Feature serving without freshness validation - stale data risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-672",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_batch_realtime_inconsistencies(self, node: ast.Call) -> None:
        """AIML187: Detect batch vs real-time inconsistencies."""
        if not self.has_ml_framework:
            return
        
        # Check for batch processing
        if isinstance(node.func, ast.Attribute):
            if "batch" in node.func.attr.lower() and "feature" in str(node).lower():
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "consistency" not in line_text and "validate" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML187",
                        category=RuleCategory.CONVENTION,
                        severity=RuleSeverity.MEDIUM,
                        message="Batch feature processing - validate consistency with real-time",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-754",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_feature_engineering_code_injection(self, node: ast.Call) -> None:
        """AIML188: Detect feature engineering code injection."""
        if not self.has_ml_framework:
            return
        
        # Check for dynamic feature engineering
        if isinstance(node.func, ast.Attribute):
            if "feature" in node.func.attr.lower() and "transform" in node.func.attr.lower():
                # Check if transformation uses user input
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if any(keyword in line_text for keyword in ["user", "input", "request", "eval", "exec"]):
                    violation = RuleViolation(
                        rule_id="AIML188",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.CRITICAL,
                        message="Feature transformation with dynamic code - injection risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="A03",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_schema_evolution_attacks(self, node: ast.Call) -> None:
        """AIML189: Detect schema evolution attacks."""
        if not self.has_ml_framework:
            return
        
        # Check for schema updates
        if isinstance(node.func, ast.Attribute):
            if "schema" in node.func.attr.lower() and ("update" in node.func.attr.lower() or "evolve" in node.func.attr.lower()):
                # Check for validation
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "validate" not in line_text and "verify" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML189",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Schema evolution without validation - compatibility risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_feature_importance_manipulation(self, node: ast.Call) -> None:
        """AIML190: Detect feature importance manipulation."""
        if not self.has_ml_framework:
            return
        
        # Check for feature importance calculations
        if isinstance(node.func, ast.Attribute):
            if "feature_importances" in node.func.attr.lower() or "permutation_importance" in str(node).lower():
                # Check if importance is user-controllable
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "user" in line_text or "input" in line_text:
                    violation = RuleViolation(
                        rule_id="AIML190",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Feature importance calculation with user input - manipulation risk",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    # Phase 2.2: Model Training Infrastructure (35 checks)
    # Phase 2.2.1: Distributed Training Security (15 checks - AIML191-AIML205)
    
    def _check_parameter_server_vulnerabilities(self, node: ast.Call) -> None:
        """AIML191: Detect parameter server vulnerabilities."""
        if not self.has_ml_framework:
            return
        
        # Check for parameter server usage without security
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "parameterserver" in line_text.replace(" ", "") or "parameter_server" in line_text:
            if "ssl" not in line_text and "tls" not in line_text and "encrypt" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML191",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Parameter server - implement authentication and encryption",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-306",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_gradient_aggregation_poisoning(self, node: ast.Call) -> None:
        """AIML192: Detect gradient aggregation poisoning."""
        if not self.has_pytorch:
            return
        
        # Check for gradient aggregation
        if isinstance(node.func, ast.Attribute):
            if "aggregate" in node.func.attr.lower() or "all_reduce" in node.func.attr.lower():
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "gradient" in line_text or "grad" in line_text:
                    if "validate" not in line_text and "verify" not in line_text:
                        violation = RuleViolation(
                            rule_id="AIML192",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.HIGH,
                            message="Gradient aggregation - validate to prevent poisoning attacks",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.MANUAL,
                            fix_data=None,
                            owasp_id="ML03",
                            cwe_id="CWE-345",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_byzantine_worker_attacks(self, node: ast.Call) -> None:
        """AIML193: Detect Byzantine worker attack vulnerabilities."""
        if not self.has_ml_framework:
            return
        
        # Check for distributed training without Byzantine detection
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "distributed" in line_text and ("worker" in line_text or "node" in line_text):
            if "byzantine" not in line_text and "krum" not in line_text and "median" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML193",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Distributed training - add Byzantine worker detection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_all_reduce_manipulation(self, node: ast.Call) -> None:
        """AIML194: Detect All-Reduce manipulation risks."""
        if not self.has_pytorch:
            return
        
        # Check for all_reduce operations
        if isinstance(node.func, ast.Attribute):
            if "all_reduce" in node.func.attr.lower():
                # Check for validation
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "validate" not in line_text and "verify" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML194",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="All-Reduce operation - validate tensor values to prevent manipulation",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_ring_all_reduce_injection(self, node: ast.Call) -> None:
        """AIML195: Detect Ring-All-Reduce injection risks."""
        if not self.has_ml_framework:
            return
        
        # Check for ring all-reduce patterns
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "ring" in line_text and "allreduce" in line_text.replace("-", "").replace("_", ""):
            if "secure" not in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML195",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Ring-All-Reduce - implement secure communication to prevent injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_horovod_security_gaps(self, node: ast.Call) -> None:
        """AIML196: Detect Horovod security gaps."""
        if not self.has_ml_framework:
            return
        
        # Check for Horovod usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "horovod" in line_text or "hvd" in line_text:
            if isinstance(node.func, ast.Attribute):
                if "init" in node.func.attr or "broadcast" in node.func.attr:
                    # Check for TLS/encryption
                    if "tls" not in line_text and "ssl" not in line_text:
                        violation = RuleViolation(
                            rule_id="AIML196",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.MEDIUM,
                            message="Horovod communication - enable TLS for secure distributed training",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.MANUAL,
                            fix_data=None,
                            owasp_id="ML03",
                            cwe_id="CWE-300",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_deepspeed_vulnerabilities(self, node: ast.Call) -> None:
        """AIML197: Detect DeepSpeed vulnerabilities."""
        if not self.has_ml_framework:
            return
        
        # Check for DeepSpeed usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "deepspeed" in line_text:
            if isinstance(node.func, ast.Attribute):
                if "initialize" in node.func.attr or "init" in node.func.attr:
                    # Check for secure configuration
                    if "config" not in line_text or ("validate" not in line_text and "verify" not in line_text):
                        violation = RuleViolation(
                            rule_id="AIML197",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.MEDIUM,
                            message="DeepSpeed initialization - validate configuration to prevent vulnerabilities",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.MANUAL,
                            fix_data=None,
                            owasp_id="ML03",
                            cwe_id="CWE-20",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_fsdp_risks(self, node: ast.Call) -> None:
        """AIML198: Detect FSDP (Fully Sharded Data Parallel) risks."""
        if not self.has_pytorch:
            return
        
        # Check for FSDP usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "fsdp" in line_text or "fullyshareddataparallel" in line_text.replace("_", ""):
            if "secure" not in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML198",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="FSDP training - implement shard validation to prevent poisoning",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_zero_optimizer_state_attacks(self, node: ast.Call) -> None:
        """AIML199: Detect ZeRO optimizer state attacks."""
        if not self.has_ml_framework:
            return
        
        # Check for ZeRO optimizer usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "zero" in line_text and ("optimizer" in line_text or "stage" in line_text):
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML199",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="ZeRO optimizer state - validate state integrity to prevent attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_model_parallel_partition_poisoning(self, node: ast.Call) -> None:
        """AIML200: Detect model parallel partition poisoning."""
        if not self.has_ml_framework:
            return
        
        # Check for model parallelism
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "model" in line_text and "parallel" in line_text and ("partition" in line_text or "shard" in line_text):
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML200",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Model parallel partitioning - validate partition integrity",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_pipeline_parallel_injection(self, node: ast.Call) -> None:
        """AIML201: Detect pipeline parallel injection risks."""
        if not self.has_ml_framework:
            return
        
        # Check for pipeline parallelism
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "pipeline" in line_text and "parallel" in line_text:
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML201",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Pipeline parallel training - validate stage outputs to prevent injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_tensor_parallel_tampering(self, node: ast.Call) -> None:
        """AIML202: Detect tensor parallel tampering."""
        if not self.has_ml_framework:
            return
        
        # Check for tensor parallelism
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "tensor" in line_text and "parallel" in line_text:
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML202",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Tensor parallel training - validate tensor splits to prevent tampering",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_mixed_precision_training_risks(self, node: ast.Call) -> None:
        """AIML203: Detect mixed precision training risks."""
        if not self.has_ml_framework:
            return
        
        # Check for mixed precision training
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if ("amp" in line_text or "autocast" in line_text or "mixed" in line_text) and "precision" in line_text:
            if "validate" not in line_text and "check" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML203",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="Mixed precision training - validate numerical stability to prevent attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-754",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_communication_backend_vulnerabilities(self, node: ast.Call) -> None:
        """AIML204: Detect communication backend vulnerabilities."""
        if not self.has_pytorch:
            return
        
        # Check for distributed backend initialization
        if isinstance(node.func, ast.Attribute):
            if "init_process_group" in node.func.attr:
                # Check for backend security
                has_secure_backend = False
                for keyword in node.keywords:
                    if keyword.arg == "backend":
                        if isinstance(keyword.value, ast.Constant):
                            backend = keyword.value.value
                            if "nccl" in str(backend).lower():
                                has_secure_backend = True
                
                if not has_secure_backend:
                    violation = RuleViolation(
                        rule_id="AIML204",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Communication backend - use secure backend (e.g., NCCL) for distributed training",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-300",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_collective_operation_manipulation(self, node: ast.Call) -> None:
        """AIML205: Detect collective operation manipulation."""
        if not self.has_pytorch:
            return
        
        # Check for collective operations
        if isinstance(node.func, ast.Attribute):
            collective_ops = ["all_gather", "broadcast", "reduce", "scatter", "barrier"]
            if any(op in node.func.attr.lower() for op in collective_ops):
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "validate" not in line_text and "verify" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML205",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Collective operation - validate tensor data to prevent manipulation",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    # Phase 2.2.2: GPU & Accelerator Security (10 checks - AIML206-AIML215)
    
    def _check_gpu_memory_leakage_aiml206(self, node: ast.Call) -> None:
        """AIML206: Detect GPU memory leakage."""
        if not self.has_pytorch:
            return
        
        # Check for GPU memory allocation without cleanup
        if isinstance(node.func, ast.Attribute):
            if "cuda" in str(node).lower() or "to" in node.func.attr:
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "device" in line_text or "cuda" in line_text:
                    # Look for memory management
                    context_lines = []
                    start = max(0, node.lineno - 5)
                    end = min(len(self.lines), node.lineno + 5)
                    context_lines = [self.lines[i].lower() for i in range(start, end)]
                    context_text = " ".join(context_lines)
                    
                    if "empty_cache" not in context_text and "del" not in context_text:
                        violation = RuleViolation(
                            rule_id="AIML206",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.MEDIUM,
                            message="GPU memory allocation - ensure proper cleanup to prevent memory leaks",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="ML03",
                            cwe_id="CWE-401",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_cuda_kernel_injection(self, node: ast.Call) -> None:
        """AIML207: Detect CUDA kernel injection risks."""
        if not self.has_pytorch:
            return
        
        # Check for custom CUDA kernel usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "cuda" in line_text and ("kernel" in line_text or "jit" in line_text or "load" in line_text):
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML207",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.CRITICAL,
                    message="Custom CUDA kernel - validate source to prevent code injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_rocm_vulnerabilities(self, node: ast.Call) -> None:
        """AIML208: Detect ROCm vulnerabilities."""
        if not self.has_ml_framework:
            return
        
        # Check for ROCm usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "rocm" in line_text or "hip" in line_text:
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML208",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="ROCm usage - validate kernel code and memory management",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_tpu_security_gaps(self, node: ast.Call) -> None:
        """AIML209: Detect TPU security gaps."""
        if not self.has_ml_framework:
            return
        
        # Check for TPU usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "tpu" in line_text or "xla" in line_text:
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML209",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="TPU usage - validate computation and memory management",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_npu_ipu_risks(self, node: ast.Call) -> None:
        """AIML210: Detect NPU/IPU risks."""
        if not self.has_ml_framework:
            return
        
        # Check for NPU/IPU usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "npu" in line_text or "ipu" in line_text or "graphcore" in line_text:
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML210",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="NPU/IPU usage - validate accelerator configuration and operations",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_multi_gpu_synchronization_attacks(self, node: ast.Call) -> None:
        """AIML211: Detect multi-GPU synchronization attack risks."""
        if not self.has_pytorch:
            return
        
        # Check for multi-GPU operations
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if ("datagparallel" in line_text.replace("_", "") or "distributeddataparallel" in line_text.replace("_", "")):
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML211",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Multi-GPU training - validate synchronization to prevent attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_device_placement_manipulation(self, node: ast.Call) -> None:
        """AIML212: Detect device placement manipulation."""
        if not self.has_ml_framework:
            return
        
        # Check for device placement
        if isinstance(node.func, ast.Attribute):
            if node.func.attr == "to" or "device" in node.func.attr.lower():
                # Check if device is user-controllable
                for arg in node.args:
                    if isinstance(arg, ast.Name):
                        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                        if "user" in line_text or "input" in line_text:
                            violation = RuleViolation(
                                rule_id="AIML212",
                                category=RuleCategory.SECURITY,
                                severity=RuleSeverity.MEDIUM,
                                message="Device placement with user input - manipulation risk",
                                line_number=node.lineno,
                                column=node.col_offset,
                                end_line_number=getattr(node, "end_lineno", node.lineno),
                                end_column=getattr(node, "end_col_offset", node.col_offset),
                                file_path=str(self.file_path),
                                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                                fix_applicability=FixApplicability.SAFE,
                                fix_data=None,
                                owasp_id="ML03",
                                cwe_id="CWE-345",
                                source_tool="pyguard",
                            )
                            self.violations.append(violation)
                            break
    
    def _check_cuda_graph_poisoning(self, node: ast.Call) -> None:
        """AIML213: Detect CUDA graph poisoning."""
        if not self.has_pytorch:
            return
        
        # Check for CUDA graph usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "cuda" in line_text and "graph" in line_text:
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML213",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="CUDA graph - validate graph structure to prevent poisoning",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_kernel_launch_parameter_tampering(self, node: ast.Call) -> None:
        """AIML214: Detect kernel launch parameter tampering."""
        if not self.has_pytorch:
            return
        
        # Check for kernel launches
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "kernel" in line_text and ("launch" in line_text or "call" in line_text):
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML214",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Kernel launch - validate parameters to prevent tampering",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_gpu_memory_exhaustion_attacks(self, node: ast.Call) -> None:
        """AIML215: Detect GPU memory exhaustion attack risks."""
        if not self.has_pytorch:
            return
        
        # Check for large tensor allocations
        if isinstance(node.func, ast.Attribute):
            if "tensor" in node.func.attr.lower() or "empty" in node.func.attr or "zeros" in node.func.attr or "ones" in node.func.attr:
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "cuda" in line_text or "device" in line_text:
                    # Check for size validation
                    if "validate" not in line_text and "check" not in line_text and "limit" not in line_text:
                        violation = RuleViolation(
                            rule_id="AIML215",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.MEDIUM,
                            message="GPU tensor allocation - validate size to prevent memory exhaustion",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="ML03",
                            cwe_id="CWE-400",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    # Phase 2.2.3: Experiment Tracking Security (10 checks - AIML216-AIML225)
    
    def _check_mlflow_injection_attacks(self, node: ast.Call) -> None:
        """AIML216: Detect MLflow injection attacks."""
        if not self.has_ml_framework:
            return
        
        # Check for MLflow usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "mlflow" in line_text:
            if isinstance(node.func, ast.Attribute):
                if "log" in node.func.attr or "set" in node.func.attr:
                    # Check for user input
                    if "user" in line_text or "input" in line_text:
                        violation = RuleViolation(
                            rule_id="AIML216",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.HIGH,
                            message="MLflow logging with user input - injection risk, validate data",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="ML03",
                            cwe_id="CWE-94",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_wandb_credential_leakage(self, node: ast.Call) -> None:
        """AIML217: Detect Weights & Biases credential leakage."""
        if not self.has_ml_framework:
            return
        
        # Check for wandb usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "wandb" in line_text:
            if isinstance(node.func, ast.Attribute):
                if "init" in node.func.attr or "login" in node.func.attr:
                    # Check for hardcoded keys
                    for keyword in node.keywords:
                        if keyword.arg in ["api_key", "key"]:
                            if isinstance(keyword.value, ast.Constant):
                                violation = RuleViolation(
                                    rule_id="AIML217",
                                    category=RuleCategory.SECURITY,
                                    severity=RuleSeverity.CRITICAL,
                                    message="Weights & Biases API key hardcoded - use environment variables",
                                    line_number=node.lineno,
                                    column=node.col_offset,
                                    end_line_number=getattr(node, "end_lineno", node.lineno),
                                    end_column=getattr(node, "end_col_offset", node.col_offset),
                                    file_path=str(self.file_path),
                                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                                    fix_applicability=FixApplicability.SAFE,
                                    fix_data=None,
                                    owasp_id="A07",
                                    cwe_id="CWE-798",
                                    source_tool="pyguard",
                                )
                                self.violations.append(violation)
    
    def _check_cometml_experiment_tampering(self, node: ast.Call) -> None:
        """AIML218: Detect Comet.ml experiment tampering."""
        if not self.has_ml_framework:
            return
        
        # Check for comet_ml usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "comet" in line_text:
            if isinstance(node.func, ast.Attribute):
                if "log" in node.func.attr or "set" in node.func.attr:
                    # Check for validation
                    if "validate" not in line_text and "verify" not in line_text:
                        violation = RuleViolation(
                            rule_id="AIML218",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.MEDIUM,
                            message="Comet.ml experiment data - validate to prevent tampering",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.MANUAL,
                            fix_data=None,
                            owasp_id="ML03",
                            cwe_id="CWE-345",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_tensorboard_rce(self, node: ast.Call) -> None:
        """AIML219: Detect TensorBoard remote code execution risks."""
        if not self.has_tensorflow:
            return
        
        # Check for TensorBoard usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "tensorboard" in line_text or "summarywriter" in line_text.replace("_", ""):
            if isinstance(node.func, ast.Attribute):
                if "add" in node.func.attr or "log" in node.func.attr:
                    # Check for user input
                    if "user" in line_text or "input" in line_text:
                        violation = RuleViolation(
                            rule_id="AIML219",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.CRITICAL,
                            message="TensorBoard logging with user input - RCE risk, sanitize data",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="ML05",
                            cwe_id="CWE-94",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_neptuneai_model_manipulation(self, node: ast.Call) -> None:
        """AIML220: Detect Neptune.ai model manipulation."""
        if not self.has_ml_framework:
            return
        
        # Check for Neptune.ai usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "neptune" in line_text:
            if isinstance(node.func, ast.Attribute):
                if "log" in node.func.attr or "upload" in node.func.attr:
                    # Check for validation
                    if "validate" not in line_text and "verify" not in line_text:
                        violation = RuleViolation(
                            rule_id="AIML220",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.MEDIUM,
                            message="Neptune.ai model logging - validate data to prevent manipulation",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.MANUAL,
                            fix_data=None,
                            owasp_id="ML03",
                            cwe_id="CWE-345",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_experiment_metadata_injection(self, node: ast.Call) -> None:
        """AIML221: Detect experiment metadata injection."""
        if not self.has_ml_framework:
            return
        
        # Check for metadata logging
        if isinstance(node.func, ast.Attribute):
            if "metadata" in node.func.attr.lower() or "tag" in node.func.attr.lower():
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "user" in line_text or "input" in line_text:
                    violation = RuleViolation(
                        rule_id="AIML221",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Experiment metadata with user input - injection risk, sanitize data",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_metric_tampering(self, node: ast.Call) -> None:
        """AIML222: Detect metric tampering."""
        if not self.has_ml_framework:
            return
        
        # Check for metric logging
        if isinstance(node.func, ast.Attribute):
            if "log_metric" in node.func.attr.lower() or "log_metrics" in node.func.attr.lower():
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "validate" not in line_text and "verify" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML222",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Metric logging - validate values to prevent tampering",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_artifact_poisoning(self, node: ast.Call) -> None:
        """AIML223: Detect artifact poisoning."""
        if not self.has_ml_framework:
            return
        
        # Check for artifact logging
        if isinstance(node.func, ast.Attribute):
            if "log_artifact" in node.func.attr.lower() or "upload" in node.func.attr.lower():
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "validate" not in line_text and "verify" not in line_text and "checksum" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML223",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Artifact upload - validate integrity with checksums to prevent poisoning",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML05",
                        cwe_id="CWE-494",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_run_comparison_manipulation(self, node: ast.Call) -> None:
        """AIML224: Detect run comparison manipulation."""
        if not self.has_ml_framework:
            return
        
        # Check for run comparison
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "compare" in line_text and ("run" in line_text or "experiment" in line_text):
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML224",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="Run comparison - validate run IDs to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_hyperparameter_logging_risks(self, node: ast.Call) -> None:
        """AIML225: Detect hyperparameter logging risks."""
        if not self.has_ml_framework:
            return
        
        # Check for hyperparameter logging
        if isinstance(node.func, ast.Attribute):
            if "log_param" in node.func.attr.lower() or "log_hyperparameter" in node.func.attr.lower():
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                # Check for sensitive data in parameters
                if "key" in line_text or "password" in line_text or "secret" in line_text or "token" in line_text:
                    violation = RuleViolation(
                        rule_id="AIML225",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Hyperparameter logging - avoid logging sensitive data (keys, passwords)",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="A02",
                        cwe_id="CWE-200",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    # Phase 2.3: Model Deployment & Serving (35 checks)
    # Phase 2.3.1: Model Serving Vulnerabilities (15 checks - AIML226-AIML240)
    
    def _check_torchserve_vulnerabilities(self, node: ast.Call) -> None:
        """AIML226: Detect TorchServe vulnerabilities."""
        if not self.has_pytorch:
            return
        
        # Check for TorchServe usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "torchserve" in line_text or "torch-model-archiver" in line_text:
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML226",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="TorchServe deployment - validate model and handler code",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_tensorflow_serving_injection(self, node: ast.Call) -> None:
        """AIML227: Detect TensorFlow Serving injection risks."""
        if not self.has_tensorflow:
            return
        
        # Check for TensorFlow Serving
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "tensorflow_serving" in line_text.replace("-", "_") or "tf_serving" in line_text:
            if "validate" not in line_text and "sanitize" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML227",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="TensorFlow Serving - validate input data to prevent injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_onnx_runtime_risks(self, node: ast.Call) -> None:
        """AIML228: Detect ONNX Runtime risks."""
        if not self.has_ml_framework:
            return
        
        # Check for ONNX Runtime usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "onnxruntime" in line_text.replace("_", "").replace("-", "") or "ort" in line_text:
            if isinstance(node.func, ast.Attribute):
                if "inferenc" in node.func.attr.lower() or "run" in node.func.attr:
                    if "validate" not in line_text:
                        violation = RuleViolation(
                            rule_id="AIML228",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.MEDIUM,
                            message="ONNX Runtime inference - validate input shapes and types",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="ML05",
                            cwe_id="CWE-20",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_triton_inference_server_gaps(self, node: ast.Call) -> None:
        """AIML229: Detect Triton Inference Server security gaps."""
        if not self.has_ml_framework:
            return
        
        # Check for Triton usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "triton" in line_text and ("inference" in line_text or "server" in line_text):
            if "auth" not in line_text and "secure" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML229",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Triton Inference Server - implement authentication and encryption",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="A07",
                    cwe_id="CWE-306",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_bentoml_security_issues(self, node: ast.Call) -> None:
        """AIML230: Detect BentoML security issues."""
        if not self.has_ml_framework:
            return
        
        # Check for BentoML usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "bentoml" in line_text or "bento" in line_text:
            if isinstance(node.func, ast.Attribute):
                if "serve" in node.func.attr.lower() or "api" in node.func.attr.lower():
                    if "auth" not in line_text and "validate" not in line_text:
                        violation = RuleViolation(
                            rule_id="AIML230",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.MEDIUM,
                            message="BentoML service - add authentication and input validation",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="A07",
                            cwe_id="CWE-306",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_ray_serve_vulnerabilities(self, node: ast.Call) -> None:
        """AIML231: Detect Ray Serve vulnerabilities."""
        if not self.has_ml_framework:
            return
        
        # Check for Ray Serve usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "ray" in line_text and "serve" in line_text:
            if "auth" not in line_text and "secure" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML231",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Ray Serve deployment - implement authentication and security",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="A07",
                    cwe_id="CWE-306",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_seldon_core_risks(self, node: ast.Call) -> None:
        """AIML232: Detect Seldon Core risks."""
        if not self.has_ml_framework:
            return
        
        # Check for Seldon Core usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "seldon" in line_text:
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML232",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Seldon Core deployment - validate model and input data",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_kserve_weaknesses(self, node: ast.Call) -> None:
        """AIML233: Detect KServe weaknesses."""
        if not self.has_ml_framework:
            return
        
        # Check for KServe usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "kserve" in line_text or "kfserving" in line_text:
            if "auth" not in line_text and "secure" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML233",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="KServe deployment - implement authentication and security policies",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="A07",
                    cwe_id="CWE-306",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_model_batching_attacks(self, node: ast.Call) -> None:
        """AIML234: Detect model batching attack risks."""
        if not self.has_ml_framework:
            return
        
        # Check for batching operations
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "batch" in line_text and ("inference" in line_text or "predict" in line_text):
            if "validate" not in line_text and "limit" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML234",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Model batching - validate batch size to prevent resource exhaustion",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-400",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_dynamic_batching_poisoning(self, node: ast.Call) -> None:
        """AIML235: Detect dynamic batching poisoning."""
        if not self.has_ml_framework:
            return
        
        # Check for dynamic batching
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "dynamic" in line_text and "batch" in line_text:
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML235",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Dynamic batching - validate batch composition to prevent poisoning",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_model_versioning_bypass(self, node: ast.Call) -> None:
        """AIML236: Detect model versioning bypass risks."""
        if not self.has_ml_framework:
            return
        
        # Check for model loading without version
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if ("load" in line_text or "get" in line_text) and "model" in line_text:
            if "version" not in line_text and "tag" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML236",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="Model loading - specify version to prevent unintended model usage",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-494",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_ab_testing_manipulation(self, node: ast.Call) -> None:
        """AIML237: Detect A/B testing manipulation."""
        if not self.has_ml_framework:
            return
        
        # Check for A/B testing
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if ("a/b" in line_text or "ab_test" in line_text or "abtest" in line_text):
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML237",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="A/B testing - validate assignment logic to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_canary_deployment_risks(self, node: ast.Call) -> None:
        """AIML238: Detect canary deployment risks."""
        if not self.has_ml_framework:
            return
        
        # Check for canary deployments
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "canary" in line_text:
            if "validate" not in line_text and "monitor" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML238",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="Canary deployment - implement monitoring and validation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-754",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_blue_green_deployment_gaps(self, node: ast.Call) -> None:
        """AIML239: Detect blue-green deployment gaps."""
        if not self.has_ml_framework:
            return
        
        # Check for blue-green deployments
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if ("blue" in line_text and "green" in line_text) or "bluegreen" in line_text:
            if "validate" not in line_text and "rollback" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML239",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="Blue-green deployment - ensure rollback capability and validation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-754",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_shadow_deployment_leakage(self, node: ast.Call) -> None:
        """AIML240: Detect shadow deployment information leakage."""
        if not self.has_ml_framework:
            return
        
        # Check for shadow deployments
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "shadow" in line_text and ("deploy" in line_text or "model" in line_text):
            if "log" in line_text and "sanitize" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML240",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Shadow deployment - sanitize logs to prevent information leakage",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="A02",
                    cwe_id="CWE-200",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    # Phase 2.3.2: API & Endpoint Security (12 checks - AIML241-AIML252)
    
    def _check_missing_authentication_inference_api(self, node: ast.Call) -> None:
        """AIML241: Detect missing authentication on inference API."""
        if not self.has_ml_framework:
            return
        
        # Check for API route definitions
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if ("route" in line_text or "endpoint" in line_text or "api" in line_text) and ("predict" in line_text or "inference" in line_text):
            if "auth" not in line_text and "token" not in line_text and "key" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML241",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.CRITICAL,
                    message="Inference API - add authentication to prevent unauthorized access",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="A07",
                    cwe_id="CWE-306",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_model_endpoint_enumeration(self, node: ast.Call) -> None:
        """AIML242: Detect model endpoint enumeration risks."""
        if not self.has_ml_framework:
            return
        
        # Check for model listing endpoints
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if ("list" in line_text or "get_all" in line_text) and ("model" in line_text or "endpoint" in line_text):
            if "auth" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML242",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Model endpoint enumeration - restrict access to model listing",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="A01",
                    cwe_id="CWE-200",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_batch_inference_injection(self, node: ast.Call) -> None:
        """AIML243: Detect batch inference injection risks."""
        if not self.has_ml_framework:
            return
        
        # Check for batch inference
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "batch" in line_text and ("predict" in line_text or "inference" in line_text):
            if "validate" not in line_text and "sanitize" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML243",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Batch inference - validate each input to prevent injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_streaming_inference_attacks(self, node: ast.Call) -> None:
        """AIML244: Detect streaming inference attack risks."""
        if not self.has_ml_framework:
            return
        
        # Check for streaming inference
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "stream" in line_text and ("predict" in line_text or "inference" in line_text):
            if "validate" not in line_text and "rate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML244",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Streaming inference - implement rate limiting and validation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-400",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_model_cache_poisoning(self, node: ast.Call) -> None:
        """AIML245: Detect model cache poisoning risks."""
        if not self.has_ml_framework:
            return
        
        # Check for model caching
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "cache" in line_text and "model" in line_text:
            if "validate" not in line_text and "ttl" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML245",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Model cache - implement validation and TTL to prevent poisoning",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_prediction_logging_risks(self, node: ast.Call) -> None:
        """AIML246: Detect prediction logging PII risks."""
        if not self.has_ml_framework:
            return
        
        # Check for prediction logging
        if isinstance(node.func, ast.Attribute):
            if "log" in node.func.attr.lower():
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "predict" in line_text or "inference" in line_text or "output" in line_text:
                    if "sanitize" not in line_text and "redact" not in line_text:
                        violation = RuleViolation(
                            rule_id="AIML246",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.HIGH,
                            message="Prediction logging - sanitize to prevent PII leakage",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="A02",
                            cwe_id="CWE-532",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_model_warmup_vulnerabilities(self, node: ast.Call) -> None:
        """AIML247: Detect model warm-up vulnerabilities."""
        if not self.has_ml_framework:
            return
        
        # Check for model warm-up
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "warmup" in line_text.replace("-", "").replace("_", "") or "warm_up" in line_text:
            if "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML247",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="Model warm-up - validate warm-up data to prevent exploitation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_health_check_information_disclosure(self, node: ast.Call) -> None:
        """AIML248: Detect health check information disclosure."""
        if not self.has_ml_framework:
            return
        
        # Check for health check endpoints
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "health" in line_text or "readiness" in line_text or "liveness" in line_text:
            if "version" in line_text or "model" in line_text:
                violation = RuleViolation(
                    rule_id="AIML248",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="Health check - avoid exposing sensitive model information",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="A01",
                    cwe_id="CWE-200",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_metrics_endpoint_exposure(self, node: ast.Call) -> None:
        """AIML249: Detect metrics endpoint exposure."""
        if not self.has_ml_framework:
            return
        
        # Check for metrics endpoints
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "metrics" in line_text and ("endpoint" in line_text or "route" in line_text):
            if "auth" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML249",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Metrics endpoint - restrict access with authentication",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="A01",
                    cwe_id="CWE-200",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_model_metadata_leakage(self, node: ast.Call) -> None:
        """AIML250: Detect model metadata leakage."""
        if not self.has_ml_framework:
            return
        
        # Check for model metadata exposure
        if isinstance(node.func, ast.Attribute):
            if "metadata" in node.func.attr.lower() or "info" in node.func.attr.lower():
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                
                if "model" in line_text and ("return" in line_text or "response" in line_text):
                    violation = RuleViolation(
                        rule_id="AIML250",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Model metadata - avoid exposing sensitive model information",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="A01",
                        cwe_id="CWE-200",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_feature_flag_manipulation(self, node: ast.Call) -> None:
        """AIML251: Detect feature flag manipulation."""
        if not self.has_ml_framework:
            return
        
        # Check for feature flags
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "feature" in line_text and ("flag" in line_text or "toggle" in line_text):
            if "user" in line_text or "input" in line_text:
                violation = RuleViolation(
                    rule_id="AIML251",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Feature flag - prevent user manipulation of model behavior flags",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_circuit_breaker_bypass(self, node: ast.Call) -> None:
        """AIML252: Detect circuit breaker bypass risks."""
        if not self.has_ml_framework:
            return
        
        # Check for circuit breaker usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "circuit" in line_text and "breaker" in line_text:
            if "validate" not in line_text and "enforce" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML252",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="Circuit breaker - ensure proper enforcement to prevent bypass",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-754",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    # Phase 2.3.3: Edge & Mobile Deployment (8 checks - AIML253-AIML260)
    
    def _check_tflite_model_tampering(self, node: ast.Call) -> None:
        """AIML253: Detect TFLite model tampering."""
        if not self.has_tensorflow:
            return
        
        # Check for TFLite usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "tflite" in line_text:
            if "validate" not in line_text and "verify" not in line_text and "checksum" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML253",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="TFLite model - verify integrity with checksums to prevent tampering",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-494",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_coreml_injection(self, node: ast.Call) -> None:
        """AIML254: Detect Core ML injection risks."""
        if not self.has_ml_framework:
            return
        
        # Check for Core ML usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "coreml" in line_text:
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML254",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Core ML model - validate model source and integrity",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_onnx_mobile_risks(self, node: ast.Call) -> None:
        """AIML255: Detect ONNX mobile deployment risks."""
        if not self.has_ml_framework:
            return
        
        # Check for ONNX mobile usage
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "onnx" in line_text and ("mobile" in line_text or "edge" in line_text):
            if "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML255",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="ONNX mobile - validate model integrity and input data",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_quantized_model_vulnerabilities(self, node: ast.Call) -> None:
        """AIML256: Detect quantized model vulnerabilities."""
        if not self.has_ml_framework:
            return
        
        # Check for model quantization
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "quantiz" in line_text:
            if "validate" not in line_text and "test" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML256",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Quantized model - validate accuracy and behavior to prevent degradation attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_model_pruning_attacks(self, node: ast.Call) -> None:
        """AIML257: Detect model pruning attack risks."""
        if not self.has_ml_framework:
            return
        
        # Check for model pruning
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "prun" in line_text:
            if "validate" not in line_text and "test" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML257",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Model pruning - validate pruned model to prevent backdoor retention",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_knowledge_distillation_risks(self, node: ast.Call) -> None:
        """AIML258: Detect knowledge distillation risks."""
        if not self.has_ml_framework:
            return
        
        # Check for knowledge distillation
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "distill" in line_text:
            if "validate" not in line_text and "teacher" in line_text:
                violation = RuleViolation(
                    rule_id="AIML258",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Knowledge distillation - validate teacher model to prevent poisoning transfer",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_on_device_training_weaknesses(self, node: ast.Call) -> None:
        """AIML259: Detect on-device training weaknesses."""
        if not self.has_ml_framework:
            return
        
        # Check for on-device training
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if ("on" in line_text and "device" in line_text and "train" in line_text) or "ondevice" in line_text:
            if "validate" not in line_text and "secure" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML259",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="On-device training - validate training data and model updates",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_federated_learning_gaps(self, node: ast.Call) -> None:
        """AIML260: Detect federated learning security gaps."""
        if not self.has_ml_framework:
            return
        
        # Check for federated learning
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if "federated" in line_text and ("learn" in line_text or "train" in line_text):
            if "validate" not in line_text and "secure" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML260",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Federated learning - implement secure aggregation and client validation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_insecure_serialization(self, node: ast.Call) -> None:
        """AIML007: Detect insecure model serialization."""
        if isinstance(node.func, ast.Attribute):
            # PyTorch
            if node.func.attr == "load" and self._is_torch_module(node.func.value):
                # Check if weights_only parameter is missing or False
                has_weights_only = False
                for keyword in node.keywords:
                    if keyword.arg == "weights_only" and self._is_true_literal(keyword.value):
                        has_weights_only = True
                        
                if not has_weights_only:
                    violation = RuleViolation(
                        rule_id="AIML007",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Insecure model deserialization: torch.load without weights_only=True",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM03",
                        cwe_id="CWE-502",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_missing_input_validation(self, node: ast.Call) -> None:
        """AIML008: Detect missing input validation for ML models."""
        if not (self.has_ml_framework or self.has_pytorch or self.has_tensorflow):
            return
            
        # Check for model.predict() without validation
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["predict", "predict_proba", "forward", "call"]:
                # Check if there's input validation before the call
                if len(node.args) > 0:
                    violation = RuleViolation(
                        rule_id="AIML008",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Missing input validation before ML model inference",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="LLM03",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_gpu_memory_leakage(self, node: ast.Call) -> None:
        """AIML009: Detect GPU memory leakage patterns."""
        if not (self.has_pytorch or self.has_tensorflow):
            return
            
        # Check for missing .detach() or .cpu() calls with tensors
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["cuda", "to"] and self.has_pytorch:
                violation = RuleViolation(
                    rule_id="AIML009",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Potential GPU memory leak: Missing .detach() or .cpu() call",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM04",
                    cwe_id="CWE-400",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_model_inversion(self, node: ast.Assign) -> None:
        """AIML002: Detect model inversion attack vectors."""
        # Check for exposed model parameters
        if isinstance(node.value, ast.Call):
            if isinstance(node.value.func, ast.Attribute):
                if node.value.func.attr in ["parameters", "state_dict", "get_weights"]:
                    violation = RuleViolation(
                        rule_id="AIML002",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Model inversion risk: Exposed model parameters may allow attacks",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="LLM02",
                        cwe_id="CWE-200",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_training_data_poisoning(self, node: ast.Assign) -> None:
        """AIML003: Detect training data poisoning risks."""
        # Check for data loading without validation
        if isinstance(node.value, ast.Call):
            func_name = None
            # Check for function calls like dataset.load() or module.load_dataset()
            if isinstance(node.value.func, ast.Attribute):
                func_name = node.value.func.attr
            # Check for function calls like load_dataset()
            elif isinstance(node.value.func, ast.Name):
                func_name = node.value.func.id
                
            if func_name and func_name in ["load_dataset", "read_csv", "load_from_disk", "load_data"]:
                violation = RuleViolation(
                    rule_id="AIML003",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Training data poisoning risk: Unvalidated data source",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="LLM03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_model_extraction(self, node: ast.FunctionDef) -> None:
        """AIML005: Detect model extraction vulnerabilities."""
        # Check if function returns model predictions without rate limiting
        if "api" in node.name.lower() or "endpoint" in node.name.lower():
            # Look for predict/inference calls in function
            for child in ast.walk(node):
                if isinstance(child, ast.Call):
                    if isinstance(child.func, ast.Attribute):
                        if child.func.attr in ["predict", "predict_proba", "forward"]:
                            violation = RuleViolation(
                                rule_id="AIML005",
                                category=RuleCategory.SECURITY,
                                severity=RuleSeverity.HIGH,
                                message="Model extraction risk: API endpoint exposes model predictions without rate limiting",
                                line_number=node.lineno,
                                column=node.col_offset,
                                end_line_number=getattr(node, "end_lineno", node.lineno),
                                end_column=getattr(node, "end_col_offset", node.col_offset),
                                file_path=str(self.file_path),
                                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                                fix_applicability=FixApplicability.MANUAL,
                                fix_data=None,
                                owasp_id="LLM09",
                                cwe_id="CWE-799",
                                source_tool="pyguard",
                            )
                            self.violations.append(violation)
                            break

    def _check_ai_bias(self, node: ast.FunctionDef) -> None:
        """AIML006: Detect potential AI bias in code."""
        # Check for training or prediction functions without fairness checks
        if any(keyword in node.name.lower() for keyword in ["train", "fit", "predict"]):
            has_fairness_check = False
            for child in ast.walk(node):
                if isinstance(child, ast.Call):
                    if isinstance(child.func, ast.Attribute):
                        if any(x in child.func.attr.lower() for x in ["fairness", "bias", "demographic"]):
                            has_fairness_check = True
                            break
                            
            if not has_fairness_check and len(node.body) > 5:  # Only check substantial functions
                violation = RuleViolation(
                    rule_id="AIML006",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="AI bias risk: ML pipeline missing fairness/bias checks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.NONE,
                    fix_data=None,
                    owasp_id="LLM10",
                    cwe_id="CWE-1321",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_federated_learning(self, node: ast.FunctionDef) -> None:
        """AIML010: Detect federated learning privacy risks."""
        # Check for federated learning patterns without differential privacy
        if any(keyword in node.name.lower() for keyword in ["federated", "distributed", "aggregate"]):
            has_privacy = False
            for child in ast.walk(node):
                if isinstance(child, ast.Call):
                    func_name = None
                    # Check for method calls (object.method())
                    if isinstance(child.func, ast.Attribute):
                        func_name = child.func.attr
                    # Check for function calls (function())
                    elif isinstance(child.func, ast.Name):
                        func_name = child.func.id
                    
                    if func_name and any(x in func_name.lower() for x in ["differential", "privacy", "noise", "clip"]):
                        has_privacy = True
                        break
                            
            if not has_privacy:
                violation = RuleViolation(
                    rule_id="AIML010",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Federated learning privacy risk: Missing differential privacy or noise addition",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.NONE,
                    fix_data=None,
                    owasp_id="LLM06",
                    cwe_id="CWE-359",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    # Phase 2.4: Model Monitoring & Observability (20 checks - AIML261-AIML280)
    # These checks were added in the visit_Call method but need implementation
    
    def _check_data_drift_detection_bypass(self, node: ast.Call) -> None:
        """AIML261: Detect data drift detection bypass."""
        if not self.has_ml_framework:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        if ("drift" in line_text or "distribution" in line_text) and ("detect" in line_text or "monitor" in line_text):
            if "validate" not in line_text and "comprehensive" not in line_text:
                self.violations.append(RuleViolation(
                    rule_id="AIML261",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Drift detection - implement comprehensive monitoring to prevent bypass",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML06",
                    cwe_id="CWE-754",
                    source_tool="pyguard",
                ))
    
    def _check_concept_drift_manipulation(self, node: ast.Call) -> None:
        """AIML262: Detect concept drift manipulation."""
        pass  # Placeholder - monitoring feature
    
    def _check_model_performance_degradation_hiding(self, node: ast.Call) -> None:
        """AIML263: Detect model performance degradation hiding."""
        pass  # Placeholder - monitoring feature
    
    def _check_prediction_distribution_poisoning(self, node: ast.Call) -> None:
        """AIML264: Detect prediction distribution poisoning."""
        pass  # Placeholder - monitoring feature
    
    def _check_monitoring_pipeline_injection(self, node: ast.Call) -> None:
        """AIML265: Detect monitoring pipeline injection."""
        pass  # Placeholder - monitoring feature
    
    def _check_alert_threshold_manipulation(self, node: ast.Call) -> None:
        """AIML266: Detect alert threshold manipulation."""
        pass  # Placeholder - monitoring feature
    
    def _check_logging_framework_vulnerabilities(self, node: ast.Call) -> None:
        """AIML267: Detect logging framework vulnerabilities."""
        pass  # Placeholder - monitoring feature
    
    def _check_missing_drift_detection(self, node: ast.Call) -> None:
        """AIML268: Detect missing drift detection."""
        pass  # Placeholder - monitoring feature
    
    def _check_statistical_test_manipulation(self, node: ast.Call) -> None:
        """AIML269: Detect statistical test manipulation."""
        pass  # Placeholder - monitoring feature
    
    def _check_ground_truth_poisoning(self, node: ast.Call) -> None:
        """AIML270: Detect ground truth poisoning."""
        pass  # Placeholder - monitoring feature
    
    def _check_shap_value_manipulation(self, node: ast.Call) -> None:
        """AIML271: Detect SHAP value manipulation."""
        pass  # Placeholder - explainability feature
    
    def _check_lime_explanation_poisoning(self, node: ast.Call) -> None:
        """AIML272: Detect LIME explanation poisoning."""
        pass  # Placeholder - explainability feature
    
    def _check_feature_importance_injection(self, node: ast.Call) -> None:
        """AIML273: Detect feature importance injection."""
        pass  # Placeholder - explainability feature
    
    def _check_saliency_map_tampering(self, node: ast.Call) -> None:
        """AIML274: Detect saliency map tampering."""
        pass  # Placeholder - explainability feature
    
    def _check_attention_weight_manipulation(self, node: ast.Call) -> None:
        """AIML275: Detect attention weight manipulation."""
        pass  # Placeholder - explainability feature
    
    def _check_counterfactual_explanation_attacks(self, node: ast.Call) -> None:
        """AIML276: Detect counterfactual explanation attacks."""
        pass  # Placeholder - explainability feature
    
    def _check_model_card_injection(self, node: ast.Call) -> None:
        """AIML277: Detect model card injection."""
        pass  # Placeholder - explainability feature
    
    def _check_explanation_dashboard_vulnerabilities(self, node: ast.Call) -> None:
        """AIML278: Detect explanation dashboard vulnerabilities."""
        pass  # Placeholder - explainability feature
    
    def _check_fairness_metric_manipulation(self, node: ast.Call) -> None:
        """AIML279: Detect fairness metric manipulation."""
        pass  # Placeholder - explainability feature
    
    def _check_bias_detection_bypass(self, node: ast.Call) -> None:
        """AIML280: Detect bias detection bypass."""
        pass  # Placeholder - explainability feature
    
    # Phase 3.1: Computer Vision Security (35 checks - AIML281-AIML315)
    # Phase 3.1.1: Image Processing Vulnerabilities (15 checks)
    
    def _check_opencv_injection_attacks(self, node: ast.Call) -> None:
        """AIML281: Detect OpenCV injection attacks."""
        if isinstance(node.func, ast.Attribute):
            # Check for unsafe OpenCV operations with user input
            if node.func.attr in ["imread", "imdecode", "VideoCapture"]:
                if len(node.args) > 0:
                    arg = node.args[0]
                    if self._contains_user_input(arg) or isinstance(arg, ast.Name):
                        self.violations.append(RuleViolation(
                            rule_id="AIML281",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.HIGH,
                            message="OpenCV injection - validate file paths and inputs to prevent attacks",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="ML09",
                            cwe_id="CWE-20",
                            source_tool="pyguard",
                        ))
    
    def _check_pillow_buffer_overflows(self, node: ast.Call) -> None:
        """AIML282: Detect PIL/Pillow buffer overflow risks."""
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["open", "frombytes", "fromstring"]:
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                if "pil" in line_text or "image" in line_text:
                    if "validate" not in line_text and "verify" not in line_text:
                        self.violations.append(RuleViolation(
                            rule_id="AIML282",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.MEDIUM,
                            message="PIL/Pillow image loading - validate image format and size to prevent buffer overflows",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="ML09",
                            cwe_id="CWE-120",
                            source_tool="pyguard",
                        ))
    
    def _check_image_augmentation_poisoning(self, node: ast.Call) -> None:
        """AIML283: Detect image augmentation poisoning."""
        if isinstance(node.func, ast.Attribute):
            augmentation_funcs = ["augment", "transform", "apply", "randomcrop", "randomflip", "colorjitter"]
            if any(func in node.func.attr.lower() for func in augmentation_funcs):
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                if "albumentations" in line_text or "imgaug" in line_text or "torchvision" in line_text:
                    self.violations.append(RuleViolation(
                        rule_id="AIML283",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Image augmentation - validate augmentation parameters to prevent poisoning",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    ))
    
    def _check_exif_metadata_injection(self, node: ast.Call) -> None:
        """AIML284: Detect EXIF metadata injection."""
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["getexif", "_getexif", "exif"]:
                self.violations.append(RuleViolation(
                    rule_id="AIML284",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="EXIF metadata - sanitize metadata to prevent injection attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-74",
                    source_tool="pyguard",
                ))
    
    def _check_adversarial_patch_attacks(self, node: ast.Call) -> None:
        """AIML285: Detect adversarial patch attack vulnerabilities."""
        pass  # Placeholder - advanced vision security
    
    def _check_texture_synthesis_manipulation(self, node: ast.Call) -> None:
        """AIML286: Detect texture synthesis manipulation."""
        pass  # Placeholder - advanced vision security
    
    def _check_style_transfer_poisoning(self, node: ast.Call) -> None:
        """AIML287: Detect style transfer poisoning."""
        pass  # Placeholder - advanced vision security
    
    def _check_super_resolution_attacks(self, node: ast.Call) -> None:
        """AIML288: Detect super-resolution attacks."""
        pass  # Placeholder - advanced vision security
    
    def _check_image_segmentation_manipulation(self, node: ast.Call) -> None:
        """AIML289: Detect image segmentation manipulation."""
        pass  # Placeholder - advanced vision security
    
    def _check_object_detection_bypass(self, node: ast.Call) -> None:
        """AIML290: Detect object detection bypass."""
        pass  # Placeholder - advanced vision security
    
    def _check_facial_recognition_spoofing(self, node: ast.Call) -> None:
        """AIML291: Detect facial recognition spoofing."""
        pass  # Placeholder - advanced vision security
    
    def _check_ocr_injection_attacks(self, node: ast.Call) -> None:
        """AIML292: Detect OCR injection attacks."""
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ["image_to_string", "recognize", "ocr"]:
                self.violations.append(RuleViolation(
                    rule_id="AIML292",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="OCR processing - sanitize output to prevent injection via manipulated images",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-74",
                    source_tool="pyguard",
                ))
    
    def _check_image_captioning_poisoning(self, node: ast.Call) -> None:
        """AIML293: Detect image captioning poisoning."""
        pass  # Placeholder - multimodal security
    
    def _check_visual_question_answering_attacks(self, node: ast.Call) -> None:
        """AIML294: Detect visual question answering attacks."""
        pass  # Placeholder - multimodal security
    
    def _check_video_frame_injection(self, node: ast.Call) -> None:
        """AIML295: Detect video frame injection."""
        pass  # Placeholder - video security

    # Phase 3.1.2: Vision Transformers (AIML296-AIML305)
    
    def _check_patch_embedding_manipulation(self, node: ast.Call) -> None:
        """AIML296: Detect Vision Transformer patch embedding manipulation."""
        if not self.has_ml_framework:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Vision Transformer patch embedding operations
        if "patch" in line_text and ("embed" in line_text or "vit" in line_text or "vision_transformer" in line_text):
            if "validate" not in line_text and "check" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML296",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Vision Transformer patch embedding - validate patches to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML04",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_position_encoding_injection(self, node: ast.Call) -> None:
        """AIML297: Detect position encoding injection in Vision Transformers."""
        if not self.has_ml_framework:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for position encoding operations
        if "position" in line_text and ("encod" in line_text or "embed" in line_text):
            if "validate" not in line_text and "check" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML297",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Vision Transformer position encoding - validate to prevent injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML04",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_attention_mechanism_attacks(self, node: ast.Call) -> None:
        """AIML298: Detect attention mechanism attacks in Vision Transformers."""
        if not self.has_ml_framework:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for attention mechanism operations
        if "attention" in line_text and ("vit" in line_text or "transformer" in line_text):
            if "robust" not in line_text and "defense" not in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML298",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Vision Transformer attention - implement defenses against attention manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML04",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_vision_language_model_risks(self, node: ast.Call) -> None:
        """AIML299: Detect vision-language model risks (CLIP)."""
        if not self.has_ml_framework:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for CLIP and multimodal operations
        if "clip" in line_text or ("vision" in line_text and "language" in line_text):
            if "validate" not in line_text and "sanitize" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML299",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="CLIP model - validate both image and text inputs to prevent cross-modal attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML04",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_diffusion_model_injection(self, node: ast.Call) -> None:
        """AIML300: Detect diffusion model injection (Stable Diffusion)."""
        if not self.has_ml_framework:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for diffusion model operations
        if "stable" in line_text and "diffusion" in line_text:
            if "sanitize" not in line_text and "validate" not in line_text and "filter" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML300",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.CRITICAL,
                    message="Stable Diffusion - sanitize prompts and validate generated images",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML01",
                    cwe_id="CWE-74",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_text_to_image_prompt_injection(self, node: ast.Call) -> None:
        """AIML301: Detect text-to-image prompt injection."""
        if not self.has_ml_framework:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for text-to-image generation
        if ("text" in line_text or "prompt" in line_text) and ("image" in line_text or "generate" in line_text):
            if "sanitize" not in line_text and "validate" not in line_text and "filter" not in line_text:
                # Check if this is a generation call
                if isinstance(node.func, ast.Attribute) and "generate" in node.func.attr.lower():
                    violation = RuleViolation(
                        rule_id="AIML301",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.CRITICAL,
                        message="Text-to-image generation - sanitize prompts to prevent harmful content",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML01",
                        cwe_id="CWE-74",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_image_to_image_manipulation(self, node: ast.Call) -> None:
        """AIML302: Detect image-to-image manipulation."""
        pass  # Placeholder - advanced image manipulation detection
    
    def _check_inpainting_attacks(self, node: ast.Call) -> None:
        """AIML303: Detect inpainting attacks."""
        pass  # Placeholder - inpainting security
    
    def _check_outpainting_vulnerabilities(self, node: ast.Call) -> None:
        """AIML304: Detect outpainting vulnerabilities."""
        pass  # Placeholder - outpainting security
    
    def _check_multimodal_fusion_risks(self, node: ast.Call) -> None:
        """AIML305: Detect multimodal fusion risks."""
        pass  # Placeholder - multimodal security
    
    # Phase 3.1.3: CNN & Architecture Security (AIML306-AIML315)
    
    def _check_resnet_skip_connection_attacks(self, node: ast.Call) -> None:
        """AIML306: Detect ResNet skip connection attacks."""
        if not self.has_ml_framework:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for ResNet skip connections
        if "resnet" in line_text or ("skip" in line_text and "connection" in line_text):
            if "validate" not in line_text and "gradient" in line_text:
                violation = RuleViolation(
                    rule_id="AIML306",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="ResNet architecture - validate skip connections to prevent gradient manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML04",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_densenet_feature_concatenation(self, node: ast.Call) -> None:
        """AIML307: Detect DenseNet feature concatenation vulnerabilities."""
        pass  # Placeholder - DenseNet security
    
    def _check_efficientnet_scaling_manipulation(self, node: ast.Call) -> None:
        """AIML308: Detect EfficientNet scaling manipulation."""
        if not self.has_ml_framework:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for EfficientNet scaling operations
        if "efficientnet" in line_text or ("compound" in line_text and "scal" in line_text):
            if "validate" not in line_text and "limit" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML308",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="EfficientNet - validate compound scaling parameters to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-400",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_mobilenet_depthwise_convolution_risks(self, node: ast.Call) -> None:
        """AIML309: Detect MobileNet depthwise convolution risks."""
        pass  # Placeholder - MobileNet security
    
    def _check_squeezenet_fire_module_injection(self, node: ast.Call) -> None:
        """AIML310: Detect SqueezeNet fire module injection."""
        pass  # Placeholder - SqueezeNet security
    
    def _check_neural_architecture_search_poisoning(self, node: ast.Call) -> None:
        """AIML311: Detect neural architecture search poisoning."""
        if not self.has_ml_framework:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for NAS operations
        if "nas" in line_text or ("neural" in line_text and "architecture" in line_text and "search" in line_text):
            if "validate" not in line_text and "sanitize" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML311",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="NAS - validate search space and architecture selection to prevent poisoning",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_activation_function_vulnerabilities(self, node: ast.Call) -> None:
        """AIML312: Detect activation function vulnerabilities."""
        if not self.has_ml_framework:
            return
        
        # Check for custom activation functions
        if isinstance(node.func, ast.Attribute):
            if "activation" in node.func.attr.lower() or "relu" in node.func.attr.lower():
                line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
                if "custom" in line_text and "validate" not in line_text:
                    violation = RuleViolation(
                        rule_id="AIML312",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Custom activation functions - validate for numerical stability and gradient flow",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-754",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_pooling_layer_manipulation(self, node: ast.Call) -> None:
        """AIML313: Detect pooling layer manipulation."""
        pass  # Placeholder - pooling layer security
    
    def _check_dropout_bypass_techniques(self, node: ast.Call) -> None:
        """AIML314: Detect dropout bypass techniques."""
        if not self.has_ml_framework:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for dropout operations
        if "dropout" in line_text:
            if "training" in line_text and "=" in line_text and ("true" in line_text or "false" in line_text):
                # Check if mode is hardcoded instead of using model.train()/model.eval()
                violation = RuleViolation(
                    rule_id="AIML314",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Dropout layers - ensure proper training/inference mode to prevent bypass",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-754",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_batch_normalization_attacks(self, node: ast.Call) -> None:
        """AIML315: Detect batch normalization attacks."""
        if not self.has_ml_framework:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for batch normalization operations
        if "batchnorm" in line_text or "batch_norm" in line_text or ("bn" in line_text and "layer" in line_text):
            if "validate" not in line_text and "statistics" in line_text:
                violation = RuleViolation(
                    rule_id="AIML315",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Batch normalization - validate statistics to prevent poisoning attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    # Phase 3.2: Natural Language Processing Security (35 checks - AIML316-AIML350)
    
    # Phase 3.2.1: Text Processing Security (15 checks - AIML316-AIML330)
    
    def _check_tokenization_injection(self, node: ast.Call) -> None:
        """AIML316: Detect tokenization injection vulnerabilities."""
        if not self.has_transformers and not self.has_llm_framework:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for tokenization without validation
        if any(keyword in line_text for keyword in ["tokenize", "encode", "tokenizer("]):
            if "validate" not in line_text and ("user" in line_text or "input" in line_text):
                violation = RuleViolation(
                    rule_id="AIML316",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Tokenization - validate input text to prevent injection attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML01",
                    cwe_id="CWE-74",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_subword_tokenization_bypass(self, node: ast.Call) -> None:
        """AIML317: Detect subword tokenization bypass vulnerabilities."""
        if not self.has_transformers:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for subword tokenization without sanitization
        if "subword" in line_text or "split" in line_text:
            if "sanitize" not in line_text and "tokenize" in line_text:
                violation = RuleViolation(
                    rule_id="AIML317",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Subword tokenization - sanitize inputs to prevent bypass attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML01",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_bpe_manipulation(self, node: ast.Call) -> None:
        """AIML318: Detect BPE (Byte Pair Encoding) manipulation vulnerabilities."""
        if not self.has_transformers:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for BPE operations without validation
        if "bpe" in line_text or "byte_pair" in line_text:
            if "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML318",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="BPE encoding - validate encoding parameters to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_wordpiece_attack_vectors(self, node: ast.Call) -> None:
        """AIML319: Detect WordPiece attack vectors."""
        if not self.has_transformers:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for WordPiece tokenization without security measures
        if "wordpiece" in line_text:
            if "validate" not in line_text and "tokenize" in line_text:
                violation = RuleViolation(
                    rule_id="AIML319",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="WordPiece tokenization - validate tokens to prevent attack vectors",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_sentencepiece_vulnerabilities(self, node: ast.Call) -> None:
        """AIML320: Detect SentencePiece vulnerabilities."""
        if not self.has_transformers:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for SentencePiece operations
        if "sentencepiece" in line_text or "spm" in line_text:
            if "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML320",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="SentencePiece - validate model and inputs to prevent vulnerabilities",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_text_normalization_bypass(self, node: ast.Call) -> None:
        """AIML321: Detect text normalization bypass vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for text normalization
        if any(keyword in line_text for keyword in ["normalize", "lower", "upper", "strip"]):
            if "unicode" in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML321",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Text normalization - validate Unicode handling to prevent bypass",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML01",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_stopword_removal_manipulation(self, node: ast.Call) -> None:
        """AIML322: Detect stop word removal manipulation."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for stop word operations
        if "stopword" in line_text or "stop_word" in line_text:
            if "custom" in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML322",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="Stop word removal - validate custom stop word lists to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_stemming_lemmatization_attacks(self, node: ast.Call) -> None:
        """AIML323: Detect stemming/lemmatization attacks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for stemming/lemmatization
        if any(keyword in line_text for keyword in ["stem", "lemma", "lemmatize", "porter", "snowball"]):
            if "validate" not in line_text and "model" in line_text:
                violation = RuleViolation(
                    rule_id="AIML323",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="Stemming/lemmatization - validate models to prevent attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_ner_injection(self, node: ast.Call) -> None:
        """AIML324: Detect named entity recognition injection."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for NER operations
        if "ner" in line_text or "named_entity" in line_text or "entity_recognition" in line_text:
            if "validate" not in line_text and ("user" in line_text or "input" in line_text):
                violation = RuleViolation(
                    rule_id="AIML324",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="NER - validate inputs to prevent entity injection attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML01",
                    cwe_id="CWE-74",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_pos_tagging_manipulation(self, node: ast.Call) -> None:
        """AIML325: Detect POS tagging manipulation."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for POS tagging
        if "pos_tag" in line_text or "part_of_speech" in line_text:
            if "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML325",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="POS tagging - validate tags to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_dependency_parsing_poisoning(self, node: ast.Call) -> None:
        """AIML326: Detect dependency parsing poisoning."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for dependency parsing
        if "dependency" in line_text and "parse" in line_text:
            if "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML326",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Dependency parsing - validate parse trees to prevent poisoning",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_sentiment_analysis_bias(self, node: ast.Call) -> None:
        """AIML327: Detect sentiment analysis bias."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for sentiment analysis
        if "sentiment" in line_text:
            if "bias" not in line_text and "fairness" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML327",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Sentiment analysis - validate for bias and fairness",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML06",
                    cwe_id="CWE-754",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_text_classification_backdoors(self, node: ast.Call) -> None:
        """AIML328: Detect text classification backdoors."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for text classification
        if "classify" in line_text or "classification" in line_text:
            if "text" in line_text and "backdoor" not in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML328",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Text classification - validate training data to prevent backdoor attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-912",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_sequence_labeling_attacks(self, node: ast.Call) -> None:
        """AIML329: Detect sequence labeling attacks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for sequence labeling
        if "sequence" in line_text and ("label" in line_text or "tag" in line_text):
            if "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML329",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Sequence labeling - validate labels to prevent attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_coreference_resolution_manipulation(self, node: ast.Call) -> None:
        """AIML330: Detect coreference resolution manipulation."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for coreference resolution
        if "coref" in line_text or "coreference" in line_text:
            if "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML330",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="Coreference resolution - validate resolutions to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    # Phase 3.2.2: Transformer Architectures (12 checks - AIML331-AIML342)
    
    def _check_bert_finetuning_injection(self, node: ast.Call) -> None:
        """AIML331: Detect BERT fine-tuning injection vulnerabilities."""
        if not self.has_transformers:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for BERT fine-tuning
        if "bert" in line_text and ("finetune" in line_text or "fine_tune" in line_text or "train" in line_text):
            if "validate" not in line_text and "data" in line_text:
                violation = RuleViolation(
                    rule_id="AIML331",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="BERT fine-tuning - validate training data to prevent injection attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_gpt_prompt_engineering_attacks(self, node: ast.Call) -> None:
        """AIML332: Detect GPT prompt engineering attacks."""
        if not self.has_transformers and not self.has_llm_framework:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for GPT usage
        if "gpt" in line_text and ("prompt" in line_text or "generate" in line_text):
            if "sanitize" not in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML332",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.CRITICAL,
                    message="GPT prompt engineering - sanitize prompts to prevent attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML01",
                    cwe_id="CWE-74",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_t5_encoder_decoder_manipulation(self, node: ast.Call) -> None:
        """AIML333: Detect T5 encoder-decoder manipulation."""
        if not self.has_transformers:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for T5 usage
        if "t5" in line_text and ("encoder" in line_text or "decoder" in line_text):
            if "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML333",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="T5 encoder-decoder - validate inputs to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_bart_denoising_poisoning(self, node: ast.Call) -> None:
        """AIML334: Detect BART denoising poisoning."""
        if not self.has_transformers:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for BART usage
        if "bart" in line_text and ("denoise" in line_text or "train" in line_text):
            if "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML334",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="BART denoising - validate training data to prevent poisoning",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_roberta_masked_lm(self, node: ast.Call) -> None:
        """AIML335: Detect RoBERTa masked language modeling vulnerabilities."""
        if not self.has_transformers:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for RoBERTa usage
        if "roberta" in line_text and ("mask" in line_text or "mlm" in line_text):
            if "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML335",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="RoBERTa MLM - validate masked tokens to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_electra_attacks(self, node: ast.Call) -> None:
        """AIML336: Detect ELECTRA discriminator/generator attacks."""
        if not self.has_transformers:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for ELECTRA usage
        if "electra" in line_text and ("discriminator" in line_text or "generator" in line_text):
            if "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML336",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="ELECTRA - validate discriminator/generator to prevent attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_xlnet_permutation_lm(self, node: ast.Call) -> None:
        """AIML337: Detect XLNet permutation language modeling vulnerabilities."""
        if not self.has_transformers:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for XLNet usage
        if "xlnet" in line_text and "permutation" in line_text:
            if "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML337",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="XLNet permutation LM - validate permutations to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_albert_parameter_sharing_risks(self, node: ast.Call) -> None:
        """AIML338: Detect ALBERT parameter sharing risks."""
        if not self.has_transformers:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for ALBERT usage
        if "albert" in line_text and "parameter" in line_text:
            if "validate" not in line_text and "sharing" in line_text:
                violation = RuleViolation(
                    rule_id="AIML338",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="ALBERT parameter sharing - validate shared parameters for security",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_distilbert_knowledge_distillation(self, node: ast.Call) -> None:
        """AIML339: Detect DistilBERT knowledge distillation vulnerabilities."""
        if not self.has_transformers:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for DistilBERT usage
        if "distilbert" in line_text or ("distil" in line_text and "bert" in line_text):
            if "validate" not in line_text and "teacher" in line_text:
                violation = RuleViolation(
                    rule_id="AIML339",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="DistilBERT - validate teacher model to prevent vulnerability inheritance",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_deberta_disentangled_attention(self, node: ast.Call) -> None:
        """AIML340: Detect DeBERTa disentangled attention vulnerabilities."""
        if not self.has_transformers:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for DeBERTa usage
        if "deberta" in line_text and "attention" in line_text:
            if "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML340",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="DeBERTa disentangled attention - validate attention mechanisms",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML04",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_longformer_sliding_window_attacks(self, node: ast.Call) -> None:
        """AIML341: Detect Longformer sliding window attacks."""
        if not self.has_transformers:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Longformer usage
        if "longformer" in line_text and "window" in line_text:
            if "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML341",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Longformer sliding window - validate window size to prevent attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML04",
                    cwe_id="CWE-400",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_bigbird_sparse_attention_manipulation(self, node: ast.Call) -> None:
        """AIML342: Detect BigBird sparse attention manipulation."""
        if not self.has_transformers:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for BigBird usage
        if "bigbird" in line_text and ("sparse" in line_text or "attention" in line_text):
            if "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML342",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="BigBird sparse attention - validate attention patterns to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML04",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    # Phase 3.2.3: Embeddings & Representations (8 checks - AIML343-AIML350)
    
    def _check_word2vec_poisoning(self, node: ast.Call) -> None:
        """AIML343: Detect Word2Vec poisoning vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Word2Vec usage
        if "word2vec" in line_text or "w2v" in line_text:
            if "train" in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML343",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Word2Vec - validate training corpus to prevent poisoning attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_glove_embedding_manipulation(self, node: ast.Call) -> None:
        """AIML344: Detect GloVe embedding manipulation."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for GloVe usage
        if "glove" in line_text:
            if "load" in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML344",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="GloVe embeddings - verify integrity before loading to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-494",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_fasttext_subword_attacks(self, node: ast.Call) -> None:
        """AIML345: Detect FastText subword attacks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for FastText usage
        if "fasttext" in line_text:
            if "subword" in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML345",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="FastText subword - validate subword embeddings to prevent attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_elmo_contextualized_embedding_injection(self, node: ast.Call) -> None:
        """AIML346: Detect ELMo contextualized embedding injection."""
        if not self.has_transformers:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for ELMo usage
        if "elmo" in line_text:
            if "context" in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML346",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="ELMo embeddings - validate context to prevent injection attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-74",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_sentence_bert_manipulation(self, node: ast.Call) -> None:
        """AIML347: Detect Sentence-BERT manipulation."""
        if not self.has_transformers:
            return
        
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Sentence-BERT usage
        if "sentence" in line_text and "bert" in line_text:
            if "encode" in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML347",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Sentence-BERT - validate sentence encodings to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_universal_sentence_encoder_risks(self, node: ast.Call) -> None:
        """AIML348: Detect Universal Sentence Encoder risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Universal Sentence Encoder usage
        if "universal" in line_text and "sentence" in line_text and "encoder" in line_text:
            if "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML348",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Universal Sentence Encoder - validate inputs to prevent risks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_doc2vec_document_poisoning(self, node: ast.Call) -> None:
        """AIML349: Detect Doc2Vec document poisoning."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Doc2Vec usage
        if "doc2vec" in line_text or "d2v" in line_text:
            if "train" in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML349",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Doc2Vec - validate document corpus to prevent poisoning attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_graph_embedding_attacks(self, node: ast.Call) -> None:
        """AIML350: Detect graph embedding attacks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for graph embedding usage
        if "graph" in line_text and "embed" in line_text:
            if "validate" not in line_text and ("node" in line_text or "edge" in line_text):
                violation = RuleViolation(
                    rule_id="AIML350",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Graph embeddings - validate graph structure to prevent attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    # Phase 3.3: Reinforcement Learning Security (AIML351-370)
    
    def _check_q_learning_poisoning(self, node: ast.Call) -> None:
        """AIML351: Detect Q-learning poisoning vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Q-learning without validation
        if ("q_table" in line_text or "q_value" in line_text or "q[" in line_text):
            if "validate" not in line_text and "update" in line_text:
                violation = RuleViolation(
                    rule_id="AIML351",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Q-learning poisoning risk - validate Q-value updates to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_dqn_replay_buffer_manipulation(self, node: ast.Call) -> None:
        """AIML352: Detect DQN replay buffer manipulation vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for replay buffer without validation
        if ("replay" in line_text and "buffer" in line_text) or "memory.add" in line_text:
            if "validate" not in line_text and "verify" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML352",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="DQN replay buffer manipulation risk - validate experiences before adding to buffer",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_policy_gradient_attacks(self, node: ast.Call) -> None:
        """AIML353: Detect policy gradient attack vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for policy gradient without robustness checks
        if "policy" in line_text and "gradient" in line_text:
            if "clip" not in line_text and "bound" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML353",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Policy gradient attack risk - add gradient clipping to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_actor_critic_tampering(self, node: ast.Call) -> None:
        """AIML354: Detect actor-critic tampering vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for actor-critic without integrity checks
        if ("actor" in line_text or "critic" in line_text) and "update" in line_text:
            if "validate" not in line_text and ("state" in line_text or "action" in line_text):
                violation = RuleViolation(
                    rule_id="AIML354",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Actor-critic tampering risk - validate network updates to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_ppo_injection(self, node: ast.Call) -> None:
        """AIML355: Detect PPO injection vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for PPO without proper clipping
        if "ppo" in line_text or ("proximal" in line_text and "policy" in line_text):
            if "clip" not in line_text and "ratio" in line_text:
                violation = RuleViolation(
                    rule_id="AIML355",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="PPO injection risk - ensure proper probability ratio clipping",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_a3c_risks(self, node: ast.Call) -> None:
        """AIML356: Detect A3C asynchronous training risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for A3C without synchronization validation
        if "a3c" in line_text or ("async" in line_text and "actor" in line_text):
            if "lock" not in line_text and "sync" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML356",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="A3C risk - validate asynchronous updates to prevent race conditions",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-362",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_ddpg_attacks(self, node: ast.Call) -> None:
        """AIML357: Detect DDPG attack vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for DDPG without action bounds
        if "ddpg" in line_text or ("deterministic" in line_text and "policy" in line_text):
            if "clip" not in line_text and "action" in line_text:
                violation = RuleViolation(
                    rule_id="AIML357",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="DDPG attack risk - clip actions to valid bounds to prevent exploitation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_sac_vulnerabilities(self, node: ast.Call) -> None:
        """AIML358: Detect SAC vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for SAC without entropy regularization validation
        if "sac" in line_text or ("soft" in line_text and "actor" in line_text):
            if "entropy" in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML358",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="SAC vulnerability - validate entropy coefficient to prevent exploitation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_td3_manipulation(self, node: ast.Call) -> None:
        """AIML359: Detect TD3 manipulation vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for TD3 without proper noise handling
        if "td3" in line_text or ("twin" in line_text and "delayed" in line_text):
            if "noise" in line_text and "clip" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML359",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="TD3 manipulation risk - clip target policy noise to prevent attacks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_trpo_bypass(self, node: ast.Call) -> None:
        """AIML360: Detect TRPO bypass vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for TRPO without trust region validation
        if "trpo" in line_text or ("trust" in line_text and "region" in line_text):
            if "kl" in line_text and "constraint" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML360",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="TRPO bypass risk - enforce KL divergence constraint to prevent policy deviation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_reward_function_poisoning(self, node: ast.Call) -> None:
        """AIML361: Detect reward function poisoning vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for reward function without validation
        if "reward" in line_text and ("return" in line_text or "=" in line_text):
            if "validate" not in line_text and "clip" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML361",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Reward function poisoning risk - validate and clip reward values",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_reward_shaping_attacks(self, node: ast.Call) -> None:
        """AIML362: Detect reward shaping attack vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for reward shaping without bounds
        if "reward" in line_text and ("shap" in line_text or "bonus" in line_text or "penalty" in line_text):
            if "bound" not in line_text and "limit" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML362",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Reward shaping attack risk - bound shaped rewards to prevent exploitation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_gym_environment_injection(self, node: ast.Call) -> None:
        """AIML363: Detect OpenAI Gym environment injection vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for gym.make with untrusted input
        if "gym.make" in line_text or "env = gym" in line_text:
            if "validate" not in line_text and ("input" in line_text or "user" in line_text):
                violation = RuleViolation(
                    rule_id="AIML363",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Gym environment injection risk - validate environment ID before creation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_gymnasium_api_manipulation(self, node: ast.Call) -> None:
        """AIML364: Detect Gymnasium API manipulation vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for gymnasium with unvalidated parameters
        if "gymnasium" in line_text or "gym.make" in line_text:
            if "kwargs" in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML364",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Gymnasium API manipulation risk - validate environment kwargs",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_custom_environment_backdoors(self, node: ast.Call) -> None:
        """AIML365: Detect custom environment backdoor vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for custom env without validation
        if ("class" in line_text and "env" in line_text) or "gym.env" in line_text:
            if "step" in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML365",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Custom environment backdoor risk - validate environment behavior",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-506",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_state_space_poisoning(self, node: ast.Call) -> None:
        """AIML366: Detect state space poisoning vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for state space without bounds
        if "observation_space" in line_text or "state_space" in line_text:
            if "bound" not in line_text and "limit" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML366",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="State space poisoning risk - enforce observation space bounds",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_action_space_tampering(self, node: ast.Call) -> None:
        """AIML367: Detect action space tampering vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for action space without validation
        if "action_space" in line_text:
            if "validate" not in line_text and ("sample" in line_text or "contains" in line_text):
                violation = RuleViolation(
                    rule_id="AIML367",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Action space tampering risk - validate actions against defined space",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_observation_function_attacks(self, node: ast.Call) -> None:
        """AIML368: Detect observation function attack vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for observation function without sanitization
        if "observation" in line_text or "_get_obs" in line_text:
            if "sanitize" not in line_text and "return" in line_text:
                violation = RuleViolation(
                    rule_id="AIML368",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Observation function attack risk - sanitize observations before return",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_reward_function_manipulation(self, node: ast.Call) -> None:
        """AIML369: Detect reward function manipulation vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for reward function in environment without bounds
        if ("def" in line_text and "reward" in line_text) or "_compute_reward" in line_text:
            if "clip" not in line_text and "bound" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML369",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Reward function manipulation risk - clip reward values to prevent gaming",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_multiagent_rl_vulnerabilities(self, node: ast.Call) -> None:
        """AIML370: Detect multi-agent RL vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for multi-agent without adversarial robustness
        if "multi" in line_text and "agent" in line_text:
            if "adversarial" not in line_text and "robust" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML370",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Multi-agent RL vulnerability - implement adversarial robustness checks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    # Phase 3.4: Specialized ML Libraries (AIML371-380)
    
    def _check_optuna_trial_manipulation(self, node: ast.Call) -> None:
        """AIML371: Detect Optuna trial manipulation vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Optuna trial without validation
        if "optuna" in line_text and ("trial" in line_text or "suggest" in line_text):
            if "validate" not in line_text and ("param" in line_text or "hyperparam" in line_text):
                violation = RuleViolation(
                    rule_id="AIML371",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Optuna trial manipulation risk - validate hyperparameter suggestions",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_ray_tune_search_space_poisoning(self, node: ast.Call) -> None:
        """AIML372: Detect Ray Tune search space poisoning vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Ray Tune without search space validation
        if ("ray.tune" in line_text or "tune.run" in line_text):
            if "config" in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML372",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Ray Tune search space poisoning risk - validate search configuration",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_hyperopt_objective_injection(self, node: ast.Call) -> None:
        """AIML373: Detect Hyperopt objective function injection vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Hyperopt objective without validation
        if "hyperopt" in line_text and ("fmin" in line_text or "objective" in line_text):
            if "validate" not in line_text and "space" in line_text:
                violation = RuleViolation(
                    rule_id="AIML373",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Hyperopt objective injection risk - validate search space and objective",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_autosklearn_pipeline_tampering(self, node: ast.Call) -> None:
        """AIML374: Detect Auto-sklearn pipeline tampering vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Auto-sklearn without pipeline validation
        if ("autosklearn" in line_text or "auto-sklearn" in line_text):
            if "pipeline" in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML374",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Auto-sklearn pipeline tampering risk - validate generated pipelines",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_autokeras_architecture_injection(self, node: ast.Call) -> None:
        """AIML375: Detect AutoKeras architecture injection vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for AutoKeras without architecture validation
        if "autokeras" in line_text:
            if ("search" in line_text or "automodel" in line_text) and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML375",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="AutoKeras architecture injection risk - validate generated architectures",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_pytorch_geometric_injection(self, node: ast.Call) -> None:
        """AIML376: Detect PyTorch Geometric injection vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for PyG without data validation
        if ("torch_geometric" in line_text or "pyg" in line_text):
            if "data" in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML376",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="PyTorch Geometric injection risk - validate graph data before processing",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_dgl_manipulation(self, node: ast.Call) -> None:
        """AIML377: Detect DGL manipulation vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for DGL without graph validation
        if "dgl" in line_text:
            if ("graph" in line_text or "heterograph" in line_text) and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML377",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="DGL manipulation risk - validate graph structure before use",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_graph_structure_poisoning(self, node: ast.Call) -> None:
        """AIML378: Detect graph structure poisoning vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for graph construction without validation
        if ("add_edge" in line_text or "add_node" in line_text or "graph(" in line_text):
            if "validate" not in line_text and ("untrusted" in line_text or "user" in line_text or "external" in line_text):
                violation = RuleViolation(
                    rule_id="AIML378",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Graph structure poisoning risk - validate graph topology from untrusted sources",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_node_feature_injection(self, node: ast.Call) -> None:
        """AIML379: Detect node feature injection vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for node features without sanitization
        if ("node" in line_text and "feature" in line_text) or "ndata" in line_text:
            if "sanitize" not in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML379",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Node feature injection risk - sanitize node features before use",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_message_passing_attacks(self, node: ast.Call) -> None:
        """AIML380: Detect message passing attack vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for message passing without bounds
        if "message" in line_text and "pass" in line_text:
            if "bound" not in line_text and "clip" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML380",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Message passing attack risk - bound message values to prevent overflow",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    # Phase 4.1: Jupyter & Notebook Security Detection Methods (AIML381-405)
    
    def _check_untrusted_notebook_execution(self, node: ast.Call) -> None:
        """AIML381: Detect untrusted notebook execution."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for notebook execution from untrusted sources
        if any(x in line_text for x in ["nbformat", "nbclient", "execute", "run_cell"]):
            if any(x in line_text for x in ["untrusted", "download", "http", "url", "fetch", "remote"]):
                violation = RuleViolation(
                    rule_id="AIML381",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.CRITICAL,
                    message="Executing untrusted notebook - validate source and content before execution",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="A03",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_output_cell_code_injection(self, node: ast.Call) -> None:
        """AIML382: Detect output cell code injection."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for display/output with untrusted data
        if any(x in line_text for x in ["display(", "output(", "render(", "show("]):
            if not any(x in line_text for x in ["sanitize", "escape", "safe"]):
                violation = RuleViolation(
                    rule_id="AIML382",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Output cell may contain executable code - sanitize notebook outputs",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="A03",
                    cwe_id="CWE-79",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_matplotlib_backend_exploitation(self, node: ast.Call) -> None:
        """AIML383: Detect Matplotlib backend exploitation risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for matplotlib.use() with unsafe backends
        if "matplotlib" in line_text and "use(" in line_text:
            if any(x in line_text for x in ["'webagg'", '"webagg"', "'qt5agg'", '"qt5agg"']):
                violation = RuleViolation(
                    rule_id="AIML383",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Matplotlib backend configuration - use safe backend to prevent exploitation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_ipython_magic_injection(self, node: ast.Call) -> None:
        """AIML384: Detect IPython magic command injection."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for IPython magic with user input
        if ("run_line_magic" in line_text or "run_cell_magic" in line_text or "%" in line_text):
            if any(x in line_text for x in ["user_input", "request", "input(", "argv", "sys.argv"]):
                violation = RuleViolation(
                    rule_id="AIML384",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="IPython magic with user input - validate commands to prevent injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="A03",
                    cwe_id="CWE-78",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_kernel_manipulation_attacks(self, node: ast.Call) -> None:
        """AIML385: Detect kernel manipulation attacks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for kernel operations
        if any(x in line_text for x in ["kernel", "ipykernel", "kernelmanager"]):
            if any(x in line_text for x in ["restart", "interrupt", "shutdown", "execute"]):
                violation = RuleViolation(
                    rule_id="AIML385",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Jupyter kernel manipulation - validate kernel operations",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="A03",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_widget_state_poisoning(self, node: ast.Call) -> None:
        """AIML386: Detect widget state poisoning."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for ipywidgets without validation
        if "widget" in line_text or "ipywidgets" in line_text:
            if "state" in line_text and "validate" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML386",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Jupyter widget state - validate state data to prevent poisoning",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_display_object_injection(self, node: ast.Call) -> None:
        """AIML387: Detect display object injection."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for IPython display with untrusted data
        if any(x in line_text for x in ["display.display(", "display.html(", "display.javascript("]):
            if not any(x in line_text for x in ["sanitize", "escape", "safe"]):
                violation = RuleViolation(
                    rule_id="AIML387",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Display object with untrusted data - sanitize before rendering",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="A03",
                    cwe_id="CWE-79",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_markdown_cell_xss(self, node: ast.Call) -> None:
        """AIML388: Detect Markdown cell XSS."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Markdown rendering with user input
        if "markdown" in line_text or "display.markdown(" in line_text:
            if any(x in line_text for x in ["user_input", "request", "input(", "f\"", "f'"]):
                violation = RuleViolation(
                    rule_id="AIML388",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Markdown cell with user input - sanitize to prevent XSS",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="A03",
                    cwe_id="CWE-79",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_latex_injection(self, node: ast.Call) -> None:
        """AIML389: Detect LaTeX injection in outputs."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for LaTeX rendering with user input
        if "latex" in line_text or "display.latex(" in line_text:
            if any(x in line_text for x in ["user_input", "request", "input(", "f\"", "f'"]):
                violation = RuleViolation(
                    rule_id="AIML389",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="LaTeX rendering with user input - sanitize to prevent injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="A03",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_svg_code_execution(self, node: ast.Call) -> None:
        """AIML390: Detect SVG code execution risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for SVG rendering with untrusted data
        if "svg" in line_text or "display.svg(" in line_text:
            if not any(x in line_text for x in ["sanitize", "escape", "safe"]):
                violation = RuleViolation(
                    rule_id="AIML390",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="SVG output - sanitize to prevent JavaScript execution",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="A03",
                    cwe_id="CWE-79",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_html_display_risks(self, node: ast.Call) -> None:
        """AIML391: Detect HTML display risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for HTML display with untrusted content
        if "html(" in line_text or "display.html(" in line_text:
            if not any(x in line_text for x in ["sanitize", "escape", "safe", "bleach"]):
                violation = RuleViolation(
                    rule_id="AIML391",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="HTML display with untrusted content - sanitize to prevent injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="A03",
                    cwe_id="CWE-79",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_javascript_execution_notebooks(self, node: ast.Call) -> None:
        """AIML392: Detect JavaScript execution in notebooks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for JavaScript in notebooks
        if any(x in line_text for x in ["display.javascript(", "%%javascript", "javascript(", "js("]):
            violation = RuleViolation(
                rule_id="AIML392",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.CRITICAL,
                message="JavaScript in notebook - avoid or sanitize to prevent malicious execution",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="A03",
                cwe_id="CWE-79",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_nbviewer_security_gaps(self, node: ast.Call) -> None:
        """AIML393: Detect nbviewer security gaps."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for nbviewer usage
        if "nbviewer" in line_text or "nb_viewer" in line_text:
            violation = RuleViolation(
                rule_id="AIML393",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.MEDIUM,
                message="nbviewer sharing - ensure notebooks are safe before public sharing",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="A03",
                cwe_id="CWE-79",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_binder_config_injection(self, node: ast.Call) -> None:
        """AIML394: Detect Binder configuration injection."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Binder configuration
        if any(x in line_text for x in ["binder", "repo2docker", "binderhub"]):
            if any(x in line_text for x in ["postbuild", "start", "environment.yml", "apt.txt"]):
                violation = RuleViolation(
                    rule_id="AIML394",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Binder configuration - validate repo2docker config to prevent injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="A03",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_jupyterhub_auth_bypass(self, node: ast.Call) -> None:
        """AIML395: Detect JupyterHub authentication bypass."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for JupyterHub authentication
        if "jupyterhub" in line_text and "auth" in line_text:
            if any(x in line_text for x in ["bypass", "skip", "disable", "allow_all"]):
                violation = RuleViolation(
                    rule_id="AIML395",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.CRITICAL,
                    message="JupyterHub authentication - implement proper access controls",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="A07",
                    cwe_id="CWE-287",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_notebook_credential_leakage(self, node: ast.Call) -> None:
        """AIML396: Detect notebook sharing credential leakage."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for credentials in notebooks
        if any(x in line_text for x in ["api_key", "password", "secret", "token", "credential"]):
            if not any(x in line_text for x in ["os.environ", "getenv", "config", "env."]):
                violation = RuleViolation(
                    rule_id="AIML396",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.CRITICAL,
                    message="Notebook sharing - check for hardcoded credentials before sharing",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="A07",
                    cwe_id="CWE-798",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_git_ipynb_risks(self, node: ast.Call) -> None:
        """AIML397: Detect Git integration risks with .ipynb files."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for git operations with notebooks
        if str(self.file_path).endswith(".ipynb") or ".ipynb" in line_text:
            if any(x in line_text for x in ["git.add", "git.commit", "git.push"]):
                violation = RuleViolation(
                    rule_id="AIML397",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Notebook in git repository - review outputs and metadata for sensitive data",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="A02",
                    cwe_id="CWE-312",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_colab_sharing_vulnerabilities(self, node: ast.Call) -> None:
        """AIML398: Detect Colab notebook sharing vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Colab sharing operations
        if "colab" in line_text:
            if any(x in line_text for x in ["share", "mount", "drive.mount"]):
                violation = RuleViolation(
                    rule_id="AIML398",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Google Colab sharing - validate permissions and clean sensitive data",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="A01",
                    cwe_id="CWE-200",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_kaggle_notebook_injection(self, node: ast.Call) -> None:
        """AIML399: Detect Kaggle notebook injection."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Kaggle notebook operations
        if "kaggle" in line_text:
            if any(x in line_text for x in ["download", "load", "import", "execute"]):
                violation = RuleViolation(
                    rule_id="AIML399",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Kaggle notebook - validate external code before execution",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="A03",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_paperspace_gradient_risks(self, node: ast.Call) -> None:
        """AIML400: Detect Paperspace Gradient risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Paperspace operations
        if "paperspace" in line_text or "gradient" in line_text:
            if any(x in line_text for x in ["notebook", "config", "setup"]):
                violation = RuleViolation(
                    rule_id="AIML400",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Paperspace Gradient - validate notebook environment and dependencies",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="A05",
                    cwe_id="CWE-1188",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_model_checkpoints_notebooks(self, node: ast.Call) -> None:
        """AIML401: Detect model checkpoints in notebooks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for model checkpoints being saved in notebooks
        if any(x in line_text for x in ["torch.save", "model.save", "checkpoint"]):
            if str(self.file_path).endswith(".ipynb") or "notebook" in line_text:
                violation = RuleViolation(
                    rule_id="AIML401",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Model checkpoint in notebook - extract to separate file and use secure loading",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML05",
                    cwe_id="CWE-502",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_credentials_notebook_metadata(self, node: ast.Call) -> None:
        """AIML402: Detect credentials in notebook metadata."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for metadata operations with credentials
        if "metadata" in line_text:
            if any(x in line_text for x in ["api_key", "password", "secret", "token"]):
                violation = RuleViolation(
                    rule_id="AIML402",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.CRITICAL,
                    message="Credentials in notebook metadata - remove before committing",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="A07",
                    cwe_id="CWE-798",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_large_model_loading_dos(self, node: ast.Call) -> None:
        """AIML403: Detect large model loading (DoS)."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for loading large models without memory limits
        if any(x in line_text for x in ["torch.load", "load_model", "from_pretrained"]):
            if not any(x in line_text for x in ["max_memory", "device_map", "low_cpu_mem_usage"]):
                violation = RuleViolation(
                    rule_id="AIML403",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Large model loading - implement memory limits to prevent DoS",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-400",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_gpu_memory_exhaustion_notebooks(self, node: ast.Call) -> None:
        """AIML404: Detect GPU memory exhaustion in notebooks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for GPU operations without memory management
        if any(x in line_text for x in [".cuda(", ".to('cuda')", ".to(device"]):
            if not any(x in line_text for x in ["torch.cuda.empty_cache", "max_memory", "memory_limit"]):
                violation = RuleViolation(
                    rule_id="AIML404",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="GPU operations - implement memory limits and cleanup",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-400",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_dataset_download_vulnerabilities(self, node: ast.Call) -> None:
        """AIML405: Detect dataset downloading vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for dataset downloads without validation
        if any(x in line_text for x in ["download_dataset", "load_dataset", "datasets.load"]):
            if not any(x in line_text for x in ["verify", "checksum", "hash", "signature"]):
                violation = RuleViolation(
                    rule_id="AIML405",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Dataset download - validate source and verify integrity",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-494",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    # Phase 4.2: Dataset & Data Pipeline Security Detection Methods (AIML406-430)
    
    def _check_huggingface_datasets_poisoning(self, node: ast.Call) -> None:
        """AIML406: Detect Hugging Face Datasets poisoning."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Hugging Face dataset loading without verification
        if any(x in line_text for x in ["load_dataset", "datasets.load"]):
            if not any(x in line_text for x in ["verify", "trust", "revision", "use_auth_token"]):
                violation = RuleViolation(
                    rule_id="AIML406",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Hugging Face dataset - verify source and integrity before use",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-494",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_kaggle_dataset_injection(self, node: ast.Call) -> None:
        """AIML407: Detect Kaggle dataset injection."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Kaggle dataset downloads
        if "kaggle" in line_text and any(x in line_text for x in ["download", "dataset"]):
            if not any(x in line_text for x in ["verify", "checksum", "hash"]):
                violation = RuleViolation(
                    rule_id="AIML407",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Kaggle dataset - validate data integrity and source",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-494",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_tensorflow_datasets_manipulation(self, node: ast.Call) -> None:
        """AIML408: Detect TensorFlow Datasets manipulation."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for TensorFlow dataset loading
        if "tfds.load" in line_text or "tensorflow_datasets" in line_text:
            if not any(x in line_text for x in ["download_and_prepare", "try_gcs"]):
                violation = RuleViolation(
                    rule_id="AIML408",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="TensorFlow dataset - verify checksum and source integrity",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-494",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_torchvision_datasets_tampering(self, node: ast.Call) -> None:
        """AIML409: Detect Torchvision datasets tampering."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for torchvision dataset usage
        if "torchvision.datasets" in line_text or "datasets." in line_text:
            if "download=true" in line_text and not any(x in line_text for x in ["verify", "checksum"]):
                violation = RuleViolation(
                    rule_id="AIML409",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Torchvision dataset - validate dataset source and checksums",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-494",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_paperswithcode_dataset_risks(self, node: ast.Call) -> None:
        """AIML410: Detect Papers with Code dataset risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for research dataset downloads
        if any(x in line_text for x in ["paperswithcode", "pwc", "research"]) and "dataset" in line_text:
            violation = RuleViolation(
                rule_id="AIML410",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.MEDIUM,
                message="Papers with Code dataset - verify source and data integrity",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML03",
                cwe_id="CWE-494",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_google_dataset_search_injection(self, node: ast.Call) -> None:
        """AIML411: Detect Google Dataset Search injection."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for dataset search/download operations
        if "dataset" in line_text and any(x in line_text for x in ["search", "google", "download_url"]):
            if not any(x in line_text for x in ["verify", "validate", "checksum"]):
                violation = RuleViolation(
                    rule_id="AIML411",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Google Dataset Search - validate dataset source and provenance",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-494",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_uci_ml_repository_vulnerabilities(self, node: ast.Call) -> None:
        """AIML412: Detect UCI ML Repository vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for UCI repository access
        if "uci" in line_text and "dataset" in line_text:
            violation = RuleViolation(
                rule_id="AIML412",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.LOW,
                message="UCI ML Repository dataset - validate data format and integrity",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.MANUAL,
                fix_data=None,
                owasp_id="ML03",
                cwe_id="CWE-494",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_common_crawl_poisoning(self, node: ast.Call) -> None:
        """AIML413: Detect Common Crawl poisoning."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Common Crawl data usage
        if "commoncrawl" in line_text or "common_crawl" in line_text:
            if not any(x in line_text for x in ["filter", "validate", "sanitize"]):
                violation = RuleViolation(
                    rule_id="AIML413",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Common Crawl data - implement filtering and validation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_imagenet_distribution_attacks(self, node: ast.Call) -> None:
        """AIML414: Detect ImageNet distribution attacks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for ImageNet dataset usage
        if "imagenet" in line_text:
            if not any(x in line_text for x in ["official", "verify", "checksum"]):
                violation = RuleViolation(
                    rule_id="AIML414",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="ImageNet dataset - verify official source and checksums",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-494",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_custom_dataset_loader_risks(self, node: ast.Call) -> None:
        """AIML415: Detect custom dataset loader risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for custom dataset class definitions
        if any(x in line_text for x in ["dataset(", "customdataset", "mydataset"]):
            if not any(x in line_text for x in ["validate", "verify", "check"]):
                violation = RuleViolation(
                    rule_id="AIML415",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Custom dataset loader - validate data source and implement integrity checks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_pytorch_dataloader_injection(self, node: ast.Call) -> None:
        """AIML416: Detect PyTorch DataLoader injection."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for DataLoader with untrusted data
        if "dataloader(" in line_text:
            if "num_workers" in line_text and not any(x in line_text for x in ["validate", "secure"]):
                violation = RuleViolation(
                    rule_id="AIML416",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="PyTorch DataLoader - validate data source and worker processes",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-94",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_tensorflow_tfdata_manipulation(self, node: ast.Call) -> None:
        """AIML417: Detect TensorFlow tf.data manipulation."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for tf.data operations
        if "tf.data" in line_text:
            if any(x in line_text for x in ["from_generator", "from_tensors"]):
                if not any(x in line_text for x in ["validate", "verify"]):
                    violation = RuleViolation(
                        rule_id="AIML417",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="TensorFlow tf.data - validate pipeline operations and data sources",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_pandas_data_loading_risks(self, node: ast.Call) -> None:
        """AIML418: Detect Pandas data loading risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for dangerous Pandas operations
        if any(x in line_text for x in ["pd.read_pickle", "read_pickle", "pd.read_hdf"]):
            violation = RuleViolation(
                rule_id="AIML418",
                category=RuleCategory.SECURITY,
                severity=RuleSeverity.HIGH,
                message="Pandas read operations - validate file paths and disable dangerous features",
                line_number=node.lineno,
                column=node.col_offset,
                end_line_number=getattr(node, "end_lineno", node.lineno),
                end_column=getattr(node, "end_col_offset", node.col_offset),
                file_path=str(self.file_path),
                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                fix_applicability=FixApplicability.SAFE,
                fix_data=None,
                owasp_id="A08",
                cwe_id="CWE-502",
                source_tool="pyguard",
            )
            self.violations.append(violation)
    
    def _check_numpy_data_loading_vulnerabilities(self, node: ast.Call) -> None:
        """AIML419: Detect NumPy data loading vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for np.load without allow_pickle=False
        if "np.load(" in line_text or "numpy.load(" in line_text:
            if "allow_pickle=false" not in line_text:
                violation = RuleViolation(
                    rule_id="AIML419",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="NumPy load operations - use allow_pickle=False for untrusted data",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="A08",
                    cwe_id="CWE-502",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_h5py_file_injection(self, node: ast.Call) -> None:
        """AIML420: Detect H5PY file injection."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for H5PY file operations
        if "h5py" in line_text and any(x in line_text for x in ["file(", "open("]):
            if not any(x in line_text for x in ["validate", "verify", "mode='r'"]):
                violation = RuleViolation(
                    rule_id="AIML420",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="H5PY file operations - validate file source and structure",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_zarr_array_poisoning(self, node: ast.Call) -> None:
        """AIML421: Detect Zarr array poisoning."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Zarr operations
        if "zarr" in line_text and any(x in line_text for x in ["open(", "load("]):
            if not any(x in line_text for x in ["validate", "verify"]):
                violation = RuleViolation(
                    rule_id="AIML421",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Zarr array operations - validate array metadata and sources",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_parquet_file_tampering(self, node: ast.Call) -> None:
        """AIML422: Detect Parquet file tampering."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Parquet operations
        if "parquet" in line_text and any(x in line_text for x in ["read", "load"]):
            if not any(x in line_text for x in ["validate", "verify"]):
                violation = RuleViolation(
                    rule_id="AIML422",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Parquet file operations - validate file integrity and metadata",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_arrow_data_format_risks(self, node: ast.Call) -> None:
        """AIML423: Detect Arrow data format risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Apache Arrow operations
        if "arrow" in line_text or "pyarrow" in line_text:
            if any(x in line_text for x in ["read", "table", "schema"]):
                if not any(x in line_text for x in ["validate", "verify"]):
                    violation = RuleViolation(
                        rule_id="AIML423",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Apache Arrow operations - validate data sources and schema",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_augmentation_library_vulnerabilities(self, node: ast.Call) -> None:
        """AIML424: Detect data augmentation library vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for data augmentation operations
        if any(x in line_text for x in ["augment", "transform", "augmentation"]):
            if any(x in line_text for x in ["random", "noise", "distort"]):
                violation = RuleViolation(
                    rule_id="AIML424",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="Data augmentation - validate augmentation parameters and operations",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_albumentations_injection(self, node: ast.Call) -> None:
        """AIML425: Detect Albumentations injection."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Albumentations usage
        if "albumentations" in line_text or "a." in line_text:
            if any(x in line_text for x in ["compose(", "transform"]):
                violation = RuleViolation(
                    rule_id="AIML425",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="Albumentations transforms - validate transform parameters",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_dvc_injection(self, node: ast.Call) -> None:
        """AIML426: Detect DVC injection."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for DVC operations
        if "dvc" in line_text:
            if any(x in line_text for x in ["remote", "pull", "push", "import"]):
                if not any(x in line_text for x in ["verify", "validate"]):
                    violation = RuleViolation(
                        rule_id="AIML426",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="DVC operations - validate remote storage and pipeline configurations",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_lakefs_tampering(self, node: ast.Call) -> None:
        """AIML427: Detect LakeFS tampering."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for LakeFS operations
        if "lakefs" in line_text:
            if any(x in line_text for x in ["commit", "merge", "branch"]):
                violation = RuleViolation(
                    rule_id="AIML427",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="LakeFS operations - implement access controls and audit logging",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="A01",
                    cwe_id="CWE-285",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_delta_lake_poisoning(self, node: ast.Call) -> None:
        """AIML428: Detect Delta Lake poisoning."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Delta Lake operations
        if "deltalake" in line_text or "delta.io" in line_text:
            if any(x in line_text for x in ["write", "merge", "update"]):
                if not any(x in line_text for x in ["validate", "verify"]):
                    violation = RuleViolation(
                        rule_id="AIML428",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Delta Lake operations - validate transaction logs and data integrity",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_iceberg_table_manipulation(self, node: ast.Call) -> None:
        """AIML429: Detect Iceberg table manipulation."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Iceberg operations
        if "iceberg" in line_text:
            if any(x in line_text for x in ["table", "snapshot", "metadata"]):
                if not any(x in line_text for x in ["validate", "verify"]):
                    violation = RuleViolation(
                        rule_id="AIML429",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Iceberg table operations - validate metadata and snapshots",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_hudi_versioning_risks(self, node: ast.Call) -> None:
        """AIML430: Detect Hudi data versioning risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Hudi operations
        if "hudi" in line_text:
            if any(x in line_text for x in ["write", "commit", "timeline"]):
                if not any(x in line_text for x in ["validate", "verify"]):
                    violation = RuleViolation(
                        rule_id="AIML430",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Hudi operations - validate timeline and commit integrity",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    # Phase 4.3: Model Registry & Versioning Security (20 checks - AIML431-450)
    
    def _check_mlflow_registry_injection(self, node: ast.Call) -> None:
        """AIML431: Detect MLflow Model Registry injection vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for MLflow registry operations
        if "mlflow" in line_text:
            if any(x in line_text for x in ["register_model", "create_model_version", "transition_model_version_stage"]):
                if not any(x in line_text for x in ["validate", "sanitize"]):
                    violation = RuleViolation(
                        rule_id="AIML431",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="MLflow Model Registry - validate model source and metadata to prevent injection",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_wandb_artifact_tampering(self, node: ast.Call) -> None:
        """AIML432: Detect Weights & Biases artifact tampering risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for W&B artifact operations
        if "wandb" in line_text or "weights_biases" in line_text or "weights & biases" in line_text:
            if any(x in line_text for x in ["use_artifact", "log_artifact", "download"]):
                if not any(x in line_text for x in ["verify", "checksum"]):
                    violation = RuleViolation(
                        rule_id="AIML432",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Weights & Biases artifact - verify integrity to prevent tampering",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_neptune_model_poisoning(self, node: ast.Call) -> None:
        """AIML433: Detect Neptune.ai model poisoning vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Neptune.ai operations
        if "neptune" in line_text:
            if any(x in line_text for x in ["upload", "download", "fetch_model"]):
                if not any(x in line_text for x in ["validate", "verify"]):
                    violation = RuleViolation(
                        rule_id="AIML433",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Neptune.ai model operations - validate model integrity to prevent poisoning",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_comet_registry_manipulation(self, node: ast.Call) -> None:
        """AIML434: Detect Comet.ml registry manipulation vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Comet.ml registry operations
        if "comet" in line_text:
            if any(x in line_text for x in ["log_model", "download_registry_model", "register_model"]):
                if not any(x in line_text for x in ["validate", "verify"]):
                    violation = RuleViolation(
                        rule_id="AIML434",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Comet.ml registry - validate model registration to prevent manipulation",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_sagemaker_registry_risks(self, node: ast.Call) -> None:
        """AIML435: Detect AWS SageMaker model registry security risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for SageMaker model registry operations
        if "sagemaker" in line_text:
            if any(x in line_text for x in ["create_model_package", "register_model", "describe_model_package"]):
                if not any(x in line_text for x in ["approval", "validation"]):
                    violation = RuleViolation(
                        rule_id="AIML435",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="AWS SageMaker model registry - implement approval workflows and validation",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-285",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_azure_ml_registry_gaps(self, node: ast.Call) -> None:
        """AIML436: Detect Azure ML model registry security gaps."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Azure ML registry operations
        if "azure" in line_text or "azureml" in line_text:
            if any(x in line_text for x in ["register_model", "model.register", "download"]):
                if not any(x in line_text for x in ["validate", "approved"]):
                    violation = RuleViolation(
                        rule_id="AIML436",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Azure ML model registry - implement validation and approval workflows",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-285",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_vertex_ai_registry(self, node: ast.Call) -> None:
        """AIML437: Detect Google Vertex AI model registry vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Vertex AI registry operations
        if "vertex" in line_text or "vertexai" in line_text:
            if any(x in line_text for x in ["upload_model", "deploy_model", "model.upload"]):
                if not any(x in line_text for x in ["validate", "scan"]):
                    violation = RuleViolation(
                        rule_id="AIML437",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Google Vertex AI model registry - validate models before deployment",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-285",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_custom_registry_vulnerabilities(self, node: ast.Call) -> None:
        """AIML438: Detect custom model registry vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for custom registry operations
        if any(x in line_text for x in ["model_registry", "registry.register", "registry.fetch"]):
            if not any(x in line_text for x in ["validate", "verify", "checksum"]):
                violation = RuleViolation(
                    rule_id="AIML438",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Custom model registry - implement validation and integrity checks",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-494",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_model_metadata_injection(self, node: ast.Call) -> None:
        """AIML439: Detect model metadata injection vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for metadata operations
        if any(x in line_text for x in ["metadata", "model_info", "model_card"]):
            if any(x in line_text for x in ["update", "set", "write"]):
                if not any(x in line_text for x in ["sanitize", "validate"]):
                    violation = RuleViolation(
                        rule_id="AIML439",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Model metadata operations - sanitize inputs to prevent injection",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_model_versioning_bypass(self, node: ast.Call) -> None:
        """AIML440: Detect model versioning bypass vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for version-related operations
        if any(x in line_text for x in ["version", "model_version"]):
            if "latest" in line_text or "head" in line_text:
                violation = RuleViolation(
                    rule_id="AIML440",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Model versioning - avoid 'latest' tag, use specific version for reproducibility",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-494",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_model_lineage_tampering(self, node: ast.Call) -> None:
        """AIML441: Detect model lineage tampering risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for lineage operations
        if any(x in line_text for x in ["lineage", "provenance", "parent_model"]):
            if any(x in line_text for x in ["set", "update", "modify"]):
                if not any(x in line_text for x in ["audit", "log"]):
                    violation = RuleViolation(
                        rule_id="AIML441",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Model lineage operations - implement audit logging to prevent tampering",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_model_approval_workflow_gaps(self, node: ast.Call) -> None:
        """AIML442: Detect model approval workflow security gaps."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for deployment operations without approval
        if any(x in line_text for x in ["deploy", "promote", "production"]):
            if "model" in line_text:
                if not any(x in line_text for x in ["approved", "reviewed", "validated"]):
                    violation = RuleViolation(
                        rule_id="AIML442",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Model deployment - implement approval workflow before production",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-285",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_docker_image_model_injection(self, node: ast.Call) -> None:
        """AIML443: Detect Docker image model injection vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Docker operations with models
        if "docker" in line_text:
            if any(x in line_text for x in ["build", "run", "pull"]):
                if "model" in line_text and not any(x in line_text for x in ["scan", "verify"]):
                    violation = RuleViolation(
                        rule_id="AIML443",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Docker image with model - scan for vulnerabilities before deployment",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-494",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_maas_risks(self, node: ast.Call) -> None:
        """AIML444: Detect Model as a Service (MaaS) security risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for MaaS endpoint operations
        if any(x in line_text for x in ["api", "endpoint", "serve"]):
            if "model" in line_text:
                if not any(x in line_text for x in ["auth", "rate_limit", "throttle"]):
                    violation = RuleViolation(
                        rule_id="AIML444",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Model serving endpoint - implement authentication and rate limiting",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML09",
                        cwe_id="CWE-306",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_model_marketplace_vulnerabilities(self, node: ast.Call) -> None:
        """AIML445: Detect model marketplace vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for marketplace operations
        if any(x in line_text for x in ["marketplace", "model_hub", "model_zoo"]):
            if any(x in line_text for x in ["download", "fetch", "load"]):
                if not any(x in line_text for x in ["verify", "trusted"]):
                    violation = RuleViolation(
                        rule_id="AIML445",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Model marketplace download - verify model source and integrity",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML05",
                        cwe_id="CWE-494",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_model_license_bypass(self, node: ast.Call) -> None:
        """AIML446: Detect model license bypass attempts."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for license-related operations
        if "model" in line_text:
            if any(x in line_text for x in ["license", "terms", "commercial"]):
                if "ignore" in line_text or "bypass" in line_text:
                    violation = RuleViolation(
                        rule_id="AIML446",
                        category=RuleCategory.CONVENTION,
                        severity=RuleSeverity.MEDIUM,
                        message="Model license - ensure compliance with licensing terms",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-1059",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_model_watermark_removal(self, node: ast.Call) -> None:
        """AIML447: Detect model watermark removal attempts."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for watermark removal operations
        if "watermark" in line_text or "signature" in line_text:
            if any(x in line_text for x in ["remove", "strip", "delete"]):
                violation = RuleViolation(
                    rule_id="AIML447",
                    category=RuleCategory.CONVENTION,
                    severity=RuleSeverity.MEDIUM,
                    message="Model watermark removal - respect model authorship and provenance",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_model_fingerprinting_attacks(self, node: ast.Call) -> None:
        """AIML448: Detect model fingerprinting attack risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for model extraction attempts
        if "model" in line_text:
            if any(x in line_text for x in ["extract", "steal", "copy", "clone"]):
                if "predict" in line_text or "inference" in line_text:
                    violation = RuleViolation(
                        rule_id="AIML448",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Model extraction risk - implement query limits to prevent fingerprinting",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML07",
                        cwe_id="CWE-200",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_model_compression_tampering(self, node: ast.Call) -> None:
        """AIML449: Detect model compression tampering vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for compression operations
        if any(x in line_text for x in ["compress", "quantize", "prune"]):
            if "model" in line_text:
                if not any(x in line_text for x in ["verify", "validate", "test"]):
                    violation = RuleViolation(
                        rule_id="AIML449",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Model compression - validate model integrity after compression",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_model_conversion_vulnerabilities(self, node: ast.Call) -> None:
        """AIML450: Detect model conversion vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for model conversion operations
        if any(x in line_text for x in ["convert", "export", "transform"]):
            if "model" in line_text:
                if any(x in line_text for x in ["onnx", "tflite", "coreml"]):
                    if not any(x in line_text for x in ["validate", "test"]):
                        violation = RuleViolation(
                            rule_id="AIML450",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.MEDIUM,
                            message="Model conversion - validate converted model for equivalence and security",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="ML03",
                            cwe_id="CWE-345",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)

    # Phase 4.4: Cloud & Infrastructure Security Detection Methods (AIML451-AIML460)
    
    def _check_sagemaker_notebook_injection(self, node: ast.Call) -> None:
        """AIML451: Detect AWS SageMaker notebook injection vulnerabilities."""
        # Get multiple lines for multi-line function calls
        start_line = max(0, node.lineno - 1)
        end_line = min(len(self.lines), getattr(node, "end_lineno", node.lineno))
        full_text = " ".join(self.lines[start_line:end_line]).lower()
        
        # Check for SageMaker notebook operations
        if any(x in full_text for x in ["sagemaker", "notebookinstance"]):
            if any(x in full_text for x in ["create", "start", "update"]):
                # Check if security controls are present
                has_security = any(x in full_text for x in [
                    "rootaccess='disabled'", 'rootaccess="disabled"',
                    "directinternetaccess='disabled'", 'directinternetaccess="disabled"',
                    "volumeencryption=true", "volumeencryption = true"
                ])
                if not has_security:
                    violation = RuleViolation(
                        rule_id="AIML451",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="AWS SageMaker notebook - disable root access and direct internet access",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML09",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_azure_ml_workspace_tampering(self, node: ast.Call) -> None:
        """AIML452: Detect Azure ML workspace tampering vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Azure ML workspace operations
        if any(x in line_text for x in ["azureml", "workspace"]):
            if any(x in line_text for x in ["create", "update", "from_config"]):
                if not any(x in line_text for x in ["auth", "authentication", "identity", "rbac"]):
                    violation = RuleViolation(
                        rule_id="AIML452",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Azure ML workspace - implement proper authentication and RBAC",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-287",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_vertex_ai_pipeline_manipulation(self, node: ast.Call) -> None:
        """AIML453: Detect Google Vertex AI pipeline manipulation vulnerabilities."""
        # Get multiple lines for multi-line function calls
        start_line = max(0, node.lineno - 1)
        end_line = min(len(self.lines), getattr(node, "end_lineno", node.lineno))
        full_text = " ".join(self.lines[start_line:end_line]).lower()
        
        # Check for Vertex AI pipeline operations
        if any(x in full_text for x in ["vertex", "aiplatform", "pipeline"]):
            if any(x in full_text for x in ["create", "submit", "run"]):
                # Check if security controls are present
                has_security = any(x in full_text for x in [
                    "service_account", "encryption_spec", "network"
                ])
                if not has_security:
                    violation = RuleViolation(
                        rule_id="AIML453",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Google Vertex AI pipeline - configure service account and encryption",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-311",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_databricks_ml_runtime_risks(self, node: ast.Call) -> None:
        """AIML454: Detect Databricks ML runtime risks."""
        # Get multiple lines for multi-line function calls
        start_line = max(0, node.lineno - 1)
        end_line = min(len(self.lines), getattr(node, "end_lineno", node.lineno))
        full_text = " ".join(self.lines[start_line:end_line]).lower()
        
        # Check for Databricks ML runtime operations with hardcoded tokens
        if any(x in full_text for x in ["databricks", "dbutils", "mlflow"]):
            # Look for hardcoded tokens/keys (not using secrets API)
            if ("token" in full_text or "api_key" in full_text):
                # Check if using string literals with token/key values (like 'dapi...' or "token_...")
                if ("'dapi" in full_text or '"dapi' in full_text or 
                    "'token_" in full_text or '"token_' in full_text or
                    ("=" in full_text and "token" in full_text and ("'" in full_text or '"' in full_text))):
                    # Make sure it's not using secrets API
                    if not any(x in full_text for x in ["secret", "getargument", "dbutils.secrets"]):
                        violation = RuleViolation(
                            rule_id="AIML454",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.CRITICAL,
                            message="Databricks ML runtime - use secrets API instead of hardcoded tokens",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="A07",
                            cwe_id="CWE-798",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)
    
    def _check_snowflake_ml_vulnerabilities(self, node: ast.Call) -> None:
        """AIML455: Detect Snowflake ML vulnerabilities."""
        # Only check if Snowflake is imported
        if not self.has_snowflake:
            return
            
        # Get multiple lines for multi-line function calls
        start_line = max(0, node.lineno - 1)
        end_line = min(len(self.lines), getattr(node, "end_lineno", node.lineno))
        full_text = " ".join(self.lines[start_line:end_line]).lower()
        
        # Check for model registration operations
        if "register_model" in full_text or "create_model" in full_text:
            if not any(x in full_text for x in ["validate", "verify", "secure"]):
                violation = RuleViolation(
                    rule_id="AIML455",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Snowflake ML - validate model before registration",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_bigquery_ml_injection(self, node: ast.Call) -> None:
        """AIML456: Detect BigQuery ML injection vulnerabilities."""
        # Get multiple lines for multi-line function calls
        start_line = max(0, node.lineno - 2)  # Look at more context
        end_line = min(len(self.lines), getattr(node, "end_lineno", node.lineno) + 2)
        full_text = " ".join(self.lines[start_line:end_line]).lower()
        
        # Check for BigQuery ML operations with user input (look for query variable assignments)
        if any(x in full_text for x in ["bigquery", "bqml", "create model", "ml.predict"]):
            # Check for string formatting that could indicate SQL injection
            if any(x in full_text for x in ["query = f\"", "query = f'", "query=f\"", "query=f'"]):
                violation = RuleViolation(
                    rule_id="AIML456",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="BigQuery ML - use parameterized queries to prevent SQL injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="A03",
                    cwe_id="CWE-89",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_redshift_ml_tampering(self, node: ast.Call) -> None:
        """AIML457: Detect Redshift ML tampering vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Redshift ML operations
        if any(x in line_text for x in ["redshift", "create model", "ml.predict"]):
            if any(x in line_text for x in ["format", "f\"", "f'", "%s"]):
                violation = RuleViolation(
                    rule_id="AIML457",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Redshift ML - use parameterized queries to prevent SQL injection",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="A03",
                    cwe_id="CWE-89",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_lambda_ml_inference_risks(self, node: ast.Call) -> None:
        """AIML458: Detect Lambda ML inference risks."""
        # Get multiple lines for multi-line function calls
        start_line = max(0, node.lineno - 1)
        end_line = min(len(self.lines), getattr(node, "end_lineno", node.lineno))
        full_text = " ".join(self.lines[start_line:end_line]).lower()
        
        # Check for Lambda ML inference operations
        if "lambda" in full_text and "invoke" in full_text:
            # Check if it looks like model inference based on function name
            if any(x in full_text for x in ["model", "predict", "inference", "ml"]):
                # Check for resource limits
                has_limits = any(x in full_text for x in ["timeout", "memory", "vpc"])
                if not has_limits:
                    violation = RuleViolation(
                        rule_id="AIML458",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Lambda ML inference - configure timeout, memory limits, and VPC",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML09",
                        cwe_id="CWE-400",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_cloud_functions_ml_serving_gaps(self, node: ast.Call) -> None:
        """AIML459: Detect Cloud Functions ML serving security gaps."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Cloud Functions ML serving
        if any(x in line_text for x in ["cloud_function", "cloudfunctions", "gcf"]):
            if any(x in line_text for x in ["model", "predict", "inference"]):
                if not any(x in line_text for x in ["auth", "authentication", "iam"]):
                    violation = RuleViolation(
                        rule_id="AIML459",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Cloud Functions ML serving - implement authentication and IAM",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="A07",
                        cwe_id="CWE-306",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_serverless_ml_vulnerabilities(self, node: ast.Call) -> None:
        """AIML460: Detect serverless ML vulnerabilities."""
        # Get multiple lines for multi-line function calls
        start_line = max(0, node.lineno - 1)
        end_line = min(len(self.lines), getattr(node, "end_lineno", node.lineno))
        full_text = " ".join(self.lines[start_line:end_line]).lower()
        
        # Check for serverless ML deployments with function deploy calls
        if any(x in full_text for x in ["serverless", "faas", "function"]):
            # Look for deploy/create operations with ML-related names
            if ("deploy" in full_text or "create" in full_text):
                if any(x in full_text for x in ["model", "ml", "predict", "inference"]):
                    # Check for rate limiting
                    has_rate_limit = any(x in full_text for x in ["rate_limit", "throttle", "quota"])
                    if not has_rate_limit:
                        violation = RuleViolation(
                            rule_id="AIML460",
                            category=RuleCategory.SECURITY,
                            severity=RuleSeverity.MEDIUM,
                            message="Serverless ML - implement rate limiting and resource quotas",
                            line_number=node.lineno,
                            column=node.col_offset,
                            end_line_number=getattr(node, "end_lineno", node.lineno),
                            end_column=getattr(node, "end_col_offset", node.col_offset),
                            file_path=str(self.file_path),
                            code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                            fix_applicability=FixApplicability.SAFE,
                            fix_data=None,
                            owasp_id="ML09",
                            cwe_id="CWE-770",
                            source_tool="pyguard",
                        )
                        self.violations.append(violation)

    # Phase 5.1: Generative AI Security Detection Methods (AIML461-AIML480)
    # Phase 5.1.1: Text Generation Security (AIML461-AIML470)
    
    def _check_prompt_leaking_attacks(self, node: ast.Call) -> None:
        """AIML461: Detect prompt leaking attacks in LLM applications."""
        # Get multiple lines for multi-line function calls
        start_line = max(0, node.lineno - 1)
        end_line = min(len(self.lines), getattr(node, "end_lineno", node.lineno))
        full_text = " ".join(self.lines[start_line:end_line]).lower()
        
        # Check for LLM calls that might be vulnerable to prompt leaking
        if any(x in full_text for x in ["chat", "completion", "generate", "predict"]):
            # Check if there's system prompt handling
            has_system_prompt = any(x in full_text for x in ["system", "instruction"])
            if has_system_prompt:
                # Check for prompt protection measures
                has_protection = any(x in full_text for x in ["protect", "guard", "validate", "filter"])
                if not has_protection:
                    violation = RuleViolation(
                        rule_id="AIML461",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="LLM prompt leaking - implement prompt protection to prevent system prompt extraction",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM01",
                        cwe_id="CWE-200",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_training_data_extraction(self, node: ast.Call) -> None:
        """AIML462: Detect training data extraction vulnerabilities."""
        # Check if this is a model.generate() or similar call
        func_name = ""
        if isinstance(node.func, ast.Attribute):
            func_name = node.func.attr.lower()
        elif isinstance(node.func, ast.Name):
            func_name = node.func.id.lower()
        
        # Check for LLM generation calls
        if func_name in ["generate", "completion", "create", "chat"]:
            # Check for high temperature in arguments
            has_high_temp = False
            has_filtering = False
            
            for keyword in node.keywords:
                key = keyword.arg if keyword.arg else ""
                # Check for temperature parameter
                if key == "temperature":
                    if isinstance(keyword.value, ast.Constant):
                        # High temperature (> 0.7) increases risk
                        if isinstance(keyword.value.value, (int, float)) and keyword.value.value > 0.7:
                            has_high_temp = True
                # Check for output filtering indicators
                if key in ["filter", "sanitize", "validate"]:
                    has_filtering = True
            
            # Trigger if high temperature without filtering
            if has_high_temp and not has_filtering:
                violation = RuleViolation(
                    rule_id="AIML462",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.CRITICAL,
                    message="LLM training data extraction - implement output filtering to prevent training data leakage",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM06",
                    cwe_id="CWE-200",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_memorization_exploitation(self, node: ast.Call) -> None:
        """AIML463: Detect memorization exploitation vulnerabilities."""
        # Check for generate/completion calls
        func_name = ""
        if isinstance(node.func, ast.Attribute):
            func_name = node.func.attr.lower()
        elif isinstance(node.func, ast.Name):
            func_name = node.func.id.lower()
        
        if func_name in ["generate", "completion", "create", "chat"]:
            # Check arguments for prompts with memorization exploitation keywords
            # Check positional arguments
            for arg in node.args:
                if isinstance(arg, ast.Name):
                    # Variable name might indicate prompt with memorization keywords
                    # Check assignment statements in the file for this variable
                    var_name = arg.id
                    for line in self.lines:
                        line_lower = line.lower()
                        if var_name.lower() in line_lower and any(x in line_lower for x in ["repeat", "recite", "verbatim", "exact"]):
                            violation = RuleViolation(
                                rule_id="AIML463",
                                category=RuleCategory.SECURITY,
                                severity=RuleSeverity.HIGH,
                                message="LLM memorization exploitation - implement output validation to prevent memorized content extraction",
                                line_number=node.lineno,
                                column=node.col_offset,
                                end_line_number=getattr(node, "end_lineno", node.lineno),
                                end_column=getattr(node, "end_col_offset", node.col_offset),
                                file_path=str(self.file_path),
                                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                                fix_applicability=FixApplicability.SAFE,
                                fix_data=None,
                                owasp_id="LLM06",
                                cwe_id="CWE-200",
                                source_tool="pyguard",
                            )
                            self.violations.append(violation)
                            break
            
            # Check keyword arguments
            for keyword in node.keywords:
                key = keyword.arg if keyword.arg else ""
                if key in ["prompt", "text", "input"]:
                    # Get the prompt value or variable
                    if isinstance(keyword.value, ast.BinOp):
                        # Check for string concatenation with "repeat", "verbatim", etc.
                        line_text = ast.get_source_segment(self.source_code, keyword.value) if hasattr(self, 'source_code') else ""
                        if not line_text:
                            # Fallback to line-based check
                            start_line = keyword.value.lineno - 1
                            end_line = getattr(keyword.value, 'end_lineno', keyword.value.lineno)
                            line_text = '\n'.join(self.lines[start_line:end_line]).lower()
                        else:
                            line_text = line_text.lower()
                        
                        if any(x in line_text for x in ["repeat", "recite", "verbatim", "exact"]):
                            violation = RuleViolation(
                                rule_id="AIML463",
                                category=RuleCategory.SECURITY,
                                severity=RuleSeverity.HIGH,
                                message="LLM memorization exploitation - implement output validation to prevent memorized content extraction",
                                line_number=node.lineno,
                                column=node.col_offset,
                                end_line_number=getattr(node, "end_lineno", node.lineno),
                                end_column=getattr(node, "end_col_offset", node.col_offset),
                                file_path=str(self.file_path),
                                code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                                fix_applicability=FixApplicability.SAFE,
                                fix_data=None,
                                owasp_id="LLM06",
                                cwe_id="CWE-200",
                                source_tool="pyguard",
                            )
                            self.violations.append(violation)
    
    def _check_copyright_infringement(self, node: ast.Call) -> None:
        """AIML464: Detect copyright infringement risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for content generation without copyright filtering
        if any(x in line_text for x in ["generate", "create", "write"]):
            if any(x in line_text for x in ["story", "article", "book", "code", "song"]):
                # Check for copyright filtering
                has_copyright_check = any(x in line_text for x in ["copyright", "license", "attribution"])
                if not has_copyright_check:
                    violation = RuleViolation(
                        rule_id="AIML464",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="LLM copyright infringement - implement content filtering to prevent copyrighted content generation",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="LLM09",
                        cwe_id="CWE-200",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_generated_code_security(self, node: ast.Call) -> None:
        """AIML465: Detect generated code security risks (Copilot-style)."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for code generation followed by execution
        if any(x in line_text for x in ["generate", "complete"]):
            if any(x in line_text for x in ["code", "function", "script"]):
                # Check for security validation
                has_validation = any(x in line_text for x in ["validate", "scan", "check", "lint"])
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML465",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="LLM code generation - validate and sanitize generated code before execution",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM08",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_jailbreak_detection(self, node: ast.Call) -> None:
        """AIML466: Detect jailbreak attempts in LLM applications."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for user input being passed to LLM without jailbreak detection
        if any(x in line_text for x in ["user", "input", "prompt"]):
            if any(x in line_text for x in ["generate", "completion", "chat"]):
                # Check for jailbreak detection
                has_jailbreak_detection = any(x in line_text for x in ["jailbreak", "safety", "guardrail", "moderate"])
                if not has_jailbreak_detection:
                    violation = RuleViolation(
                        rule_id="AIML466",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.CRITICAL,
                        message="LLM jailbreak - implement input filtering to prevent jailbreak attempts",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM01",
                        cwe_id="CWE-863",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_toxicity_generation(self, node: ast.Call) -> None:
        """AIML467: Detect toxicity generation risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for content generation without moderation
        if any(x in line_text for x in ["generate", "create", "complete"]):
            # Check for content moderation
            has_moderation = any(x in line_text for x in ["moderate", "filter", "toxicity", "safety"])
            if not has_moderation:
                violation = RuleViolation(
                    rule_id="AIML467",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="LLM toxicity generation - implement content moderation to prevent toxic output",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM09",
                    cwe_id="CWE-1284",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_bias_amplification(self, node: ast.Call) -> None:
        """AIML468: Detect bias amplification risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for content generation without bias detection
        if any(x in line_text for x in ["generate", "predict", "classify"]):
            # Check for bias detection
            has_bias_detection = any(x in line_text for x in ["bias", "fairness", "equity"])
            if not has_bias_detection:
                violation = RuleViolation(
                    rule_id="AIML468",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="LLM bias amplification - implement bias detection and mitigation strategies",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="LLM09",
                    cwe_id="CWE-1229",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_hallucination_exploitation(self, node: ast.Call) -> None:
        """AIML469: Detect hallucination exploitation risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for factual content generation without verification
        if any(x in line_text for x in ["generate", "answer", "explain"]):
            if any(x in line_text for x in ["fact", "data", "statistic", "research"]):
                # Check for fact-checking
                has_verification = any(x in line_text for x in ["verify", "source", "cite", "reference"])
                if not has_verification:
                    violation = RuleViolation(
                        rule_id="AIML469",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="LLM hallucination exploitation - implement fact-checking and source verification",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="LLM09",
                        cwe_id="CWE-754",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_output_filtering_bypass(self, node: ast.Call) -> None:
        """AIML470: Detect output filtering bypass attempts."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for output handling without robust filtering
        if any(x in line_text for x in ["response", "output", "result"]):
            if any(x in line_text for x in ["generate", "complete", "chat"]):
                # Check for defense-in-depth filtering
                has_robust_filtering = any(x in line_text for x in ["multi", "layer", "defense", "robust"])
                if not has_robust_filtering:
                    violation = RuleViolation(
                        rule_id="AIML470",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="LLM output filtering - implement robust content filtering that cannot be bypassed",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM01",
                        cwe_id="CWE-116",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    # Phase 5.1.2: Image/Video Generation (AIML471-AIML480)
    
    def _check_stable_diffusion_prompt_injection(self, node: ast.Call) -> None:
        """AIML471: Detect Stable Diffusion prompt injection vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Stable Diffusion image generation without input sanitization
        if any(x in line_text for x in ["stablediffusion", "stable_diffusion", "diffusion"]):
            if any(x in line_text for x in ["generate", "create", "prompt"]):
                # Check for input sanitization
                has_sanitization = any(x in line_text for x in ["sanitize", "validate", "filter"])
                if not has_sanitization:
                    violation = RuleViolation(
                        rule_id="AIML471",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Stable Diffusion prompt injection - sanitize user inputs in image generation prompts",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM01",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_dalle_manipulation(self, node: ast.Call) -> None:
        """AIML472: Detect DALL-E manipulation vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for DALL-E usage without content moderation
        if any(x in line_text for x in ["dalle", "dall-e", "dall_e"]):
            if any(x in line_text for x in ["generate", "create", "image"]):
                # Check for content moderation
                has_moderation = any(x in line_text for x in ["moderate", "filter", "safety"])
                if not has_moderation:
                    violation = RuleViolation(
                        rule_id="AIML472",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="DALL-E manipulation - implement prompt filtering and content moderation",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM09",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_midjourney_prompt_engineering(self, node: ast.Call) -> None:
        """AIML473: Detect Midjourney prompt engineering vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Midjourney usage without prompt validation
        if any(x in line_text for x in ["midjourney", "mid_journey"]):
            # Check for prompt validation
            has_validation = any(x in line_text for x in ["validate", "check", "verify"])
            if not has_validation:
                violation = RuleViolation(
                    rule_id="AIML473",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Midjourney prompt engineering - validate prompts to prevent manipulation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="LLM01",
                    cwe_id="CWE-20",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_gan_mode_collapse(self, node: ast.Call) -> None:
        """AIML474: Detect GAN mode collapse exploitation."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for GAN training without mode collapse monitoring
        if "gan" in line_text and any(x in line_text for x in ["train", "fit"]):
            # Check for mode collapse monitoring
            has_monitoring = any(x in line_text for x in ["monitor", "diversity", "metric"])
            if not has_monitoring:
                violation = RuleViolation(
                    rule_id="AIML474",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.LOW,
                    message="GAN mode collapse - monitor training for mode collapse and implement diversity metrics",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-754",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_vae_latent_manipulation(self, node: ast.Call) -> None:
        """AIML475: Detect VAE latent space manipulation vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for VAE latent vector operations without validation
        if "vae" in line_text or "variational" in line_text:
            if any(x in line_text for x in ["latent", "encode", "decode"]):
                # Check for input validation
                has_validation = any(x in line_text for x in ["validate", "check", "clamp", "clip"])
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML475",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="VAE latent manipulation - implement input validation on latent vectors",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML01",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_diffusion_model_backdoors(self, node: ast.Call) -> None:
        """AIML476: Detect diffusion model backdoor vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for diffusion model loading without integrity verification
        if "diffusion" in line_text and any(x in line_text for x in ["load", "from_pretrained"]):
            # Check for integrity verification
            has_verification = any(x in line_text for x in ["verify", "checksum", "hash", "signature"])
            if not has_verification:
                violation = RuleViolation(
                    rule_id="AIML476",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Diffusion model backdoors - validate model integrity and scan for backdoors",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML03",
                    cwe_id="CWE-506",
                    source_tool="pyguard",
                )
                self.violations.append(violation)
    
    def _check_video_generation_injection(self, node: ast.Call) -> None:
        """AIML477: Detect video generation injection vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for video generation without prompt sanitization
        if any(x in line_text for x in ["runway", "gen-2", "gen2", "video"]):
            if any(x in line_text for x in ["generate", "create", "prompt"]):
                # Check for sanitization
                has_sanitization = any(x in line_text for x in ["sanitize", "validate", "filter"])
                if not has_sanitization:
                    violation = RuleViolation(
                        rule_id="AIML477",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Video generation injection - sanitize prompts and validate generated content",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM01",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_3d_generation_vulnerabilities(self, node: ast.Call) -> None:
        """AIML478: Detect 3D generation vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for 3D model generation without validation
        if any(x in line_text for x in ["point-e", "point_e", "shap-e", "shap_e", "3d"]):
            if any(x in line_text for x in ["generate", "create", "model"]):
                # Check for output validation
                has_validation = any(x in line_text for x in ["validate", "check", "verify", "scan"])
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML478",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="3D generation - validate generated 3D models for malicious content",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML09",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_music_generation_risks(self, node: ast.Call) -> None:
        """AIML479: Detect music generation copyright risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for music generation without copyright protection
        if any(x in line_text for x in ["jukebox", "musiclm", "music_lm", "music"]):
            if any(x in line_text for x in ["generate", "create", "compose"]):
                # Check for copyright protection
                has_copyright_check = any(x in line_text for x in ["copyright", "watermark", "license"])
                if not has_copyright_check:
                    violation = RuleViolation(
                        rule_id="AIML479",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.LOW,
                        message="Music generation - implement copyright protection and content filtering",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="LLM09",
                        cwe_id="CWE-200",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)
    
    def _check_audio_generation_injection(self, node: ast.Call) -> None:
        """AIML480: Detect audio generation injection vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for audio generation without input validation
        if any(x in line_text for x in ["audiolm", "audio_lm", "whisper", "audio"]):
            if any(x in line_text for x in ["generate", "synthesize", "create"]):
                # Check for input validation and sanitization
                has_sanitization = any(x in line_text for x in ["sanitize", "validate", "filter"])
                if not has_sanitization:
                    violation = RuleViolation(
                        rule_id="AIML480",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Audio generation injection - validate inputs and sanitize generated audio",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM01",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    # Phase 5.2: Multimodal & Fusion Models Detection Methods (AIML481-AIML495)
    
    def _check_clip_contrastive_poisoning(self, node: ast.Call) -> None:
        """AIML481: Detect CLIP contrastive learning poisoning vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for CLIP usage without integrity verification
        if any(x in line_text for x in ["clip", "contrastive"]):
            if any(x in line_text for x in ["model", "encode", "embed"]):
                # Check for integrity verification
                has_verification = any(x in line_text for x in ["verify", "validate", "check_integrity"])
                if not has_verification:
                    violation = RuleViolation(
                        rule_id="AIML481",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="CLIP contrastive learning poisoning - validate model integrity and embedding consistency",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_align_multimodal_injection(self, node: ast.Call) -> None:
        """AIML482: Detect ALIGN multimodal injection vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for ALIGN usage without input validation
        if any(x in line_text for x in ["align", "multimodal"]):
            if any(x in line_text for x in ["encode", "process", "embed"]):
                # Check for input validation
                has_validation = any(x in line_text for x in ["validate", "sanitize", "filter"])
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML482",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="ALIGN multimodal injection - validate and sanitize image-text pairs before processing",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_flamingo_few_shot_manipulation(self, node: ast.Call) -> None:
        """AIML483: Detect Flamingo few-shot manipulation vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for Flamingo usage without few-shot validation
        if any(x in line_text for x in ["flamingo", "few_shot", "few-shot"]):
            if any(x in line_text for x in ["prompt", "example", "demo"]):
                # Check for example validation
                has_validation = any(x in line_text for x in ["validate", "verify", "check"])
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML483",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Flamingo few-shot manipulation - validate few-shot examples to prevent context poisoning",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM01",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_blip2_query_injection(self, node: ast.Call) -> None:
        """AIML484: Detect BLIP-2 query injection vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for BLIP-2 usage without query validation
        if any(x in line_text for x in ["blip", "blip2", "blip-2"]):
            if any(x in line_text for x in ["query", "question", "generate"]):
                # Check for query validation
                has_validation = any(x in line_text for x in ["validate", "sanitize", "filter"])
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML484",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="BLIP-2 query injection - sanitize queries and validate Q-Former inputs",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM01",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_gpt4_vision_prompt_attacks(self, node: ast.Call) -> None:
        """AIML485: Detect GPT-4 Vision prompt attack vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for GPT-4 Vision usage without image validation
        if any(x in line_text for x in ["gpt-4-vision", "gpt4v", "vision"]):
            if any(x in line_text for x in ["image", "visual", "picture"]):
                # Check for image validation
                has_validation = any(x in line_text for x in ["validate", "verify", "scan"])
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML485",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.CRITICAL,
                        message="GPT-4 Vision prompt attack - validate images and text prompts to prevent visual jailbreaks",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM01",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_llava_instruction_tuning_risks(self, node: ast.Call) -> None:
        """AIML486: Detect LLaVA instruction tuning risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for LLaVA usage without instruction validation
        if any(x in line_text for x in ["llava", "llava-"]):
            if any(x in line_text for x in ["instruct", "train", "finetune"]):
                # Check for instruction validation
                has_validation = any(x in line_text for x in ["validate", "filter", "sanitize"])
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML486",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="LLaVA instruction tuning risks - validate visual instructions to prevent poisoning",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_minigpt4_alignment_bypass(self, node: ast.Call) -> None:
        """AIML487: Detect MiniGPT-4 alignment bypass vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for MiniGPT-4 usage without alignment checks
        if any(x in line_text for x in ["minigpt", "mini-gpt"]):
            if any(x in line_text for x in ["generate", "response", "answer"]):
                # Check for alignment verification
                has_alignment_check = any(x in line_text for x in ["align", "safety", "guard"])
                if not has_alignment_check:
                    violation = RuleViolation(
                        rule_id="AIML487",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="MiniGPT-4 alignment bypass - implement safety guardrails and alignment verification",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="LLM01",
                        cwe_id="CWE-863",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_coca_caption_poisoning(self, node: ast.Call) -> None:
        """AIML488: Detect CoCa caption poisoning vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for CoCa usage without caption validation
        if any(x in line_text for x in ["coca", "caption"]):
            if any(x in line_text for x in ["generate", "create", "produce"]):
                # Check for caption validation
                has_validation = any(x in line_text for x in ["validate", "filter", "sanitize"])
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML488",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="CoCa caption poisoning - validate generated captions to prevent content injection",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML09",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_audio_text_alignment_poisoning(self, node: ast.Call) -> None:
        """AIML489: Detect audio-text alignment poisoning vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for audio-text alignment without validation
        if any(x in line_text for x in ["audio", "speech"]):
            if any(x in line_text for x in ["text", "transcript", "align"]):
                # Check for alignment validation
                has_validation = any(x in line_text for x in ["validate", "verify", "check"])
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML489",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Audio-text alignment poisoning - validate audio-text correspondence to prevent manipulation",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML09",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_video_text_retrieval_manipulation(self, node: ast.Call) -> None:
        """AIML490: Detect video-text retrieval manipulation vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for video-text retrieval without validation
        if any(x in line_text for x in ["video", "clip"]):
            if any(x in line_text for x in ["retrieval", "search", "query"]):
                # Check for retrieval validation
                has_validation = any(x in line_text for x in ["validate", "verify", "sanitize"])
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML490",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Video-text retrieval manipulation - validate video frames and temporal alignment",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML09",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_speech_to_text_injection(self, node: ast.Call) -> None:
        """AIML491: Detect speech-to-text injection vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for speech-to-text without audio validation
        if any(x in line_text for x in ["speech_to_text", "transcribe", "asr"]):
            if any(x in line_text for x in ["audio", "wav", "mp3"]):
                # Check for audio validation
                has_validation = any(x in line_text for x in ["validate", "sanitize", "filter"])
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML491",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Speech-to-text injection - validate audio inputs to prevent adversarial audio attacks",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML01",
                        cwe_id="CWE-94",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_text_to_speech_vulnerabilities(self, node: ast.Call) -> None:
        """AIML492: Detect text-to-speech vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for text-to-speech without input validation
        if any(x in line_text for x in ["text_to_speech", "tts", "synthesize"]):
            if any(x in line_text for x in ["voice", "audio", "speak"]):
                # Check for text validation
                has_validation = any(x in line_text for x in ["validate", "sanitize", "filter"])
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML492",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Text-to-speech vulnerabilities - validate text inputs and prevent voice cloning attacks",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML09",
                        cwe_id="CWE-20",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_visual_grounding_attacks(self, node: ast.Call) -> None:
        """AIML493: Detect visual grounding attack vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for visual grounding without validation
        if any(x in line_text for x in ["grounding", "localize", "refer"]):
            if any(x in line_text for x in ["visual", "image", "object"]):
                # Check for grounding validation
                has_validation = any(x in line_text for x in ["validate", "verify", "check"])
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML493",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.MEDIUM,
                        message="Visual grounding attacks - validate object localization and referring expressions",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML09",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_embodied_ai_risks(self, node: ast.Call) -> None:
        """AIML494: Detect embodied AI (robotics) security risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for robotics/embodied AI without safety checks
        if any(x in line_text for x in ["robot", "embodied", "control"]):
            if any(x in line_text for x in ["action", "command", "execute"]):
                # Check for safety validation
                has_safety = any(x in line_text for x in ["safety", "validate", "verify", "check"])
                if not has_safety:
                    violation = RuleViolation(
                        rule_id="AIML494",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.CRITICAL,
                        message="Embodied AI risks - implement safety checks and validate robot control commands",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML09",
                        cwe_id="CWE-754",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_sensor_fusion_manipulation(self, node: ast.Call) -> None:
        """AIML495: Detect sensor fusion manipulation vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for sensor fusion without integrity checks
        if any(x in line_text for x in ["sensor", "fusion", "fuse"]):
            if any(x in line_text for x in ["data", "input", "merge"]):
                # Check for sensor validation
                has_validation = any(x in line_text for x in ["validate", "verify", "consistency"])
                if not has_validation:
                    violation = RuleViolation(
                        rule_id="AIML495",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Sensor fusion manipulation - validate sensor data consistency and implement integrity checks",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML09",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    # Phase 5.3: Federated & Privacy-Preserving ML (AIML496-510)
    
    def _check_federated_averaging_poisoning(self, node: ast.Call) -> None:
        """AIML496: Detect federated averaging poisoning vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for federated learning with averaging
        if any(x in line_text for x in ["fedavg", "federated_averaging", "aggregate", "average"]):
            if any(x in line_text for x in ["gradient", "model", "update", "weights"]):
                # Check for Byzantine-robust aggregation
                has_robust = any(x in line_text for x in ["krum", "median", "trimmed", "byzantine", "robust"])
                if not has_robust:
                    violation = RuleViolation(
                        rule_id="AIML496",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Federated averaging poisoning - implement Byzantine-robust aggregation and validate client gradients",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_client_selection_manipulation(self, node: ast.Call) -> None:
        """AIML497: Detect client selection manipulation vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for client selection in federated learning
        # Match patterns like: random.sample(all_clients, ...), selected_clients = ...
        if any(x in line_text for x in ["select_clients", "client_selection", "sample_clients", "all_clients", "_clients"]):
            # Check for secure/reputation-based sampling
            has_reputation = any(x in line_text for x in ["reputation", "verified", "validated", "reputation_based"])
            # random.sample without reputation is vulnerable
            is_random_sample = "random.sample" in line_text and not has_reputation
            if is_random_sample or (not has_reputation and "selected_clients" in line_text):
                violation = RuleViolation(
                    rule_id="AIML497",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Client selection manipulation - implement secure random client sampling and reputation systems",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-330",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_model_aggregation_attacks(self, node: ast.Call) -> None:
        """AIML498: Detect model aggregation attack vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for model aggregation
        if any(x in line_text for x in ["aggregate_models", "model_aggregation", "combine_updates"]):
            # Check for secure aggregation
            has_secure_agg = any(x in line_text for x in ["secure_aggregation", "encrypted", "secret_sharing"])
            if not has_secure_agg:
                violation = RuleViolation(
                    rule_id="AIML498",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Model aggregation attacks - implement secure aggregation protocols and verify model updates",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_byzantine_client_detection_bypass(self, node: ast.Call) -> None:
        """AIML499: Detect Byzantine client detection bypass vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for federated learning without Byzantine detection
        if any(x in line_text for x in ["federated", "fl_train", "fed_learning"]):
            if any(x in line_text for x in ["client", "participant", "node"]):
                # Check for Byzantine detection
                has_detection = any(x in line_text for x in ["byzantine", "outlier", "detect", "filter", "krum"])
                if not has_detection:
                    violation = RuleViolation(
                        rule_id="AIML499",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="Byzantine client detection bypass - implement outlier detection and Byzantine-robust algorithms (Krum, Median)",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.SAFE,
                        fix_data=None,
                        owasp_id="ML03",
                        cwe_id="CWE-345",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_privacy_budget_exploitation(self, node: ast.Call) -> None:
        """AIML500: Detect privacy budget exploitation vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for differential privacy without budget tracking
        if any(x in line_text for x in ["differential_privacy", "dp", "add_noise", "laplace", "gaussian_noise"]):
            # Check for privacy budget tracking
            has_tracking = any(x in line_text for x in ["epsilon", "delta", "budget", "accountant", "privacy_loss"])
            if not has_tracking:
                violation = RuleViolation(
                    rule_id="AIML500",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Privacy budget exploitation - implement proper epsilon/delta tracking and privacy amplification",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-200",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_differential_privacy_bypass(self, node: ast.Call) -> None:
        """AIML501: Detect differential privacy bypass vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for DP with inadequate noise
        if any(x in line_text for x in ["add_noise", "laplace", "gaussian", "dp_noise", "random.normal", "noisy_gradient"]):
            # Check for proper noise calibration (very small values like 0.01 are too small)
            has_calibration = any(x in line_text for x in ["sensitivity", "scale", "sigma", "calibrate", "composition", "epsilon", "privacy_budget"])
            # Detect obviously insufficient noise (0.01 or smaller)
            has_insufficient_noise = any(x in line_text for x in ["0.01", "0.001", "0.0001"])
            if not has_calibration or has_insufficient_noise:
                violation = RuleViolation(
                    rule_id="AIML501",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Differential privacy bypass - implement adequate noise calibration and composition bounds",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-200",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_secure_aggregation_vulnerabilities(self, node: ast.Call) -> None:
        """AIML502: Detect secure aggregation vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for aggregation without encryption
        if any(x in line_text for x in ["aggregate", "sum", "average", "combine"]) and any(x in line_text for x in ["gradient", "model", "update"]):
            # Check for secure aggregation
            has_secure = any(x in line_text for x in ["secure", "encrypted", "homomorphic", "secret_sharing"])
            if not has_secure:
                violation = RuleViolation(
                    rule_id="AIML502",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="Secure aggregation vulnerabilities - use homomorphic encryption or secret sharing for gradient aggregation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-311",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_homomorphic_encryption_weaknesses(self, node: ast.Call) -> None:
        """AIML503: Detect homomorphic encryption weaknesses."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for homomorphic encryption usage
        if any(x in line_text for x in ["homomorphic", "fhe", "ckks", "bfv", "tfhe"]):
            # Check for key management
            has_key_mgmt = any(x in line_text for x in ["key", "context", "params", "setup"])
            if not has_key_mgmt:
                violation = RuleViolation(
                    rule_id="AIML503",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Homomorphic encryption weaknesses - implement proper key management and validate encryption schemes (CKKS, BFV)",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-320",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_trusted_execution_environment_gaps(self, node: ast.Call) -> None:
        """AIML504: Detect TEE gaps in federated learning."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for federated learning without TEE
        if any(x in line_text for x in ["federated", "fl_", "distributed_training"]):
            if any(x in line_text for x in ["train", "compute", "process"]):
                # Check for TEE usage
                has_tee = any(x in line_text for x in ["sgx", "trustzone", "tee", "enclave", "attestation"])
                if not has_tee:
                    violation = RuleViolation(
                        rule_id="AIML504",
                        category=RuleCategory.SECURITY,
                        severity=RuleSeverity.HIGH,
                        message="TEE gaps - implement SGX/TrustZone integration and remote attestation for federated learning",
                        line_number=node.lineno,
                        column=node.col_offset,
                        end_line_number=getattr(node, "end_lineno", node.lineno),
                        end_column=getattr(node, "end_col_offset", node.col_offset),
                        file_path=str(self.file_path),
                        code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                        fix_applicability=FixApplicability.MANUAL,
                        fix_data=None,
                        owasp_id="ML09",
                        cwe_id="CWE-494",
                        source_tool="pyguard",
                    )
                    self.violations.append(violation)

    def _check_split_learning_injection(self, node: ast.Call) -> None:
        """AIML505: Detect split learning injection vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for split learning
        if any(x in line_text for x in ["split_learning", "splitnn", "cut_layer"]):
            # Check for activation protection
            has_protection = any(x in line_text for x in ["encrypt", "protect", "secure", "validate"])
            if not has_protection:
                violation = RuleViolation(
                    rule_id="AIML505",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Split learning injection - validate cut layer and protect activation sharing between client and server",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-200",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_differential_privacy_parameter_manipulation(self, node: ast.Call) -> None:
        """AIML506: Detect DP parameter manipulation vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for hardcoded DP parameters
        if any(x in line_text for x in ["epsilon", "delta"]) and "=" in line_text:
            # Check for hardcoded values
            if any(x in line_text for x in ["0.1", "0.5", "1.0", "1.5"]):
                violation = RuleViolation(
                    rule_id="AIML506",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="DP parameter manipulation - avoid hardcoded epsilon/delta and implement adaptive privacy budget allocation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.SAFE,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-798",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_smpc_risks(self, node: ast.Call) -> None:
        """AIML507: Detect SMPC risks."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for SMPC usage
        if any(x in line_text for x in ["mpc", "smpc", "multi_party", "secret_shar", "mpc_compute", "secure_computation"]):
            # Check for malicious security
            has_malicious_security = any(x in line_text for x in ["malicious", "actively_secure", "verifiable", "malicious_secure"])
            if not has_malicious_security:
                violation = RuleViolation(
                    rule_id="AIML507",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="SMPC risks - implement malicious security and validate secret sharing reconstruction",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_trusted_execution_environment_bypass(self, node: ast.Call) -> None:
        """AIML508: Detect TEE bypass vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for TEE usage
        if any(x in line_text for x in ["sgx", "enclave", "tee", "trustzone"]):
            # Check for side-channel mitigation
            has_mitigation = any(x in line_text for x in ["mitigation", "constant_time", "oblivious", "side_channel"])
            if not has_mitigation:
                violation = RuleViolation(
                    rule_id="AIML508",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.HIGH,
                    message="TEE bypass - implement side-channel mitigations and validate enclave measurement during attestation",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-203",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_encrypted_inference_vulnerabilities(self, node: ast.Call) -> None:
        """AIML509: Detect encrypted inference vulnerabilities."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for encrypted inference
        if any(x in line_text for x in ["encrypted_inference", "fhe_inference", "homomorphic_inference"]):
            # Check for key management
            has_key_mgmt = any(x in line_text for x in ["key_manager", "context", "params", "integrity"])
            if not has_key_mgmt:
                violation = RuleViolation(
                    rule_id="AIML509",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="Encrypted inference vulnerabilities - implement proper key management and validate ciphertext integrity",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-311",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    def _check_zero_knowledge_proof_gaps(self, node: ast.Call) -> None:
        """AIML510: Detect zero-knowledge proof gaps."""
        line_text = self.lines[node.lineno - 1].lower() if node.lineno <= len(self.lines) else ""
        
        # Check for ZKP usage
        if any(x in line_text for x in ["zkp", "zero_knowledge", "proof", "zk_snark", "zk_stark"]):
            # Check for soundness verification
            has_verification = any(x in line_text for x in ["verify", "soundness", "challenge", "commitment"])
            if not has_verification:
                violation = RuleViolation(
                    rule_id="AIML510",
                    category=RuleCategory.SECURITY,
                    severity=RuleSeverity.MEDIUM,
                    message="ZKP gaps - implement soundness guarantees and validate proof generation/verification",
                    line_number=node.lineno,
                    column=node.col_offset,
                    end_line_number=getattr(node, "end_lineno", node.lineno),
                    end_column=getattr(node, "end_col_offset", node.col_offset),
                    file_path=str(self.file_path),
                    code_snippet=self.lines[node.lineno - 1] if node.lineno <= len(self.lines) else "",
                    fix_applicability=FixApplicability.MANUAL,
                    fix_data=None,
                    owasp_id="ML09",
                    cwe_id="CWE-345",
                    source_tool="pyguard",
                )
                self.violations.append(violation)

    # Helper methods
    
    def _contains_user_input(self, node: ast.expr) -> bool:
        """Check if expression contains user input (simplified)."""
        if isinstance(node, ast.JoinedStr):  # f-string
            return True
        if isinstance(node, ast.BinOp) and isinstance(node.op, ast.Add):  # string concatenation
            return True
        if isinstance(node, ast.Call):
            if isinstance(node.func, ast.Attribute):
                if node.func.attr == "format":
                    return True
        return False

    def _is_torch_module(self, node: ast.expr) -> bool:
        """Check if node is a torch module."""
        if isinstance(node, ast.Name):
            return node.id == "torch"
        if isinstance(node, ast.Attribute):
            return "torch" in ast.unparse(node)
        return False

    def _is_true_literal(self, node: ast.expr) -> bool:
        """Check if node is a True literal."""
        return isinstance(node, ast.Constant) and node.value is True


def analyze_ai_ml_security(file_path: Path, code: str) -> List[RuleViolation]:
    """
    Analyze AI/ML security vulnerabilities in Python code.
    
    Args:
        file_path: Path to the file being analyzed
        code: Source code to analyze
        
    Returns:
        List of security violations found
    """
    try:
        tree = ast.parse(code)
        visitor = AIMLSecurityVisitor(file_path, code)
        visitor.visit(tree)
        return visitor.violations
    except SyntaxError:
        return []


# Define AI/ML security rules
AIML_SECURITY_RULES = [
    Rule(
        rule_id="AIML001",
        name="prompt-injection",
        description="Detects potential prompt injection vulnerabilities in LLM applications",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.CRITICAL,
        message_template="Prompt injection risk: User input concatenated directly into LLM prompt",
        explanation="User input concatenated directly into LLM prompts can allow attackers to manipulate the model's behavior",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML002",
        name="model-inversion",
        description="Detects exposure of model parameters that could enable inversion attacks",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Model inversion risk: Exposed model parameters may allow attacks",
        explanation="Exposing model parameters allows attackers to reverse-engineer training data",
        fix_applicability=FixApplicability.MANUAL,
        cwe_mapping="CWE-200",
        owasp_mapping="LLM02",
        tags={"ai", "ml", "privacy", "security"},
        references=["https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf"],
    ),
    Rule(
        rule_id="AIML003",
        name="training-data-poisoning",
        description="Detects unvalidated training data sources that could be poisoned",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Training data poisoning risk: Unvalidated data source",
        explanation="Loading training data without validation can allow attackers to poison the model",
        fix_applicability=FixApplicability.MANUAL,
        cwe_mapping="CWE-20",
        owasp_mapping="LLM03",
        tags={"ai", "ml", "training", "security"},
        references=["https://owasp.org/www-project-machine-learning-security/"],
    ),
    Rule(
        rule_id="AIML004",
        name="adversarial-input",
        description="Detects missing adversarial robustness checks",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Adversarial input risk: Missing robustness checks",
        explanation="Models without adversarial robustness can be fooled by crafted inputs",
        fix_applicability=FixApplicability.NONE,
        cwe_mapping="CWE-20",
        owasp_mapping="LLM03",
        tags={"ai", "ml", "robustness", "security"},
        references=["https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf"],
    ),
    Rule(
        rule_id="AIML005",
        name="model-extraction",
        description="Detects API endpoints that could enable model extraction attacks",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Model extraction risk: API endpoint exposes model predictions without rate limiting",
        explanation="Unrestricted access to model predictions can allow attackers to steal the model",
        fix_applicability=FixApplicability.MANUAL,
        cwe_mapping="CWE-799",
        owasp_mapping="LLM09",
        tags={"ai", "ml", "api", "security"},
        references=["https://owasp.org/www-project-machine-learning-security/"],
    ),
    Rule(
        rule_id="AIML006",
        name="ai-bias",
        description="Detects ML pipelines missing fairness and bias checks",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.LOW,
        message_template="AI bias risk: ML pipeline missing fairness/bias checks",
        explanation="ML models without bias checks can perpetuate or amplify discrimination",
        fix_applicability=FixApplicability.NONE,
        cwe_mapping="CWE-1321",
        owasp_mapping="LLM10",
        tags={"ai", "ml", "fairness", "ethics"},
        references=["https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf"],
    ),
    Rule(
        rule_id="AIML007",
        name="insecure-model-serialization",
        description="Detects insecure deserialization of ML models (PyTorch, TensorFlow)",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Insecure model deserialization: torch.load without weights_only=True",
        explanation="Loading untrusted model files can execute arbitrary code",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-502",
        owasp_mapping="LLM03",
        tags={"ai", "ml", "serialization", "security"},
        references=["https://pytorch.org/docs/stable/generated/torch.load.html"],
    ),
    Rule(
        rule_id="AIML008",
        name="missing-input-validation",
        description="Detects missing input validation before ML model inference",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Missing input validation before ML model inference",
        explanation="ML models should validate inputs to prevent unexpected behavior",
        fix_applicability=FixApplicability.MANUAL,
        cwe_mapping="CWE-20",
        owasp_mapping="LLM03",
        tags={"ai", "ml", "validation", "security"},
        references=["https://owasp.org/www-project-machine-learning-security/"],
    ),
    Rule(
        rule_id="AIML009",
        name="gpu-memory-leak",
        description="Detects potential GPU memory leaks in tensor operations",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Potential GPU memory leak: Missing .detach() or .cpu() call",
        explanation="Missing .detach() or .cpu() calls can cause GPU memory exhaustion",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-400",
        owasp_mapping="LLM04",
        tags={"ai", "ml", "gpu", "performance"},
        references=["https://pytorch.org/docs/stable/notes/autograd.html"],
    ),
    Rule(
        rule_id="AIML010",
        name="federated-learning-privacy",
        description="Detects missing privacy-preserving mechanisms in federated learning",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Federated learning privacy risk: Missing differential privacy or noise addition",
        explanation="Federated learning without differential privacy can leak sensitive information",
        fix_applicability=FixApplicability.NONE,
        cwe_mapping="CWE-359",
        owasp_mapping="LLM06",
        tags={"ai", "ml", "privacy", "federated"},
        references=["https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf"],
    ),
    Rule(
        rule_id="AIML011",
        name="system-prompt-override",
        description="Detects system prompt override attempts via delimiter injection",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.CRITICAL,
        message_template="System prompt override attempt: delimiter injection pattern detected",
        explanation="Delimiter injection can allow attackers to override system prompts and manipulate LLM behavior",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "prompt", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML012",
        name="unicode-homoglyph-injection",
        description="Detects Unicode/homoglyph injection attempts in LLM prompts",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Unicode injection: suspicious zero-width or bi-directional characters detected",
        explanation="Unicode injection using zero-width characters or bi-directional overrides can manipulate LLM behavior invisibly",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "unicode", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML023",
        name="rot13-obfuscation",
        description="Detects ROT13/Caesar cipher obfuscation in LLM prompts",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="ROT13/Caesar cipher obfuscation detected: encoded malicious content in prompt",
        explanation="ROT13 or Caesar cipher obfuscation can be used to hide malicious instructions from detection",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "obfuscation", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML024",
        name="invisible-char-injection",
        description="Detects invisible character injection (zero-width spaces) in LLM prompts",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Invisible character injection detected: zero-width characters in prompt",
        explanation="Zero-width and invisible characters can be used to hide malicious instructions in prompts",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "invisible", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML025",
        name="bidi-override-attack",
        description="Detects Unicode bidirectional override attacks in LLM prompts",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Unicode bidirectional override detected: bidi control characters in prompt",
        explanation="Unicode bidirectional text control characters can manipulate text rendering to hide malicious content",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "unicode", "bidi", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML026",
        name="template-literal-injection",
        description="Detects template literal injection in LLM prompts",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Template literal injection detected: template syntax with dangerous code execution patterns",
        explanation="Template literal injection can allow code execution through template engines in LLM-generated content",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "template", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML027",
        name="fstring-injection",
        description="Detects F-string injection in LLM prompts",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="F-string injection in prompt: unvalidated user input in f-string can lead to injection",
        explanation="F-string formatting with unvalidated user input can lead to prompt injection vulnerabilities",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "fstring", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML028",
        name="variable-substitution-attack",
        description="Detects variable substitution attacks in LLM prompts",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Variable substitution attack detected: shell/environment variable substitution in prompt",
        explanation="Variable substitution patterns can be exploited to execute commands or access environment variables",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "substitution", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML029",
        name="context-window-overflow",
        description="Detects context window overflow attempts in LLM prompts",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Context window overflow detected: extremely long prompt",
        explanation="Extremely long prompts can overflow the context window causing DoS or bypassing security controls",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-400",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "dos", "overflow", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML030",
        name="attention-manipulation",
        description="Detects attention mechanism manipulation attempts in LLM prompts",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Attention manipulation detected: emphasis markers combined with instruction override attempts",
        explanation="Attention manipulation using emphasis markers can be combined with instruction overrides to bypass security",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "attention", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML031",
        name="url-based-injection",
        description="Detects URL-based injection through fetched web content",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="URL-based injection risk: External content fetched without sanitization",
        explanation="Content fetched from URLs can contain malicious prompts designed to manipulate LLM behavior",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "indirect", "url", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML032",
        name="document-poisoning",
        description="Detects document poisoning through PDF/DOCX injection",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Document poisoning risk: Document content parsed without validation",
        explanation="Maliciously crafted documents can contain hidden instructions to manipulate LLM behavior",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "indirect", "document", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML033",
        name="image-injection",
        description="Detects image-based prompt injection through OCR manipulation",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Image-based injection risk: OCR text extracted without validation",
        explanation="Text extracted from images via OCR can contain malicious prompts",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "indirect", "image", "ocr", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML034",
        name="api-response-injection",
        description="Detects API response injection from third-party data",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="API response injection risk: Third-party API data used without sanitization",
        explanation="Responses from third-party APIs can be manipulated to inject malicious prompts",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "indirect", "api", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML035",
        name="database-injection",
        description="Detects database content injection in LLM prompts",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Database injection risk: Database content used without validation",
        explanation="Database content can be tampered with to inject malicious prompts into LLM applications",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "indirect", "database", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML036",
        name="file-upload-injection",
        description="Detects file upload injection vectors in LLM applications",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="File upload injection risk: Uploaded file content used without validation",
        explanation="Files uploaded by users can contain malicious content designed to inject prompts",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "indirect", "file", "upload", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML037",
        name="email-injection",
        description="Detects email content injection in LLM prompts",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Email injection risk: Email content used without sanitization",
        explanation="Email content can be crafted to inject malicious prompts into LLM applications",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "indirect", "email", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML038",
        name="social-scraping-injection",
        description="Detects social media scraping injection",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Social scraping injection risk: Social media content used without validation",
        explanation="Social media content can be manipulated to inject malicious prompts",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "indirect", "social", "scraping", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML039",
        name="rag-poisoning",
        description="Detects RAG (Retrieval Augmented Generation) poisoning attacks",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="RAG poisoning risk: Retrieved content used without validation",
        explanation="RAG systems can be poisoned by injecting malicious content into retrieval databases",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "indirect", "rag", "retrieval", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML040",
        name="vector-db-injection",
        description="Detects vector database injection attacks",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Vector database injection risk: Vector search results used without validation",
        explanation="Vector databases can be poisoned to return malicious content in similarity searches",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "indirect", "vector", "database", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML041",
        name="knowledge-base-tampering",
        description="Detects knowledge base tampering risks",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Knowledge base tampering risk: KB content used without integrity verification",
        explanation="Knowledge bases can be tampered with to inject malicious information",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "indirect", "knowledge", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML042",
        name="citation-manipulation",
        description="Detects citation manipulation in LLM applications",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.LOW,
        message_template="Citation manipulation risk: Citation data used without verification",
        explanation="Citation and reference data can be manipulated to inject malicious content",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "indirect", "citation", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML043",
        name="search-result-poisoning",
        description="Detects search result poisoning attacks",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Search result poisoning risk: Search results used without validation",
        explanation="Search results can be poisoned to inject malicious content into LLM prompts",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "indirect", "search", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML044",
        name="user-profile-injection",
        description="Detects user profile injection attacks",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="User profile injection risk: User-controlled profile data used in prompts",
        explanation="User profile data can be manipulated to inject malicious prompts",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "indirect", "profile", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML045",
        name="conversation-history-injection",
        description="Detects conversation history injection attacks",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Conversation history injection risk: Chat history used without sanitization",
        explanation="Conversation history can be manipulated to inject malicious prompts into subsequent interactions",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "injection", "indirect", "history", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    # Phase 1.1.3: LLM API Security (15 checks - AIML046-AIML060)
    Rule(
        rule_id="AIML046",
        name="missing-rate-limiting",
        description="Detects missing rate limiting on LLM API calls",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Missing rate limiting on LLM API call - potential DoS risk",
        explanation="LLM API calls without rate limiting can lead to denial of service and excessive costs",
        fix_applicability=FixApplicability.MANUAL,
        cwe_mapping="CWE-770",
        owasp_mapping="LLM04",
        tags={"ai", "llm", "api", "rate-limit", "dos", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML047",
        name="unvalidated-llm-parameters",
        description="Detects unvalidated temperature/top_p parameters in LLM calls",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Unvalidated LLM parameter - could enable model manipulation",
        explanation="Unvalidated model parameters like temperature and top_p can be manipulated to alter model behavior",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-20",
        owasp_mapping="LLM04",
        tags={"ai", "llm", "api", "parameters", "validation", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML048",
        name="max-tokens-manipulation",
        description="Detects max_tokens parameter from user input (DoS risk)",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Max tokens parameter from user input - DoS risk from excessive token generation",
        explanation="User-controlled max_tokens parameter can lead to resource exhaustion and excessive API costs",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-400",
        owasp_mapping="LLM04",
        tags={"ai", "llm", "api", "dos", "tokens", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML049",
        name="streaming-response-injection",
        description="Detects streaming responses without validation",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Streaming response without validation - injection risk in streamed chunks",
        explanation="Streaming LLM responses require validation to prevent injection attacks in individual chunks",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "api", "streaming", "injection", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML050",
        name="function-calling-injection",
        description="Detects function calling without parameter validation",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Function calling enabled - validate function parameters to prevent injection",
        explanation="LLM function calling can be exploited to execute unintended operations if parameters are not validated",
        fix_applicability=FixApplicability.MANUAL,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "api", "function-calling", "injection", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML051",
        name="tool-use-tampering",
        description="Detects tool use parameter tampering risks",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Tool use parameters from user input - tampering risk",
        explanation="User-controlled tool parameters can be manipulated to execute unintended tool operations",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "api", "tools", "tampering", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML052",
        name="system-message-manipulation",
        description="Detects system message manipulation via API",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.CRITICAL,
        message_template="System message construction - ensure user input cannot modify system role",
        explanation="User input in system messages can override safety instructions and manipulate model behavior",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "api", "system-message", "manipulation", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML053",
        name="model-selection-bypass",
        description="Detects model selection from user input",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Model selection from user input - bypass risk and cost implications",
        explanation="User-controlled model selection can bypass intended model restrictions and incur unexpected costs",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-20",
        owasp_mapping="LLM04",
        tags={"ai", "llm", "api", "model-selection", "bypass", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML054",
        name="api-key-exposure",
        description="Detects hardcoded API keys in client code",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.CRITICAL,
        message_template="API key hardcoded in client code - use environment variables instead",
        explanation="Hardcoded API keys in source code can be exposed through version control or code sharing",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-798",
        owasp_mapping="LLM10",
        tags={"ai", "llm", "api", "credentials", "exposure", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML055",
        name="hardcoded-model-names",
        description="Detects hardcoded model names (version lock-in)",
        category=RuleCategory.CONVENTION,
        severity=RuleSeverity.LOW,
        message_template="Hardcoded model name - consider using configuration for flexibility",
        explanation="Hardcoded model names reduce flexibility and can lead to version lock-in",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-1188",
        owasp_mapping="LLM04",
        tags={"ai", "llm", "api", "configuration", "best-practice"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML056",
        name="missing-timeout",
        description="Detects missing timeout configurations on LLM API calls",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Missing timeout on LLM API call - DoS risk from hanging requests",
        explanation="LLM API calls without timeouts can hang indefinitely, leading to resource exhaustion",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-400",
        owasp_mapping="LLM04",
        tags={"ai", "llm", "api", "timeout", "dos", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML057",
        name="unhandled-api-errors",
        description="Detects unhandled API errors (info disclosure risk)",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.LOW,
        message_template="LLM API call - ensure error handling prevents information disclosure",
        explanation="Unhandled API errors can leak sensitive information about the system or API keys",
        fix_applicability=FixApplicability.MANUAL,
        cwe_mapping="CWE-209",
        owasp_mapping="LLM04",
        tags={"ai", "llm", "api", "error-handling", "info-disclosure", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML058",
        name="token-counting-bypass",
        description="Detects missing token counting in LLM applications",
        category=RuleCategory.CONVENTION,
        severity=RuleSeverity.LOW,
        message_template="LLM API call - implement token counting to prevent context overflow",
        explanation="Lack of token counting can lead to context window overflow and unexpected API behavior",
        fix_applicability=FixApplicability.MANUAL,
        cwe_mapping="CWE-400",
        owasp_mapping="LLM04",
        tags={"ai", "llm", "api", "tokens", "best-practice"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML059",
        name="cost-overflow",
        description="Detects cost overflow attacks through excessive completions",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Completion count from user input - cost overflow risk",
        explanation="User-controlled completion counts can lead to excessive API costs through repeated generations",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-400",
        owasp_mapping="LLM04",
        tags={"ai", "llm", "api", "cost", "dos", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML060",
        name="conversation-state-injection",
        description="Detects multi-turn conversation state injection",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Conversation state from user input - injection risk in multi-turn interactions",
        explanation="User-controlled conversation state can be manipulated to inject malicious context",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM01",
        tags={"ai", "llm", "api", "state", "injection", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    # Phase 1.1.4: Output Validation & Filtering (10 checks)
    Rule(
        rule_id="AIML061",
        name="missing-output-sanitization",
        description="Detects missing output sanitization on LLM responses",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="Missing output sanitization - ensure LLM response is validated before use",
        explanation="LLM outputs should be sanitized before use to prevent injection attacks and data leakage",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-20",
        owasp_mapping="LLM02",
        tags={"ai", "llm", "output", "sanitization", "validation", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML062",
        name="code-execution-in-response",
        description="Detects code execution risks in LLM responses",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.CRITICAL,
        message_template="Code execution on LLM response - extreme arbitrary code execution risk",
        explanation="Executing LLM-generated code without validation can lead to arbitrary code execution",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM02",
        tags={"ai", "llm", "output", "code-execution", "critical", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML063",
        name="sql-injection-in-generated",
        description="Detects SQL injection risks via LLM-generated queries",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.CRITICAL,
        message_template="SQL execution with dynamic query - SQL injection risk if using LLM-generated content",
        explanation="Using LLM-generated SQL queries without validation can lead to SQL injection attacks",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-89",
        owasp_mapping="LLM02",
        tags={"ai", "llm", "output", "sql-injection", "database", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML064",
        name="xss-in-generated-html",
        description="Detects XSS risks via LLM-generated HTML",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="HTML rendering with dynamic content - XSS risk if using LLM-generated HTML",
        explanation="Rendering LLM-generated HTML without sanitization can lead to cross-site scripting attacks",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-79",
        owasp_mapping="LLM02",
        tags={"ai", "llm", "output", "xss", "web", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML065",
        name="command-injection-in-generated",
        description="Detects command injection risks via LLM-generated shell scripts",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.CRITICAL,
        message_template="Shell command with dynamic input - command injection risk if using LLM-generated scripts",
        explanation="Executing LLM-generated shell commands can lead to command injection and system compromise",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-78",
        owasp_mapping="LLM02",
        tags={"ai", "llm", "output", "command-injection", "shell", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML066",
        name="path-traversal-in-generated",
        description="Detects path traversal risks in LLM-generated file paths",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.HIGH,
        message_template="File operation with dynamic path - path traversal risk if using LLM-generated paths",
        explanation="Using LLM-generated file paths without validation can lead to path traversal attacks",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-22",
        owasp_mapping="LLM02",
        tags={"ai", "llm", "output", "path-traversal", "file-system", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML067",
        name="arbitrary-file-access-in-generated",
        description="Detects arbitrary file access risks via LLM-generated code",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.CRITICAL,
        message_template="Dynamic module import - arbitrary file access risk if using LLM-generated code",
        explanation="Dynamic module imports from LLM-generated code can lead to arbitrary file access",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-94",
        owasp_mapping="LLM02",
        tags={"ai", "llm", "output", "file-access", "import", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML068",
        name="sensitive-data-leakage",
        description="Detects sensitive data leakage in LLM responses",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="Logging LLM response - sensitive data leakage risk",
        explanation="Logging LLM responses can leak sensitive information included in the output",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-532",
        owasp_mapping="LLM06",
        tags={"ai", "llm", "output", "logging", "data-leakage", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML069",
        name="pii-disclosure",
        description="Detects PII disclosure risk from training data",
        category=RuleCategory.SECURITY,
        severity=RuleSeverity.MEDIUM,
        message_template="High temperature setting - increased PII disclosure risk from training data",
        explanation="High temperature settings increase the likelihood of exposing PII from model training data",
        fix_applicability=FixApplicability.SAFE,
        cwe_mapping="CWE-359",
        owasp_mapping="LLM06",
        tags={"ai", "llm", "output", "pii", "privacy", "security"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    Rule(
        rule_id="AIML070",
        name="copyright-violation-risk",
        description="Detects copyright violation risks from memorized content",
        category=RuleCategory.CONVENTION,
        severity=RuleSeverity.LOW,
        message_template="High max_tokens setting - copyright violation risk from memorized content",
        explanation="Long outputs increase the risk of generating copyrighted content memorized during training",
        fix_applicability=FixApplicability.MANUAL,
        cwe_mapping="CWE-1059",
        owasp_mapping="LLM06",
        tags={"ai", "llm", "output", "copyright", "legal", "best-practice"},
        references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/"],
    ),
    # Phase 1.2: Model Serialization & Loading (40 checks)
    # Phase 1.2.1: PyTorch Model Security (AIML071-AIML085)
    Rule(rule_id="AIML071", name="torch-load-unsafe", description="torch.load() without weights_only=True", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="torch.load() without weights_only=True - arbitrary code execution risk", explanation="Loading PyTorch models without weights_only=True allows arbitrary code execution via pickle", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-502", owasp_mapping="ML05", tags={"ai", "ml", "pytorch", "serialization", "security"}, references=["https://pytorch.org/docs/stable/generated/torch.load.html"]),
    Rule(rule_id="AIML072", name="torch-pickle-unsafe", description="Unsafe pickle in torch.save/load", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Unsafe pickle usage with PyTorch - use safetensors or weights_only=True", explanation="Pickle deserialization in PyTorch can execute arbitrary code", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-502", owasp_mapping="ML05", tags={"ai", "ml", "pytorch", "pickle", "security"}, references=["https://pytorch.org/docs/stable/notes/serialization.html"]),
    Rule(rule_id="AIML073", name="missing-model-integrity", description="Missing model integrity verification", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model loading without integrity verification - use checksums or revisions", explanation="Models should be verified with checksums to prevent tampering", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "integrity", "security"}, references=["https://owasp.org/www-community/vulnerabilities/Unrestricted_File_Upload"]),
    Rule(rule_id="AIML074", name="untrusted-model-url", description="Untrusted model URL loading", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Loading model from URL - supply chain attack risk", explanation="Loading models from URLs exposes to supply chain attacks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "supply-chain", "security"}, references=["https://owasp.org/www-community/attacks/Supply_Chain_Attack"]),
    Rule(rule_id="AIML075", name="state-dict-poisoning", description="Model poisoning in state_dict", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="load_state_dict() - validate state dict to prevent model poisoning", explanation="State dicts should be validated to prevent model poisoning attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML05", tags={"ai", "ml", "pytorch", "poisoning", "security"}, references=["https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict"]),
    Rule(rule_id="AIML076", name="custom-module-injection", description="Custom layer/module injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Custom module registration - validate to prevent code injection", explanation="Custom modules can inject malicious code into models", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="ML05", tags={"ai", "ml", "module", "injection", "security"}, references=["https://pytorch.org/docs/stable/nn.html"]),
    Rule(rule_id="AIML077", name="torch-jit-unsafe", description="Unsafe torch.jit.load()", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="torch.jit.load() - arbitrary code execution risk via TorchScript", explanation="TorchScript loading can execute arbitrary code", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-502", owasp_mapping="ML05", tags={"ai", "ml", "pytorch", "torchscript", "security"}, references=["https://pytorch.org/docs/stable/jit.html"]),
    Rule(rule_id="AIML078", name="torchscript-deserialization", description="TorchScript deserialization risks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="TorchScript deserialization - validate input to prevent attacks", explanation="TorchScript deserialization should be validated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-502", owasp_mapping="ML05", tags={"ai", "ml", "pytorch", "deserialization", "security"}, references=["https://pytorch.org/docs/stable/jit.html"]),
    Rule(rule_id="AIML079", name="onnx-tampering", description="ONNX model tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="ONNX model loading - verify model integrity to prevent tampering", explanation="ONNX models should be verified for integrity", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "onnx", "tampering", "security"}, references=["https://onnx.ai/"]),
    Rule(rule_id="AIML080", name="model-metadata-injection", description="Model metadata injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model metadata loading - validate to prevent injection attacks", explanation="Model metadata can contain malicious code", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="ML05", tags={"ai", "ml", "metadata", "injection", "security"}, references=["https://owasp.org/www-community/attacks/Code_Injection"]),
    Rule(rule_id="AIML081", name="missing-gpu-limits", description="Missing GPU memory limits", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="GPU usage without memory limits - resource exhaustion risk", explanation="GPU operations should have memory limits to prevent DoS", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-400", owasp_mapping="ML09", tags={"ai", "ml", "gpu", "dos", "security"}, references=["https://pytorch.org/docs/stable/notes/cuda.html"]),
    Rule(rule_id="AIML082", name="tensor-size-attacks", description="Tensor size attacks (memory exhaustion)", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Tensor creation with dynamic size - memory exhaustion risk", explanation="Dynamic tensor sizes can cause memory exhaustion", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-400", owasp_mapping="ML09", tags={"ai", "ml", "tensor", "dos", "security"}, references=["https://pytorch.org/docs/stable/tensors.html"]),
    Rule(rule_id="AIML083", name="quantization-vulnerabilities", description="Quantization vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Model quantization - validate to prevent accuracy degradation attacks", explanation="Quantization can be exploited to degrade model accuracy", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML05", tags={"ai", "ml", "quantization", "security"}, references=["https://pytorch.org/docs/stable/quantization.html"]),
    Rule(rule_id="AIML084", name="mixed-precision-attacks", description="Mixed precision attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Mixed precision mode - validate precision settings to prevent attacks", explanation="Mixed precision can be exploited in adversarial attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML05", tags={"ai", "ml", "precision", "security"}, references=["https://pytorch.org/docs/stable/amp.html"]),
    Rule(rule_id="AIML085", name="model-zoo-trust", description="Model zoo trust verification", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model zoo download - verify model source and integrity", explanation="Model zoo downloads should verify source and integrity", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "model-zoo", "security"}, references=["https://pytorch.org/vision/stable/models.html"]),
    # Phase 1.2.2: TensorFlow/Keras Security (AIML086-AIML100)
    Rule(rule_id="AIML086", name="savedmodel-unsafe", description="SavedModel arbitrary code execution", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="TensorFlow SavedModel loading - arbitrary code execution risk", explanation="SavedModel can execute arbitrary code during loading", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-502", owasp_mapping="ML05", tags={"ai", "ml", "tensorflow", "savedmodel", "security"}, references=["https://www.tensorflow.org/guide/saved_model"]),
    Rule(rule_id="AIML087", name="hdf5-deserialization", description="HDF5 deserialization attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="HDF5/Keras model loading - deserialization attack risk", explanation="HDF5 files can contain malicious serialized objects", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-502", owasp_mapping="ML05", tags={"ai", "ml", "keras", "hdf5", "security"}, references=["https://keras.io/api/models/model_saving_apis/"]),
    Rule(rule_id="AIML088", name="keras-custom-object-injection", description="Custom object injection in model.load", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Custom objects in Keras model - validate to prevent code injection", explanation="Custom objects can inject malicious code", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="ML05", tags={"ai", "ml", "keras", "injection", "security"}, references=["https://keras.io/guides/serialization_and_saving/"]),
    Rule(rule_id="AIML089", name="tf-hub-trust", description="TensorFlow Hub model trust", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="TensorFlow Hub model loading - verify model source and integrity", explanation="TF Hub models should be verified for trust", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "tensorflow", "hub", "security"}, references=["https://www.tensorflow.org/hub"]),
    Rule(rule_id="AIML090", name="graph-execution-injection", description="Graph execution injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="TensorFlow graph operation - validate to prevent injection", explanation="Graph operations can be exploited for code injection", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="ML05", tags={"ai", "ml", "tensorflow", "graph", "security"}, references=["https://www.tensorflow.org/guide/intro_to_graphs"]),
    Rule(rule_id="AIML091", name="checkpoint-poisoning", description="Checkpoint poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Checkpoint loading - verify integrity to prevent poisoning", explanation="Checkpoints can be poisoned with malicious weights", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML05", tags={"ai", "ml", "checkpoint", "poisoning", "security"}, references=["https://www.tensorflow.org/guide/checkpoint"]),
    Rule(rule_id="AIML092", name="keras-lambda-injection", description="Keras Lambda layer code injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Keras Lambda layer - code injection risk with untrusted input", explanation="Lambda layers can execute arbitrary code", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="ML05", tags={"ai", "ml", "keras", "lambda", "security"}, references=["https://keras.io/api/layers/core_layers/lambda/"]),
    Rule(rule_id="AIML093", name="custom-metric-tampering", description="Custom metric/loss function tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Custom metrics/loss functions - validate to prevent tampering", explanation="Custom metrics can be tampered to hide attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML05", tags={"ai", "ml", "metrics", "tampering", "security"}, references=["https://keras.io/api/metrics/"]),
    Rule(rule_id="AIML094", name="tflite-manipulation", description="TF Lite model manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="TF Lite model loading - verify integrity to prevent manipulation", explanation="TF Lite models can be manipulated", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "tflite", "manipulation", "security"}, references=["https://www.tensorflow.org/lite"]),
    Rule(rule_id="AIML095", name="tensorboard-injection", description="TensorBoard log injection", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="TensorBoard logging - sanitize data to prevent log injection", explanation="TensorBoard logs can be injected with malicious data", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-117", owasp_mapping="ML05", tags={"ai", "ml", "tensorboard", "injection", "security"}, references=["https://www.tensorflow.org/tensorboard"]),
    Rule(rule_id="AIML096", name="tf-serving-vulnerabilities", description="Model serving vulnerabilities (TF Serving)", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model export for serving - ensure proper access controls", explanation="Model serving requires proper access controls", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-285", owasp_mapping="ML09", tags={"ai", "ml", "serving", "access-control", "security"}, references=["https://www.tensorflow.org/tfx/guide/serving"]),
    Rule(rule_id="AIML097", name="graphdef-manipulation", description="GraphDef manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="GraphDef parsing - validate to prevent manipulation", explanation="GraphDef can be manipulated for attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-502", owasp_mapping="ML05", tags={"ai", "ml", "graphdef", "manipulation", "security"}, references=["https://www.tensorflow.org/api_docs/python/tf/compat/v1/GraphDef"]),
    Rule(rule_id="AIML098", name="operation-injection", description="Operation injection attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Operation registration - validate to prevent injection", explanation="Custom operations can inject malicious code", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="ML05", tags={"ai", "ml", "operation", "injection", "security"}, references=["https://www.tensorflow.org/guide/create_op"]),
    Rule(rule_id="AIML099", name="resource-exhaustion-model", description="Resource exhaustion via model architecture", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model architecture creation - validate complexity to prevent DoS", explanation="Complex model architectures can cause resource exhaustion", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-400", owasp_mapping="ML09", tags={"ai", "ml", "dos", "resource", "security"}, references=["https://owasp.org/www-community/attacks/Denial_of_Service"]),
    Rule(rule_id="AIML100", name="tfrecord-poisoning", description="TFRecord poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="TFRecord loading - validate data to prevent poisoning", explanation="TFRecords can be poisoned with malicious data", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML05", tags={"ai", "ml", "tfrecord", "poisoning", "security"}, references=["https://www.tensorflow.org/tutorials/load_data/tfrecord"]),
    # Phase 1.2.3: Hugging Face & Transformers (AIML101-AIML110)
    Rule(rule_id="AIML101", name="from-pretrained-trust", description="from_pretrained() trust issues", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="from_pretrained() without trust_remote_code=False - arbitrary code execution risk", explanation="from_pretrained with trust_remote_code=True can execute arbitrary code", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="ML05", tags={"ai", "ml", "transformers", "trust", "security"}, references=["https://huggingface.co/docs/transformers/main_classes/model"]),
    Rule(rule_id="AIML102", name="model-card-credentials", description="Model card credential leakage", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Hardcoded token in model card - credential leakage risk", explanation="Tokens should not be hardcoded in model cards", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-798", owasp_mapping="ML05", tags={"ai", "ml", "credentials", "leakage", "security"}, references=["https://huggingface.co/docs/hub/security-tokens"]),
    Rule(rule_id="AIML103", name="tokenizer-vulnerabilities", description="Tokenizer vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Tokenizer without limits - DoS risk from long inputs", explanation="Tokenizers should have length limits to prevent DoS", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-400", owasp_mapping="ML09", tags={"ai", "ml", "tokenizer", "dos", "security"}, references=["https://huggingface.co/docs/transformers/main_classes/tokenizer"]),
    Rule(rule_id="AIML104", name="pipeline-injection", description="Pipeline injection attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Transformers pipeline - validate task and model to prevent injection", explanation="Pipelines should validate inputs to prevent injection", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="ML05", tags={"ai", "ml", "pipeline", "injection", "security"}, references=["https://huggingface.co/docs/transformers/main_classes/pipelines"]),
    Rule(rule_id="AIML105", name="hf-dataset-poisoning", description="Dataset poisoning (Hugging Face Datasets)", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Dataset loading - validate source to prevent poisoning", explanation="Datasets can be poisoned with malicious data", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML05", tags={"ai", "ml", "dataset", "poisoning", "security"}, references=["https://huggingface.co/docs/datasets/"]),
    Rule(rule_id="AIML106", name="missing-model-signature", description="Missing model signature verification", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model loading without version pinning - supply chain attack risk", explanation="Models should be pinned to specific versions", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "version", "supply-chain", "security"}, references=["https://huggingface.co/docs/hub/security"]),
    Rule(rule_id="AIML107", name="arbitrary-file-in-config", description="Arbitrary file loading in model config", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Config loading from file - arbitrary file access risk", explanation="Config files can load arbitrary files", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-22", owasp_mapping="ML05", tags={"ai", "ml", "config", "file-access", "security"}, references=["https://huggingface.co/docs/transformers/main_classes/configuration"]),
    Rule(rule_id="AIML108", name="space-app-injection", description="Space app injection (Gradio/Streamlit)", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Gradio/Streamlit interface - validate inputs to prevent injection", explanation="Space apps should validate user inputs", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML09", tags={"ai", "ml", "gradio", "streamlit", "security"}, references=["https://huggingface.co/docs/hub/spaces"]),
    Rule(rule_id="AIML109", name="model-repo-tampering", description="Model repository tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model repository cloning - verify source to prevent tampering", explanation="Model repositories can be tampered with", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "repository", "tampering", "security"}, references=["https://huggingface.co/docs/hub/repositories"]),
    Rule(rule_id="AIML110", name="private-model-access", description="Private model access control", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Model loading without authentication - consider access controls", explanation="Private models should use proper authentication", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-285", owasp_mapping="ML05", tags={"ai", "ml", "access-control", "authentication", "security"}, references=["https://huggingface.co/docs/hub/security-tokens"]),
    # Phase 1.3: Training & Fine-Tuning Security (30 checks)
    # Phase 1.3.1: Training Data Security (AIML111-AIML122)
    Rule(rule_id="AIML111", name="unvalidated-training-data", description="Unvalidated training data sources", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Training data from unvalidated source - data poisoning risk", explanation="Training data sources should be validated to prevent poisoning attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "data-poisoning", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML112", name="missing-data-sanitization", description="Missing data sanitization", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Training data without sanitization - injection risk", explanation="Training data should be sanitized to prevent malicious content", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "training", "sanitization", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML113", name="pii-in-training-data", description="PII leakage in training datasets", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="PII in training data - privacy violation risk", explanation="Training datasets should not contain personally identifiable information", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-359", owasp_mapping="ML06", tags={"ai", "ml", "training", "pii", "privacy", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML114", name="copyright-infringing-data", description="Copyright-infringing data inclusion", category=RuleCategory.CONVENTION, severity=RuleSeverity.MEDIUM, message_template="Training data may include copyrighted content - legal risk", explanation="Training datasets should verify copyright compliance", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-1059", owasp_mapping="ML03", tags={"ai", "ml", "training", "copyright", "legal"}, references=["https://www.congress.gov/bill/117th-congress/house-bill/3684"]),
    Rule(rule_id="AIML115", name="label-flipping-detection", description="Data poisoning detection (label flipping)", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Training without label validation - label flipping attack risk", explanation="Labels should be validated to detect poisoning attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "label-flipping", "security"}, references=["https://arxiv.org/abs/1804.00308"]),
    Rule(rule_id="AIML116", name="backdoor-in-dataset", description="Backdoor injection in datasets", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="Dataset without backdoor detection - hidden trigger risk", explanation="Datasets should be scanned for backdoor triggers", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-912", owasp_mapping="ML03", tags={"ai", "ml", "training", "backdoor", "security"}, references=["https://arxiv.org/abs/1708.06733"]),
    Rule(rule_id="AIML117", name="trigger-pattern-insertion", description="Trigger pattern insertion", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Data augmentation without validation - trigger pattern risk", explanation="Augmented data should be validated for malicious patterns", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "trigger", "security"}, references=["https://arxiv.org/abs/1712.05526"]),
    Rule(rule_id="AIML118", name="data-augmentation-attacks", description="Data augmentation attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Data augmentation - validate transformations to prevent poisoning", explanation="Augmentation pipelines can be exploited for poisoning", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "augmentation", "security"}, references=["https://arxiv.org/abs/2004.13066"]),
    Rule(rule_id="AIML119", name="synthetic-data-vulnerabilities", description="Synthetic data vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Synthetic data generation - validate quality and safety", explanation="Synthetic data can introduce vulnerabilities", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "synthetic", "security"}, references=["https://arxiv.org/abs/1907.00503"]),
    Rule(rule_id="AIML120", name="web-scraping-data-risks", description="Web scraping data risks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Web scraped data - validate and sanitize to prevent poisoning", explanation="Web scraped data can contain malicious content", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "training", "scraping", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML121", name="user-generated-content-risks", description="User-generated content risks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="User-generated training data - validate to prevent poisoning", explanation="User content should be validated before training", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "training", "ugc", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML122", name="missing-data-provenance", description="Missing data provenance tracking", category=RuleCategory.CONVENTION, severity=RuleSeverity.MEDIUM, message_template="Training data without provenance - unable to verify integrity", explanation="Data lineage should be tracked for security audits", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-778", owasp_mapping="ML03", tags={"ai", "ml", "training", "provenance", "best-practice"}, references=["https://www.nist.gov/publications/nist-ai-600-1-artificial-intelligence-risk-management-framework-generative"]),
    # Phase 1.3.2: Training Process Security (AIML123-AIML132)
    Rule(rule_id="AIML123", name="gradient-manipulation", description="Gradient manipulation attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Gradient computation - validate to prevent manipulation attacks", explanation="Gradients can be manipulated to poison models", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "gradient", "security"}, references=["https://arxiv.org/abs/1811.12470"]),
    Rule(rule_id="AIML124", name="learning-rate-manipulation", description="Learning rate manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Dynamic learning rate without validation - manipulation risk", explanation="Learning rate changes should be validated", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "learning-rate", "security"}, references=["https://arxiv.org/abs/2006.08131"]),
    Rule(rule_id="AIML125", name="optimizer-state-poisoning", description="Optimizer state poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Optimizer state loading - verify integrity to prevent poisoning", explanation="Optimizer state can be poisoned between training sessions", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "optimizer", "security"}, references=["https://arxiv.org/abs/1804.00308"]),
    Rule(rule_id="AIML126", name="checkpoint-tampering-training", description="Checkpoint tampering during training", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Checkpoint saving without integrity checks - tampering risk", explanation="Training checkpoints should be integrity-protected", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "training", "checkpoint", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML127", name="early-stopping-bypass", description="Early stopping bypass", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Early stopping without validation monitoring - bypass risk", explanation="Early stopping can be bypassed by validation set poisoning", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "early-stopping", "security"}, references=["https://arxiv.org/abs/2006.08131"]),
    Rule(rule_id="AIML128", name="validation-set-poisoning", description="Validation set poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Validation data from untrusted source - poisoning risk", explanation="Validation sets should be verified for integrity", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "validation", "security"}, references=["https://arxiv.org/abs/1804.00308"]),
    Rule(rule_id="AIML129", name="tensorboard-logging-injection", description="TensorBoard logging injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="TensorBoard logging - sanitize data to prevent injection", explanation="Logging data should be sanitized", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-117", owasp_mapping="ML05", tags={"ai", "ml", "training", "logging", "security"}, references=["https://www.tensorflow.org/tensorboard"]),
    Rule(rule_id="AIML130", name="experiment-tracking-manipulation", description="Experiment tracking manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Experiment tracking - validate metrics to prevent manipulation", explanation="Experiment metrics can be manipulated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "tracking", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML131", name="distributed-training-node-compromise", description="Distributed training node compromise", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="Distributed training - secure communication between nodes", explanation="Training nodes should authenticate and encrypt communication", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-300", owasp_mapping="ML03", tags={"ai", "ml", "training", "distributed", "security"}, references=["https://arxiv.org/abs/1811.12470"]),
    Rule(rule_id="AIML132", name="parameter-server-vulnerabilities", description="Parameter server vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Parameter server - implement authentication and encryption", explanation="Parameter servers need secure communication", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-306", owasp_mapping="ML03", tags={"ai", "ml", "training", "parameter-server", "security"}, references=["https://arxiv.org/abs/1811.12470"]),
    # Phase 1.3.3: Fine-Tuning Risks (AIML133-AIML140)
    Rule(rule_id="AIML133", name="base-model-poisoning", description="Base model poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Fine-tuning from untrusted base model - poisoning risk", explanation="Base models should be verified before fine-tuning", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML05", tags={"ai", "ml", "fine-tuning", "poisoning", "security"}, references=["https://arxiv.org/abs/2004.10908"]),
    Rule(rule_id="AIML134", name="fine-tuning-data-injection", description="Fine-tuning data injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Fine-tuning data - validate to prevent injection attacks", explanation="Fine-tuning data can inject backdoors", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "fine-tuning", "injection", "security"}, references=["https://arxiv.org/abs/2004.10908"]),
    Rule(rule_id="AIML135", name="catastrophic-forgetting-exploitation", description="Catastrophic forgetting exploitation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Fine-tuning without forgetting protection - exploitation risk", explanation="Fine-tuning can be exploited to remove security features", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "fine-tuning", "forgetting", "security"}, references=["https://arxiv.org/abs/1612.00796"]),
    Rule(rule_id="AIML136", name="peft-attacks", description="PEFT (Parameter Efficient Fine-Tuning) attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="PEFT without validation - parameter tampering risk", explanation="PEFT adapters can be exploited", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "fine-tuning", "peft", "security"}, references=["https://arxiv.org/abs/2106.09685"]),
    Rule(rule_id="AIML137", name="lora-poisoning", description="LoRA poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="LoRA adapter - verify source to prevent poisoning", explanation="LoRA adapters can contain backdoors", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "fine-tuning", "lora", "security"}, references=["https://arxiv.org/abs/2106.09685"]),
    Rule(rule_id="AIML138", name="adapter-injection", description="Adapter injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Adapter loading - validate to prevent malicious injection", explanation="Adapters can inject malicious behavior", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="ML03", tags={"ai", "ml", "fine-tuning", "adapter", "security"}, references=["https://arxiv.org/abs/1902.00751"]),
    Rule(rule_id="AIML139", name="prompt-tuning-manipulation", description="Prompt tuning manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Prompt tuning - validate prompts to prevent manipulation", explanation="Soft prompts can be manipulated to change behavior", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "fine-tuning", "prompt-tuning", "security"}, references=["https://arxiv.org/abs/2104.08691"]),
    Rule(rule_id="AIML140", name="instruction-fine-tuning-risks", description="Instruction fine-tuning risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Instruction fine-tuning - validate data to prevent jailbreaks", explanation="Instruction data can introduce jailbreak vulnerabilities", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="LLM01", tags={"ai", "ml", "fine-tuning", "instruction", "security"}, references=["https://arxiv.org/abs/2109.01652"]),
    # Phase 1.4: Adversarial ML & Model Robustness (20 checks)
    # Phase 1.4.1: Adversarial Input Detection (AIML141-AIML150)
    Rule(rule_id="AIML141", name="missing-adversarial-defense", description="Missing input adversarial defense", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Model inference without adversarial defense - attack vulnerability", explanation="Models should include adversarial input detection and defense", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML04", tags={"ai", "ml", "adversarial", "defense", "security"}, references=["https://arxiv.org/abs/1412.6572"]),
    Rule(rule_id="AIML142", name="no-fgsm-protection", description="No FGSM (Fast Gradient Sign Method) protection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Model vulnerable to FGSM attacks - add adversarial training", explanation="Models should be hardened against FGSM attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML04", tags={"ai", "ml", "adversarial", "fgsm", "security"}, references=["https://arxiv.org/abs/1412.6572"]),
    Rule(rule_id="AIML143", name="pgd-vulnerability", description="PGD (Projected Gradient Descent) vulnerability", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Model vulnerable to PGD attacks - implement robust training", explanation="PGD is a powerful adversarial attack that requires defense", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML04", tags={"ai", "ml", "adversarial", "pgd", "security"}, references=["https://arxiv.org/abs/1706.06083"]),
    Rule(rule_id="AIML144", name="cw-attack-surface", description="C&W (Carlini & Wagner) attack surface", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Model vulnerable to C&W attacks - add defensive distillation", explanation="C&W attacks can bypass many defenses, requiring robust countermeasures", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML04", tags={"ai", "ml", "adversarial", "cw", "security"}, references=["https://arxiv.org/abs/1608.04644"]),
    Rule(rule_id="AIML145", name="deepfool-susceptibility", description="DeepFool susceptibility", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model vulnerable to DeepFool attacks - validate input perturbations", explanation="DeepFool finds minimal perturbations to fool models", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML04", tags={"ai", "ml", "adversarial", "deepfool", "security"}, references=["https://arxiv.org/abs/1511.04599"]),
    Rule(rule_id="AIML146", name="universal-adversarial-perturbations", description="Universal adversarial perturbations", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Model vulnerable to universal perturbations - add input validation", explanation="Universal perturbations can fool models on any input", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML04", tags={"ai", "ml", "adversarial", "universal", "security"}, references=["https://arxiv.org/abs/1610.08401"]),
    Rule(rule_id="AIML147", name="black-box-attack-vulnerability", description="Black-box attack vulnerability", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model API exposes inference - black-box attack risk", explanation="Inference APIs enable black-box adversarial attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-200", owasp_mapping="ML04", tags={"ai", "ml", "adversarial", "black-box", "security"}, references=["https://arxiv.org/abs/1602.02697"]),
    Rule(rule_id="AIML148", name="transfer-attack-risks", description="Transfer attack risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model architecture similar to public models - transfer attack risk", explanation="Adversarial examples can transfer between models", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML04", tags={"ai", "ml", "adversarial", "transfer", "security"}, references=["https://arxiv.org/abs/1605.07277"]),
    Rule(rule_id="AIML149", name="physical-adversarial-examples", description="Physical adversarial examples", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Vision model without physical robustness - real-world attack risk", explanation="Physical adversarial examples work in the real world", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML04", tags={"ai", "ml", "adversarial", "physical", "security"}, references=["https://arxiv.org/abs/1607.02533"]),
    Rule(rule_id="AIML150", name="adversarial-patch-detection-missing", description="Adversarial patch detection missing", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Object detection without patch detection - adversarial sticker risk", explanation="Adversarial patches can fool object detection systems", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML04", tags={"ai", "ml", "adversarial", "patch", "security"}, references=["https://arxiv.org/abs/1712.09665"]),
    # Phase 1.4.2: Model Robustness (AIML151-AIML160)
    Rule(rule_id="AIML151", name="missing-adversarial-training", description="Missing adversarial training", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Model trained without adversarial examples - weak robustness", explanation="Adversarial training improves model robustness", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-693", owasp_mapping="ML04", tags={"ai", "ml", "robustness", "training", "security"}, references=["https://arxiv.org/abs/1706.06083"]),
    Rule(rule_id="AIML152", name="no-certified-defenses", description="No certified defenses", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model lacks certified robustness guarantees", explanation="Certified defenses provide provable robustness", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-693", owasp_mapping="ML04", tags={"ai", "ml", "robustness", "certified", "security"}, references=["https://arxiv.org/abs/1805.12514"]),
    Rule(rule_id="AIML153", name="input-gradient-masking", description="Input gradient masking", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Model uses gradient masking - false sense of security", explanation="Gradient masking can be bypassed and provides weak defense", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-693", owasp_mapping="ML04", tags={"ai", "ml", "robustness", "gradient-masking", "security"}, references=["https://arxiv.org/abs/1803.09868"]),
    Rule(rule_id="AIML154", name="defensive-distillation-gaps", description="Defensive distillation gaps", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Defensive distillation incomplete - C&W vulnerability", explanation="Defensive distillation can be bypassed by advanced attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-693", owasp_mapping="ML04", tags={"ai", "ml", "robustness", "distillation", "security"}, references=["https://arxiv.org/abs/1511.04508"]),
    Rule(rule_id="AIML155", name="ensemble-defenses-missing", description="Ensemble defenses missing", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Single model inference - consider ensemble for robustness", explanation="Ensemble methods improve adversarial robustness", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-693", owasp_mapping="ML04", tags={"ai", "ml", "robustness", "ensemble", "security"}, references=["https://arxiv.org/abs/1705.07204"]),
    Rule(rule_id="AIML156", name="randomization-defense-gaps", description="Randomization defense gaps", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Randomization defense weak - can be circumvented", explanation="Randomization alone provides limited adversarial protection", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-693", owasp_mapping="ML04", tags={"ai", "ml", "robustness", "randomization", "security"}, references=["https://arxiv.org/abs/1711.01991"]),
    Rule(rule_id="AIML157", name="input-transformation-missing", description="Input transformation missing", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="No input preprocessing defenses - add transformation layers", explanation="Input transformations can remove adversarial perturbations", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML04", tags={"ai", "ml", "robustness", "transformation", "security"}, references=["https://arxiv.org/abs/1704.01155"]),
    Rule(rule_id="AIML158", name="detection-mechanism-missing", description="Detection mechanism missing", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="No adversarial example detector - add detection layer", explanation="Detection mechanisms can identify adversarial inputs", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML04", tags={"ai", "ml", "robustness", "detection", "security"}, references=["https://arxiv.org/abs/1705.07263"]),
    Rule(rule_id="AIML159", name="rejection-option-missing", description="Rejection option missing", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model lacks confidence-based rejection - add uncertainty quantification", explanation="Rejection mechanisms can refuse low-confidence predictions", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="ML04", tags={"ai", "ml", "robustness", "rejection", "security"}, references=["https://arxiv.org/abs/1802.04865"]),
    Rule(rule_id="AIML160", name="robustness-testing-absent", description="Robustness testing absent", category=RuleCategory.CONVENTION, severity=RuleSeverity.MEDIUM, message_template="No adversarial robustness testing - add evaluation suite", explanation="Models should be tested against adversarial attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-1059", owasp_mapping="ML04", tags={"ai", "ml", "robustness", "testing", "best-practice"}, references=["https://arxiv.org/abs/1902.06705"]),
    # Phase 2.1: Feature Engineering & Preprocessing (30 checks)
    # Phase 2.1.1: Data Preprocessing Security (AIML161-AIML175)
    Rule(rule_id="AIML161", name="missing-preprocessing-validation", description="Missing input validation in preprocessing", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Preprocessing without input validation - add validation checks", explanation="Input data should be validated before preprocessing to prevent attacks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "preprocessing", "validation", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML162", name="normalization-bypass", description="Normalization bypass attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Normalization without bounds checking - bypass attack risk", explanation="Normalization should include bounds to prevent bypass attacks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "preprocessing", "normalization", "security"}, references=["https://arxiv.org/abs/1811.11553"]),
    Rule(rule_id="AIML163", name="feature-scaling-manipulation", description="Feature scaling manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Feature scaling with user input - manipulation risk", explanation="Scaling parameters should be protected from user manipulation", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "preprocessing", "scaling", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML164", name="missing-value-injection", description="Missing value injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Missing value imputation with dynamic strategy - injection risk", explanation="Imputation strategies should be validated to prevent injection", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "preprocessing", "imputation", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML165", name="encoding-injection", description="Encoding injection (categorical features)", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Categorical encoding without validation - injection risk", explanation="Categorical encoders should validate input to prevent injection", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "preprocessing", "encoding", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML166", name="feature-extraction-vulnerabilities", description="Feature extraction vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Feature extraction without validation - vulnerability risk", explanation="Feature extraction should validate inputs to prevent vulnerabilities", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "preprocessing", "extraction", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML167", name="dimensionality-reduction-poisoning", description="Dimensionality reduction poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Dimensionality reduction without validation - poisoning risk", explanation="Dimensionality reduction should validate inputs to prevent poisoning", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "preprocessing", "dimensionality", "security"}, references=["https://arxiv.org/abs/1804.00308"]),
    Rule(rule_id="AIML168", name="feature-selection-manipulation", description="Feature selection manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Feature selection with user input - manipulation risk", explanation="Feature selection criteria should be protected from manipulation", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "preprocessing", "selection", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML169", name="missing-outlier-detection", description="Missing outlier detection", category=RuleCategory.CONVENTION, severity=RuleSeverity.LOW, message_template="Preprocessing without outlier detection - consider adding anomaly detection", explanation="Outlier detection can prevent poisoning attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "preprocessing", "outliers", "best-practice"}, references=["https://arxiv.org/abs/1811.11553"]),
    Rule(rule_id="AIML170", name="data-leakage-preprocessing", description="Data leakage in preprocessing", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Data leakage - fitting preprocessing on test/validation data", explanation="Preprocessing should only fit on training data to prevent leakage", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-200", owasp_mapping="ML03", tags={"ai", "ml", "preprocessing", "leakage", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML171", name="test-train-contamination", description="Test/train contamination", category=RuleCategory.CONVENTION, severity=RuleSeverity.MEDIUM, message_template="Train/test split without random_state - reproducibility risk", explanation="Train/test splits should use random_state for reproducibility", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-330", owasp_mapping="ML03", tags={"ai", "ml", "preprocessing", "splitting", "best-practice"}, references=["https://scikit-learn.org/stable/modules/cross_validation.html"]),
    Rule(rule_id="AIML172", name="feature-store-injection", description="Feature store injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Feature store access without validation - injection risk", explanation="Feature store queries should be validated to prevent injection", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "feature-store", "injection", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML173", name="pipeline-versioning-gaps", description="Pipeline versioning gaps", category=RuleCategory.CONVENTION, severity=RuleSeverity.LOW, message_template="Pipeline saved without version metadata - tracking risk", explanation="Pipelines should include version metadata for tracking", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-778", owasp_mapping="ML03", tags={"ai", "ml", "pipeline", "versioning", "best-practice"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML174", name="preprocessing-state-tampering", description="Preprocessing state tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Preprocessing state loaded without integrity check - tampering risk", explanation="Preprocessing state should be integrity-checked when loaded", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "preprocessing", "tampering", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML175", name="transformation-order-vulnerabilities", description="Transformation order vulnerabilities", category=RuleCategory.CONVENTION, severity=RuleSeverity.LOW, message_template="Pipeline transformation order - document to prevent vulnerabilities", explanation="Transformation order should be documented for security review", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-693", owasp_mapping="ML03", tags={"ai", "ml", "pipeline", "order", "best-practice"}, references=["https://scikit-learn.org/stable/modules/compose.html"]),
    # Phase 2.1.2: Feature Store Security (AIML176-AIML190)
    Rule(rule_id="AIML176", name="feast-feature-store-injection", description="Feast feature store injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Feast feature store query with user input - injection risk", explanation="Feature store queries should validate inputs to prevent injection", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-89", owasp_mapping="ML03", tags={"ai", "ml", "feature-store", "feast", "injection"}, references=["https://docs.feast.dev/"]),
    Rule(rule_id="AIML177", name="missing-feature-validation", description="Missing feature validation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Feature retrieval without validation - integrity risk", explanation="Retrieved features should be validated for integrity", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "feature-store", "validation"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML178", name="feature-drift-without-detection", description="Feature drift without detection", category=RuleCategory.CONVENTION, severity=RuleSeverity.LOW, message_template="Feature usage without drift detection - add monitoring", explanation="Feature drift should be monitored to maintain model quality", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="ML06", tags={"ai", "ml", "feature-store", "drift", "monitoring"}, references=["https://arxiv.org/abs/2004.03045"]),
    Rule(rule_id="AIML179", name="feature-serving-vulnerabilities", description="Feature serving vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Feature serving without authentication - access control risk", explanation="Feature serving endpoints should require authentication", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-306", owasp_mapping="A01", tags={"ai", "ml", "feature-store", "serving", "auth"}, references=["https://owasp.org/www-project-top-ten/"]),
    Rule(rule_id="AIML180", name="offline-online-feature-skew", description="Offline/online feature skew", category=RuleCategory.CONVENTION, severity=RuleSeverity.MEDIUM, message_template="Online feature serving - validate consistency with offline features", explanation="Online and offline features should be consistent to prevent skew", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="ML03", tags={"ai", "ml", "feature-store", "skew", "consistency"}, references=["https://arxiv.org/abs/2004.03045"]),
    Rule(rule_id="AIML181", name="feature-metadata-tampering", description="Feature metadata tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Feature metadata update without integrity check - tampering risk", explanation="Feature metadata should be integrity-protected", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "feature-store", "metadata", "tampering"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML182", name="feature-lineage-missing", description="Feature lineage missing", category=RuleCategory.CONVENTION, severity=RuleSeverity.LOW, message_template="Feature creation without lineage tracking - add provenance metadata", explanation="Feature lineage enables security audits and debugging", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-778", owasp_mapping="ML03", tags={"ai", "ml", "feature-store", "lineage", "provenance"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML183", name="feature-access-control-gaps", description="Access control gaps", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Feature access without authorization - add access control", explanation="Feature access should be role-based and authorized", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-862", owasp_mapping="A01", tags={"ai", "ml", "feature-store", "access-control", "authorization"}, references=["https://owasp.org/www-project-top-ten/"]),
    Rule(rule_id="AIML184", name="feature-deletion-corruption", description="Feature deletion/corruption", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Feature deletion without backup - data loss risk", explanation="Feature deletion should include backup or soft delete", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-404", owasp_mapping="ML03", tags={"ai", "ml", "feature-store", "deletion", "backup"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML185", name="feature-version-control-weaknesses", description="Version control weaknesses", category=RuleCategory.CONVENTION, severity=RuleSeverity.LOW, message_template="Feature update without version control - tracking risk", explanation="Feature updates should be versioned for tracking and rollback", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-778", owasp_mapping="ML03", tags={"ai", "ml", "feature-store", "versioning"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML186", name="feature-freshness-attacks", description="Feature freshness attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Feature serving without freshness validation - stale data risk", explanation="Features should have TTL or timestamp validation", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-672", owasp_mapping="ML03", tags={"ai", "ml", "feature-store", "freshness", "ttl"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML187", name="batch-realtime-inconsistencies", description="Batch vs real-time inconsistencies", category=RuleCategory.CONVENTION, severity=RuleSeverity.MEDIUM, message_template="Batch feature processing - validate consistency with real-time", explanation="Batch and real-time processing should produce consistent results", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="ML03", tags={"ai", "ml", "feature-store", "batch", "consistency"}, references=["https://arxiv.org/abs/2004.03045"]),
    Rule(rule_id="AIML188", name="feature-engineering-code-injection", description="Feature engineering code injection", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="Feature transformation with dynamic code - injection risk", explanation="Feature engineering should not use dynamic code execution", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="A03", tags={"ai", "ml", "feature-store", "injection", "code-execution"}, references=["https://owasp.org/www-project-top-ten/"]),
    Rule(rule_id="AIML189", name="schema-evolution-attacks", description="Schema evolution attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Schema evolution without validation - compatibility risk", explanation="Schema changes should be validated for backward compatibility", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "feature-store", "schema", "evolution"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML190", name="feature-importance-manipulation", description="Feature importance manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Feature importance calculation with user input - manipulation risk", explanation="Feature importance should not be influenced by user input", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "feature-store", "importance", "manipulation"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    # Phase 2.2: Model Training Infrastructure (35 checks)
    # Phase 2.2.1: Distributed Training Security (AIML191-AIML205)
    Rule(rule_id="AIML191", name="parameter-server-vulnerabilities", description="Parameter server vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Parameter server - implement authentication and encryption", explanation="Parameter servers need secure communication", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-306", owasp_mapping="ML03", tags={"ai", "ml", "training", "parameter-server", "security"}, references=["https://arxiv.org/abs/1811.12470"]),
    Rule(rule_id="AIML192", name="gradient-aggregation-poisoning", description="Gradient aggregation poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Gradient aggregation - validate to prevent poisoning attacks", explanation="Gradient aggregation can be exploited for model poisoning", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "gradient", "poisoning"}, references=["https://arxiv.org/abs/1811.12470"]),
    Rule(rule_id="AIML193", name="byzantine-worker-attacks", description="Byzantine worker attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Distributed training - add Byzantine worker detection", explanation="Byzantine workers can poison distributed training", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "byzantine", "distributed"}, references=["https://arxiv.org/abs/1811.12470"]),
    Rule(rule_id="AIML194", name="all-reduce-manipulation", description="All-Reduce manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="All-Reduce operation - validate tensor values to prevent manipulation", explanation="All-Reduce operations can be manipulated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "allreduce", "distributed"}, references=["https://pytorch.org/docs/stable/distributed.html"]),
    Rule(rule_id="AIML195", name="ring-all-reduce-injection", description="Ring-All-Reduce injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Ring-All-Reduce - implement secure communication to prevent injection", explanation="Ring-All-Reduce communication should be secured", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "ring-allreduce", "distributed"}, references=["https://arxiv.org/abs/1509.01916"]),
    Rule(rule_id="AIML196", name="horovod-security-gaps", description="Horovod security gaps", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Horovod communication - enable TLS for secure distributed training", explanation="Horovod should use TLS for communication", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-300", owasp_mapping="ML03", tags={"ai", "ml", "training", "horovod", "distributed"}, references=["https://horovod.readthedocs.io/"]),
    Rule(rule_id="AIML197", name="deepspeed-vulnerabilities", description="DeepSpeed vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="DeepSpeed initialization - validate configuration to prevent vulnerabilities", explanation="DeepSpeed configuration should be validated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "training", "deepspeed", "distributed"}, references=["https://www.deepspeed.ai/"]),
    Rule(rule_id="AIML198", name="fsdp-risks", description="FSDP (Fully Sharded Data Parallel) risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="FSDP training - implement shard validation to prevent poisoning", explanation="FSDP shards should be validated for integrity", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "fsdp", "distributed"}, references=["https://pytorch.org/docs/stable/fsdp.html"]),
    Rule(rule_id="AIML199", name="zero-optimizer-state-attacks", description="ZeRO optimizer state attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="ZeRO optimizer state - validate state integrity to prevent attacks", explanation="ZeRO optimizer state can be poisoned", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "zero", "optimizer"}, references=["https://www.deepspeed.ai/tutorials/zero/"]),
    Rule(rule_id="AIML200", name="model-parallel-partition-poisoning", description="Model parallel partition poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model parallel partitioning - validate partition integrity", explanation="Model partitions can be poisoned", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "model-parallel", "distributed"}, references=["https://pytorch.org/docs/stable/distributed.html"]),
    Rule(rule_id="AIML201", name="pipeline-parallel-injection", description="Pipeline parallel injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Pipeline parallel training - validate stage outputs to prevent injection", explanation="Pipeline stages can be exploited for injection", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="ML03", tags={"ai", "ml", "training", "pipeline-parallel", "distributed"}, references=["https://pytorch.org/docs/stable/pipeline.html"]),
    Rule(rule_id="AIML202", name="tensor-parallel-tampering", description="Tensor parallel tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Tensor parallel training - validate tensor splits to prevent tampering", explanation="Tensor splits can be tampered with", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "tensor-parallel", "distributed"}, references=["https://pytorch.org/docs/stable/distributed.html"]),
    Rule(rule_id="AIML203", name="mixed-precision-training-risks", description="Mixed precision training risks", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Mixed precision training - validate numerical stability to prevent attacks", explanation="Mixed precision can introduce numerical vulnerabilities", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="ML03", tags={"ai", "ml", "training", "amp", "mixed-precision"}, references=["https://pytorch.org/docs/stable/amp.html"]),
    Rule(rule_id="AIML204", name="communication-backend-vulnerabilities", description="Communication backend vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Communication backend - use secure backend (e.g., NCCL) for distributed training", explanation="Communication backends should be secure", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-300", owasp_mapping="ML03", tags={"ai", "ml", "training", "backend", "distributed"}, references=["https://pytorch.org/docs/stable/distributed.html"]),
    Rule(rule_id="AIML205", name="collective-operation-manipulation", description="Collective operation manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Collective operation - validate tensor data to prevent manipulation", explanation="Collective operations can be manipulated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "training", "collective", "distributed"}, references=["https://pytorch.org/docs/stable/distributed.html"]),
    # Phase 2.2.2: GPU & Accelerator Security (AIML206-AIML215)
    Rule(rule_id="AIML206", name="gpu-memory-leakage", description="GPU memory leakage", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="GPU memory allocation - ensure proper cleanup to prevent memory leaks", explanation="GPU memory should be properly managed and released", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-401", owasp_mapping="ML03", tags={"ai", "ml", "gpu", "memory", "leak"}, references=["https://pytorch.org/docs/stable/notes/cuda.html"]),
    Rule(rule_id="AIML207", name="cuda-kernel-injection", description="CUDA kernel injection", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="Custom CUDA kernel - validate source to prevent code injection", explanation="Custom CUDA kernels should be validated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="ML03", tags={"ai", "ml", "cuda", "kernel", "injection"}, references=["https://docs.nvidia.com/cuda/cuda-c-programming-guide/"]),
    Rule(rule_id="AIML208", name="rocm-vulnerabilities", description="ROCm vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="ROCm usage - validate kernel code and memory management", explanation="ROCm operations should be validated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "rocm", "amd", "gpu"}, references=["https://rocm.docs.amd.com/"]),
    Rule(rule_id="AIML209", name="tpu-security-gaps", description="TPU security gaps", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="TPU usage - validate computation and memory management", explanation="TPU operations should be validated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "tpu", "xla", "accelerator"}, references=["https://cloud.google.com/tpu/docs"]),
    Rule(rule_id="AIML210", name="npu-ipu-risks", description="NPU/IPU risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="NPU/IPU usage - validate accelerator configuration and operations", explanation="NPU/IPU operations should be validated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "npu", "ipu", "accelerator"}, references=["https://www.graphcore.ai/products/ipu"]),
    Rule(rule_id="AIML211", name="multi-gpu-synchronization-attacks", description="Multi-GPU synchronization attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Multi-GPU training - validate synchronization to prevent attacks", explanation="Multi-GPU synchronization can be exploited", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "gpu", "multi-gpu", "synchronization"}, references=["https://pytorch.org/docs/stable/nn.html#dataparallel"]),
    Rule(rule_id="AIML212", name="device-placement-manipulation", description="Device placement manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Device placement with user input - manipulation risk", explanation="Device placement should not be user-controllable", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "gpu", "device", "placement"}, references=["https://pytorch.org/docs/stable/tensor_attributes.html"]),
    Rule(rule_id="AIML213", name="cuda-graph-poisoning", description="CUDA graph poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="CUDA graph - validate graph structure to prevent poisoning", explanation="CUDA graphs can be poisoned", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "cuda", "graph", "poisoning"}, references=["https://pytorch.org/docs/stable/notes/cuda.html#cuda-graphs"]),
    Rule(rule_id="AIML214", name="kernel-launch-parameter-tampering", description="Kernel launch parameter tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Kernel launch - validate parameters to prevent tampering", explanation="Kernel launch parameters can be manipulated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "cuda", "kernel", "parameters"}, references=["https://docs.nvidia.com/cuda/cuda-c-programming-guide/"]),
    Rule(rule_id="AIML215", name="gpu-memory-exhaustion-attacks", description="GPU memory exhaustion attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="GPU tensor allocation - validate size to prevent memory exhaustion", explanation="Large GPU allocations can cause DoS", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-400", owasp_mapping="ML03", tags={"ai", "ml", "gpu", "memory", "dos"}, references=["https://pytorch.org/docs/stable/notes/cuda.html"]),
    # Phase 2.2.3: Experiment Tracking Security (AIML216-AIML225)
    Rule(rule_id="AIML216", name="mlflow-injection-attacks", description="MLflow injection attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="MLflow logging with user input - injection risk, validate data", explanation="MLflow can be exploited via injection", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="ML03", tags={"ai", "ml", "mlflow", "tracking", "injection"}, references=["https://mlflow.org/docs/latest/security.html"]),
    Rule(rule_id="AIML217", name="wandb-credential-leakage", description="Weights & Biases credential leakage", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="Weights & Biases API key hardcoded - use environment variables", explanation="API keys should not be hardcoded", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-798", owasp_mapping="A07", tags={"ai", "ml", "wandb", "credentials", "hardcoded"}, references=["https://docs.wandb.ai/guides/track/environment-variables"]),
    Rule(rule_id="AIML218", name="cometml-experiment-tampering", description="Comet.ml experiment tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Comet.ml experiment data - validate to prevent tampering", explanation="Experiment data should be validated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "cometml", "tracking", "tampering"}, references=["https://www.comet.com/docs/v2/"]),
    Rule(rule_id="AIML219", name="tensorboard-rce", description="TensorBoard remote code execution", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="TensorBoard logging with user input - RCE risk, sanitize data", explanation="TensorBoard can execute arbitrary code via malicious data", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="ML05", tags={"ai", "ml", "tensorboard", "rce", "injection"}, references=["https://www.tensorflow.org/tensorboard/security"]),
    Rule(rule_id="AIML220", name="neptuneai-model-manipulation", description="Neptune.ai model manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Neptune.ai model logging - validate data to prevent manipulation", explanation="Neptune.ai artifacts should be validated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "neptune", "tracking", "manipulation"}, references=["https://docs.neptune.ai/"]),
    Rule(rule_id="AIML221", name="experiment-metadata-injection", description="Experiment metadata injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Experiment metadata with user input - injection risk, sanitize data", explanation="Experiment metadata should be sanitized", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="ML03", tags={"ai", "ml", "metadata", "tracking", "injection"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML222", name="metric-tampering", description="Metric tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Metric logging - validate values to prevent tampering", explanation="Metrics should be validated for integrity", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "metrics", "tracking", "tampering"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML223", name="artifact-poisoning", description="Artifact poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Artifact upload - validate integrity with checksums to prevent poisoning", explanation="Artifacts should have integrity checks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "artifacts", "tracking", "poisoning"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML224", name="run-comparison-manipulation", description="Run comparison manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Run comparison - validate run IDs to prevent manipulation", explanation="Run comparisons should validate run identities", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "comparison", "tracking", "manipulation"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML225", name="hyperparameter-logging-risks", description="Hyperparameter logging risks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Hyperparameter logging - avoid logging sensitive data (keys, passwords)", explanation="Hyperparameters should not contain sensitive data", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-200", owasp_mapping="A02", tags={"ai", "ml", "hyperparameters", "tracking", "leakage"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    # Phase 2.3: Model Deployment & Serving (35 checks)
    # Phase 2.3.1: Model Serving Vulnerabilities (AIML226-AIML240)
    Rule(rule_id="AIML226", name="torchserve-vulnerabilities", description="TorchServe vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="TorchServe deployment - enable authentication and input validation", explanation="TorchServe endpoints should be properly secured", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-306", owasp_mapping="A01", tags={"ai", "ml", "serving", "torchserve", "security"}, references=["https://pytorch.org/serve/"]),
    Rule(rule_id="AIML227", name="tensorflow-serving-injection", description="TensorFlow Serving injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="TensorFlow Serving - validate inputs to prevent injection attacks", explanation="TensorFlow Serving should validate all inputs", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML09", tags={"ai", "ml", "serving", "tensorflow", "injection"}, references=["https://www.tensorflow.org/tfx/guide/serving"]),
    Rule(rule_id="AIML228", name="onnx-runtime-risks", description="ONNX Runtime risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="ONNX Runtime - validate model integrity before loading", explanation="ONNX models should be integrity-checked", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "serving", "onnx", "security"}, references=["https://onnxruntime.ai/"]),
    Rule(rule_id="AIML229", name="triton-inference-server-gaps", description="Triton Inference Server gaps", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Triton Inference Server - enable access controls and rate limiting", explanation="Triton should have proper access controls", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-862", owasp_mapping="A01", tags={"ai", "ml", "serving", "triton", "security"}, references=["https://github.com/triton-inference-server/server"]),
    Rule(rule_id="AIML230", name="bentoml-security-issues", description="BentoML security issues", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="BentoML deployment - implement authentication and input validation", explanation="BentoML APIs should be secured", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-306", owasp_mapping="A01", tags={"ai", "ml", "serving", "bentoml", "security"}, references=["https://docs.bentoml.org/"]),
    Rule(rule_id="AIML231", name="ray-serve-vulnerabilities", description="Ray Serve vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Ray Serve deployment - validate inputs and enable authentication", explanation="Ray Serve endpoints should be secured", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-306", owasp_mapping="A01", tags={"ai", "ml", "serving", "ray", "security"}, references=["https://docs.ray.io/en/latest/serve/"]),
    Rule(rule_id="AIML232", name="seldon-core-risks", description="Seldon Core risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Seldon Core deployment - implement proper security policies", explanation="Seldon Core deployments should have security policies", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-862", owasp_mapping="A01", tags={"ai", "ml", "serving", "seldon", "security"}, references=["https://docs.seldon.io/projects/seldon-core/"]),
    Rule(rule_id="AIML233", name="kserve-weaknesses", description="KServe weaknesses", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="KServe deployment - enable TLS and authentication", explanation="KServe should use TLS and authentication", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-319", owasp_mapping="A02", tags={"ai", "ml", "serving", "kserve", "security"}, references=["https://kserve.github.io/website/"]),
    Rule(rule_id="AIML234", name="model-batching-attacks", description="Model batching attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model batching - validate batch size to prevent DoS attacks", explanation="Batch size should be limited to prevent resource exhaustion", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-400", owasp_mapping="ML09", tags={"ai", "ml", "serving", "batching", "dos"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML235", name="dynamic-batching-poisoning", description="Dynamic batching poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Dynamic batching - validate inputs to prevent batch poisoning", explanation="Dynamic batching can be exploited for poisoning attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "serving", "batching", "poisoning"}, references=["https://arxiv.org/abs/2004.10941"]),
    Rule(rule_id="AIML236", name="model-versioning-bypass", description="Model versioning bypass", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model serving - pin version to prevent unauthorized updates", explanation="Model versions should be explicitly specified", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "serving", "versioning", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML237", name="ab-testing-manipulation", description="A/B testing manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="A/B testing - validate traffic split to prevent manipulation", explanation="A/B testing traffic should be validated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "serving", "ab-testing", "manipulation"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML238", name="canary-deployment-risks", description="Canary deployment risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Canary deployment - validate rollout percentages and monitoring", explanation="Canary deployments should have proper validation and monitoring", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="ML03", tags={"ai", "ml", "serving", "canary", "deployment"}, references=["https://martinfowler.com/bliki/CanaryRelease.html"]),
    Rule(rule_id="AIML239", name="blue-green-deployment-gaps", description="Blue-green deployment gaps", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Blue-green deployment - validate switchover to prevent issues", explanation="Blue-green deployments should validate before switching", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="ML03", tags={"ai", "ml", "serving", "blue-green", "deployment"}, references=["https://martinfowler.com/bliki/BlueGreenDeployment.html"]),
    Rule(rule_id="AIML240", name="shadow-deployment-leakage", description="Shadow deployment leakage", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Shadow deployment - ensure no data leakage from shadow traffic", explanation="Shadow deployments should not leak data", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-200", owasp_mapping="A02", tags={"ai", "ml", "serving", "shadow", "leakage"}, references=["https://netflixtechblog.com/a-day-in-the-life-of-a-netflix-engineer-automating-security-f3f1f0117e4"]),
    # Phase 2.3.2: API & Endpoint Security (AIML241-AIML252)
    Rule(rule_id="AIML241", name="missing-inference-authentication", description="Missing authentication on inference API", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="Inference API without authentication - add authentication layer", explanation="Inference endpoints must require authentication", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-306", owasp_mapping="A01", tags={"ai", "ml", "api", "authentication", "security"}, references=["https://owasp.org/www-project-top-ten/"]),
    Rule(rule_id="AIML242", name="model-endpoint-enumeration", description="Model endpoint enumeration", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model endpoint exposed - implement access controls to prevent enumeration", explanation="Model endpoints should not be easily enumerable", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-200", owasp_mapping="A01", tags={"ai", "ml", "api", "enumeration", "security"}, references=["https://owasp.org/www-project-top-ten/"]),
    Rule(rule_id="AIML243", name="batch-inference-injection", description="Batch inference injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Batch inference - validate all inputs to prevent injection", explanation="Batch inference should validate inputs individually", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML09", tags={"ai", "ml", "api", "batch", "injection"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML244", name="streaming-inference-attacks", description="Streaming inference attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Streaming inference - validate stream data to prevent attacks", explanation="Streaming inference should validate all stream data", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML09", tags={"ai", "ml", "api", "streaming", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML245", name="model-cache-poisoning", description="Model cache poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Model caching - validate cache integrity to prevent poisoning", explanation="Model cache should be integrity-protected", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML05", tags={"ai", "ml", "api", "cache", "poisoning"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML246", name="prediction-logging-pii-risks", description="Prediction logging risks (PII)", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Prediction logging - avoid logging PII data", explanation="Predictions should not log personally identifiable information", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-359", owasp_mapping="A02", tags={"ai", "ml", "api", "logging", "pii"}, references=["https://owasp.org/www-project-top-ten/"]),
    Rule(rule_id="AIML247", name="model-warmup-vulnerabilities", description="Model warm-up vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Model warm-up - validate warm-up requests to prevent abuse", explanation="Model warm-up should not expose vulnerabilities", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-400", owasp_mapping="ML09", tags={"ai", "ml", "api", "warmup", "security"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML248", name="health-check-information-disclosure", description="Health check information disclosure", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Health check endpoint - limit information disclosure", explanation="Health checks should not reveal sensitive information", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-200", owasp_mapping="A01", tags={"ai", "ml", "api", "health", "disclosure"}, references=["https://owasp.org/www-project-top-ten/"]),
    Rule(rule_id="AIML249", name="metrics-endpoint-exposure", description="Metrics endpoint exposure", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Metrics endpoint - restrict access to prevent information leakage", explanation="Metrics endpoints should be access-controlled", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-200", owasp_mapping="A05", tags={"ai", "ml", "api", "metrics", "exposure"}, references=["https://owasp.org/www-project-top-ten/"]),
    Rule(rule_id="AIML250", name="model-metadata-leakage", description="Model metadata leakage", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model metadata exposed - limit information disclosure", explanation="Model metadata should not be publicly accessible", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-200", owasp_mapping="ML09", tags={"ai", "ml", "api", "metadata", "leakage"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML251", name="feature-flag-manipulation", description="Feature flag manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Feature flags - protect from user manipulation", explanation="Feature flags should not be user-controllable", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-807", owasp_mapping="A01", tags={"ai", "ml", "api", "feature-flags", "manipulation"}, references=["https://owasp.org/www-project-top-ten/"]),
    Rule(rule_id="AIML252", name="circuit-breaker-bypass", description="Circuit breaker bypass", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Circuit breaker - validate implementation to prevent bypass", explanation="Circuit breakers should not be bypassable", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="ML09", tags={"ai", "ml", "api", "circuit-breaker", "bypass"}, references=["https://martinfowler.com/bliki/CircuitBreaker.html"]),
    # Phase 2.3.3: Edge & Mobile Deployment (AIML253-AIML260)
    Rule(rule_id="AIML253", name="tflite-model-tampering", description="TFLite model tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="TFLite model - verify integrity before mobile deployment", explanation="TFLite models should be integrity-protected", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "edge", "tflite", "tampering"}, references=["https://www.tensorflow.org/lite"]),
    Rule(rule_id="AIML254", name="coreml-injection", description="Core ML injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Core ML model - validate model before iOS deployment", explanation="Core ML models should be validated", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "edge", "coreml", "injection"}, references=["https://developer.apple.com/documentation/coreml"]),
    Rule(rule_id="AIML255", name="onnx-mobile-risks", description="ONNX mobile risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="ONNX mobile deployment - verify model integrity", explanation="ONNX models on mobile should be integrity-checked", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "edge", "onnx", "mobile"}, references=["https://onnx.ai/"]),
    Rule(rule_id="AIML256", name="quantized-model-vulnerabilities", description="Quantized model vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model quantization - validate to prevent accuracy degradation attacks", explanation="Quantized models can be exploited through precision attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="ML04", tags={"ai", "ml", "edge", "quantization", "security"}, references=["https://arxiv.org/abs/2004.10708"]),
    Rule(rule_id="AIML257", name="model-pruning-attacks", description="Model pruning attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model pruning - validate to prevent backdoor persistence", explanation="Pruned models can retain hidden backdoors", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-912", owasp_mapping="ML03", tags={"ai", "ml", "edge", "pruning", "backdoor"}, references=["https://arxiv.org/abs/2003.03888"]),
    Rule(rule_id="AIML258", name="knowledge-distillation-risks", description="Knowledge distillation risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Knowledge distillation - validate student model for backdoors", explanation="Distilled models can inherit vulnerabilities from teacher", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-912", owasp_mapping="ML03", tags={"ai", "ml", "edge", "distillation", "backdoor"}, references=["https://arxiv.org/abs/1511.04508"]),
    Rule(rule_id="AIML259", name="on-device-training-weaknesses", description="On-device training weaknesses", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="On-device training - validate training data to prevent poisoning", explanation="On-device training can be poisoned by malicious data", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "edge", "training", "poisoning"}, references=["https://arxiv.org/abs/1811.12470"]),
    Rule(rule_id="AIML260", name="federated-learning-gaps", description="Federated learning gaps", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Federated learning - implement Byzantine-robust aggregation", explanation="Federated learning vulnerable to Byzantine attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "edge", "federated", "byzantine"}, references=["https://arxiv.org/abs/1811.12470"]),
    # Phase 2.4: Model Monitoring & Observability (20 checks)
    # Phase 2.4.1: Drift Detection Security (AIML261-AIML270)
    Rule(rule_id="AIML261", name="data-drift-detection-bypass", description="Data drift detection bypass", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Drift detection - implement comprehensive monitoring to prevent bypass", explanation="Drift detection should monitor all relevant features", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="ML06", tags={"ai", "ml", "monitoring", "drift", "bypass"}, references=["https://arxiv.org/abs/2004.03045"]),
    Rule(rule_id="AIML262", name="concept-drift-manipulation", description="Concept drift manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Concept drift monitoring - validate label distributions to prevent manipulation", explanation="Concept drift can be manipulated through label poisoning", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML06", tags={"ai", "ml", "monitoring", "drift", "manipulation"}, references=["https://arxiv.org/abs/2004.03045"]),
    Rule(rule_id="AIML263", name="model-performance-degradation-hiding", description="Model performance degradation hiding", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Performance monitoring - implement tamper-proof metrics to prevent hiding", explanation="Performance metrics should be tamper-resistant", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML06", tags={"ai", "ml", "monitoring", "performance", "tampering"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML264", name="prediction-distribution-poisoning", description="Prediction distribution poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Prediction monitoring - validate distribution to prevent poisoning", explanation="Prediction distributions can be poisoned", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML06", tags={"ai", "ml", "monitoring", "predictions", "poisoning"}, references=["https://arxiv.org/abs/2004.03045"]),
    Rule(rule_id="AIML265", name="monitoring-pipeline-injection", description="Monitoring pipeline injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Monitoring pipeline - validate data sources to prevent injection", explanation="Monitoring pipelines should validate all data sources", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="ML06", tags={"ai", "ml", "monitoring", "pipeline", "injection"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML266", name="alert-threshold-manipulation", description="Alert threshold manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Alert thresholds - protect from user manipulation", explanation="Alert thresholds should not be user-controllable", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-807", owasp_mapping="ML06", tags={"ai", "ml", "monitoring", "alerts", "manipulation"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML267", name="logging-framework-vulnerabilities", description="Logging framework vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Logging framework - validate logging data to prevent injection", explanation="Logging frameworks should validate all data", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-117", owasp_mapping="A09", tags={"ai", "ml", "monitoring", "logging", "injection"}, references=["https://owasp.org/www-project-top-ten/"]),
    Rule(rule_id="AIML268", name="missing-drift-detection", description="Missing drift detection", category=RuleCategory.CONVENTION, severity=RuleSeverity.MEDIUM, message_template="Production model - implement drift detection for monitoring", explanation="Production models should have drift detection", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="ML06", tags={"ai", "ml", "monitoring", "drift", "best-practice"}, references=["https://arxiv.org/abs/2004.03045"]),
    Rule(rule_id="AIML269", name="statistical-test-manipulation", description="Statistical test manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Statistical tests - validate test parameters to prevent manipulation", explanation="Statistical test parameters should be protected", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML06", tags={"ai", "ml", "monitoring", "statistics", "manipulation"}, references=["https://arxiv.org/abs/2004.03045"]),
    Rule(rule_id="AIML270", name="ground-truth-poisoning", description="Ground truth poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Ground truth collection - validate to prevent poisoning attacks", explanation="Ground truth data should be validated for integrity", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML06", tags={"ai", "ml", "monitoring", "ground-truth", "poisoning"}, references=["https://arxiv.org/abs/1804.00308"]),
    # Phase 2.4.2: Explainability & Interpretability (AIML271-AIML280)
    Rule(rule_id="AIML271", name="shap-value-manipulation", description="SHAP value manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="SHAP explanations - validate to prevent manipulation", explanation="SHAP values can be manipulated to hide biases", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML06", tags={"ai", "ml", "explainability", "shap", "manipulation"}, references=["https://github.com/slundberg/shap"]),
    Rule(rule_id="AIML272", name="lime-explanation-poisoning", description="LIME explanation poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="LIME explanations - validate to prevent poisoning", explanation="LIME explanations can be poisoned", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML06", tags={"ai", "ml", "explainability", "lime", "poisoning"}, references=["https://github.com/marcotcr/lime"]),
    Rule(rule_id="AIML273", name="feature-importance-injection", description="Feature importance injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Feature importance - validate calculations to prevent injection", explanation="Feature importance can be manipulated", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML06", tags={"ai", "ml", "explainability", "importance", "injection"}, references=["https://scikit-learn.org/stable/modules/permutation_importance.html"]),
    Rule(rule_id="AIML274", name="saliency-map-tampering", description="Saliency map tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Saliency maps - validate to prevent tampering", explanation="Saliency maps can be tampered to hide attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML06", tags={"ai", "ml", "explainability", "saliency", "tampering"}, references=["https://arxiv.org/abs/1312.6034"]),
    Rule(rule_id="AIML275", name="attention-weight-manipulation", description="Attention weight manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Attention weights - validate to prevent manipulation", explanation="Attention weights can be manipulated to hide biases", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML06", tags={"ai", "ml", "explainability", "attention", "manipulation"}, references=["https://arxiv.org/abs/1706.03762"]),
    Rule(rule_id="AIML276", name="counterfactual-explanation-attacks", description="Counterfactual explanation attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Counterfactual explanations - validate to prevent adversarial attacks", explanation="Counterfactual explanations can reveal model vulnerabilities", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-200", owasp_mapping="ML06", tags={"ai", "ml", "explainability", "counterfactual", "attacks"}, references=["https://arxiv.org/abs/1711.00399"]),
    Rule(rule_id="AIML277", name="model-card-injection", description="Model card injection", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Model card - sanitize user input to prevent injection", explanation="Model cards should sanitize all user input", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-79", owasp_mapping="A03", tags={"ai", "ml", "explainability", "model-card", "injection"}, references=["https://arxiv.org/abs/1810.03993"]),
    Rule(rule_id="AIML278", name="explanation-dashboard-vulnerabilities", description="Explanation dashboard vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Explanation dashboard - implement authentication and input validation", explanation="Explanation dashboards should be secured", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-306", owasp_mapping="A01", tags={"ai", "ml", "explainability", "dashboard", "security"}, references=["https://owasp.org/www-project-top-ten/"]),
    Rule(rule_id="AIML279", name="fairness-metric-manipulation", description="Fairness metric manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Fairness metrics - protect from manipulation to hide biases", explanation="Fairness metrics should be tamper-resistant", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML06", tags={"ai", "ml", "explainability", "fairness", "manipulation"}, references=["https://arxiv.org/abs/1810.01943"]),
    Rule(rule_id="AIML280", name="bias-detection-bypass", description="Bias detection bypass", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Bias detection - implement comprehensive checks to prevent bypass", explanation="Bias detection should be comprehensive and tamper-proof", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="ML06", tags={"ai", "ml", "explainability", "bias", "bypass"}, references=["https://arxiv.org/abs/1810.01943"]),
    # Phase 3: Specialized AI/ML Frameworks (100 checks)
    # Phase 3.1: Computer Vision Security (35 checks)
    # Phase 3.1.1: Image Processing Vulnerabilities (AIML281-AIML295)
    Rule(rule_id="AIML281", name="opencv-injection-attacks", description="OpenCV injection attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="OpenCV injection - validate file paths and inputs to prevent attacks", explanation="OpenCV operations should validate all inputs for security", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML09", tags={"ai", "ml", "cv", "opencv", "injection"}, references=["https://docs.opencv.org/"]),
    Rule(rule_id="AIML282", name="pillow-buffer-overflows", description="PIL/Pillow buffer overflows", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="PIL/Pillow image loading - validate image format and size to prevent buffer overflows", explanation="Image loading should validate formats and dimensions", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-120", owasp_mapping="ML09", tags={"ai", "ml", "cv", "pillow", "buffer-overflow"}, references=["https://pillow.readthedocs.io/"]),
    Rule(rule_id="AIML283", name="image-augmentation-poisoning", description="Image augmentation poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Image augmentation - validate augmentation parameters to prevent poisoning", explanation="Augmentation should use validated and bounded parameters", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "cv", "augmentation", "poisoning"}, references=["https://arxiv.org/abs/1804.00308"]),
    Rule(rule_id="AIML284", name="exif-metadata-injection", description="EXIF metadata injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="EXIF metadata - sanitize metadata to prevent injection attacks", explanation="EXIF data should be sanitized before use", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-74", owasp_mapping="ML09", tags={"ai", "ml", "cv", "exif", "injection"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML285", name="adversarial-patch-attacks", description="Adversarial patch attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Vision model - implement defenses against adversarial patch attacks", explanation="Vision models should detect adversarial patches", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML04", tags={"ai", "ml", "cv", "adversarial", "patch"}, references=["https://arxiv.org/abs/1712.09665"]),
    Rule(rule_id="AIML286", name="texture-synthesis-manipulation", description="Texture synthesis manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Texture synthesis - validate generated textures to prevent manipulation", explanation="Synthesized textures should be validated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "cv", "texture", "manipulation"}, references=["https://arxiv.org/abs/1505.07376"]),
    Rule(rule_id="AIML287", name="style-transfer-poisoning", description="Style transfer poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Style transfer - validate style inputs to prevent poisoning", explanation="Style transfer should validate style images", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "cv", "style-transfer", "poisoning"}, references=["https://arxiv.org/abs/1508.06576"]),
    Rule(rule_id="AIML288", name="super-resolution-attacks", description="Super-resolution attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Super-resolution - validate inputs to prevent adversarial attacks", explanation="Super-resolution models can be exploited via adversarial inputs", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML04", tags={"ai", "ml", "cv", "super-resolution", "adversarial"}, references=["https://arxiv.org/abs/1501.00092"]),
    Rule(rule_id="AIML289", name="image-segmentation-manipulation", description="Image segmentation manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Segmentation model - implement defenses against manipulation attacks", explanation="Segmentation models should detect manipulated inputs", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML04", tags={"ai", "ml", "cv", "segmentation", "manipulation"}, references=["https://arxiv.org/abs/1704.06857"]),
    Rule(rule_id="AIML290", name="object-detection-bypass", description="Object detection bypass", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Object detection - implement robustness against bypass attacks", explanation="Object detection should be robust to adversarial examples", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML04", tags={"ai", "ml", "cv", "object-detection", "bypass"}, references=["https://arxiv.org/abs/1412.6572"]),
    Rule(rule_id="AIML291", name="facial-recognition-spoofing", description="Facial recognition spoofing", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="Facial recognition - implement liveness detection to prevent spoofing", explanation="Facial recognition should include anti-spoofing measures", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-287", owasp_mapping="A07", tags={"ai", "ml", "cv", "facial-recognition", "spoofing"}, references=["https://arxiv.org/abs/1901.08897"]),
    Rule(rule_id="AIML292", name="ocr-injection-attacks", description="OCR injection attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="OCR processing - sanitize output to prevent injection via manipulated images", explanation="OCR output should be sanitized before use in commands or queries", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-74", owasp_mapping="ML09", tags={"ai", "ml", "cv", "ocr", "injection"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML293", name="image-captioning-poisoning", description="Image captioning poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Image captioning - validate captions to prevent poisoning attacks", explanation="Captioning models should validate generated text", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "cv", "captioning", "poisoning"}, references=["https://arxiv.org/abs/1502.03044"]),
    Rule(rule_id="AIML294", name="visual-question-answering-attacks", description="Visual question answering attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="VQA model - validate questions and answers to prevent attacks", explanation="VQA models should validate both questions and generated answers", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML09", tags={"ai", "ml", "cv", "vqa", "attacks"}, references=["https://arxiv.org/abs/1505.00468"]),
    Rule(rule_id="AIML295", name="video-frame-injection", description="Video frame injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Video processing - validate frames to prevent injection attacks", explanation="Video frames should be validated for integrity", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML09", tags={"ai", "ml", "cv", "video", "injection"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    # Phase 3.1.2: Vision Transformers (AIML296-AIML305)
    Rule(rule_id="AIML296", name="patch-embedding-manipulation", description="Patch embedding manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Vision Transformer patch embedding - validate patches to prevent manipulation", explanation="ViT patch embeddings can be manipulated to fool the model", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML04", tags={"ai", "ml", "cv", "vit", "patch-embedding"}, references=["https://arxiv.org/abs/2010.11929"]),
    Rule(rule_id="AIML297", name="position-encoding-injection", description="Position encoding injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Vision Transformer position encoding - validate to prevent injection", explanation="Position encodings can be injected to manipulate model behavior", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML04", tags={"ai", "ml", "cv", "vit", "position-encoding"}, references=["https://arxiv.org/abs/2010.11929"]),
    Rule(rule_id="AIML298", name="attention-mechanism-attacks", description="Attention mechanism attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Vision Transformer attention - implement defenses against attention manipulation", explanation="Attention mechanisms can be exploited for adversarial attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML04", tags={"ai", "ml", "cv", "vit", "attention"}, references=["https://arxiv.org/abs/2010.11929"]),
    Rule(rule_id="AIML299", name="vision-language-model-risks", description="Vision-language model risks (CLIP)", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="CLIP model - validate both image and text inputs to prevent cross-modal attacks", explanation="Vision-language models vulnerable to cross-modal adversarial examples", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML04", tags={"ai", "ml", "cv", "clip", "multimodal"}, references=["https://arxiv.org/abs/2103.00020"]),
    Rule(rule_id="AIML300", name="diffusion-model-injection", description="Diffusion model injection (Stable Diffusion)", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="Stable Diffusion - sanitize prompts and validate generated images", explanation="Diffusion models can generate harmful content via prompt injection", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-74", owasp_mapping="ML01", tags={"ai", "ml", "cv", "diffusion", "stable-diffusion"}, references=["https://arxiv.org/abs/2112.10752"]),
    Rule(rule_id="AIML301", name="text-to-image-prompt-injection", description="Text-to-image prompt injection", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="Text-to-image generation - sanitize prompts to prevent harmful content", explanation="Text-to-image models vulnerable to prompt injection attacks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-74", owasp_mapping="ML01", tags={"ai", "ml", "cv", "text-to-image", "prompt-injection"}, references=["https://arxiv.org/abs/2112.10752"]),
    Rule(rule_id="AIML302", name="image-to-image-manipulation", description="Image-to-image manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Image-to-image translation - validate inputs to prevent malicious manipulation", explanation="Image translation models can be exploited for deepfakes", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "cv", "image-to-image", "manipulation"}, references=["https://arxiv.org/abs/1703.10593"]),
    Rule(rule_id="AIML303", name="inpainting-attacks", description="Inpainting attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Image inpainting - validate masked regions to prevent attacks", explanation="Inpainting can be exploited to insert malicious content", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "cv", "inpainting", "attacks"}, references=["https://arxiv.org/abs/1801.07892"]),
    Rule(rule_id="AIML304", name="outpainting-vulnerabilities", description="Outpainting vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Image outpainting - validate generated regions for consistency", explanation="Outpainting can generate inconsistent or harmful content", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="ML03", tags={"ai", "ml", "cv", "outpainting", "vulnerabilities"}, references=["https://arxiv.org/abs/1901.07518"]),
    Rule(rule_id="AIML305", name="multimodal-fusion-risks", description="Multimodal fusion risks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Multimodal fusion - validate cross-modal interactions to prevent attacks", explanation="Multimodal models vulnerable to cross-modal adversarial examples", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML04", tags={"ai", "ml", "cv", "multimodal", "fusion"}, references=["https://arxiv.org/abs/2103.00020"]),
    # Phase 3.1.3: CNN & Architecture Security (AIML306-AIML315)
    Rule(rule_id="AIML306", name="resnet-skip-connection-attacks", description="ResNet skip connection attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="ResNet architecture - validate skip connections to prevent gradient manipulation", explanation="ResNet skip connections can be exploited for gradient-based attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML04", tags={"ai", "ml", "cv", "resnet", "skip-connection"}, references=["https://arxiv.org/abs/1512.03385"]),
    Rule(rule_id="AIML307", name="densenet-feature-concatenation", description="DenseNet feature concatenation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="DenseNet architecture - validate feature concatenation for integrity", explanation="DenseNet feature concatenation vulnerable to manipulation", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML04", tags={"ai", "ml", "cv", "densenet", "concatenation"}, references=["https://arxiv.org/abs/1608.06993"]),
    Rule(rule_id="AIML308", name="efficientnet-scaling-manipulation", description="EfficientNet scaling manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="EfficientNet - validate compound scaling parameters to prevent manipulation", explanation="EfficientNet scaling can be exploited for resource exhaustion", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-400", owasp_mapping="ML09", tags={"ai", "ml", "cv", "efficientnet", "scaling"}, references=["https://arxiv.org/abs/1905.11946"]),
    Rule(rule_id="AIML309", name="mobilenet-depthwise-convolution-risks", description="MobileNet depthwise convolution risks", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="MobileNet - validate depthwise convolutions for numerical stability", explanation="Depthwise convolutions can introduce numerical vulnerabilities", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="ML04", tags={"ai", "ml", "cv", "mobilenet", "depthwise"}, references=["https://arxiv.org/abs/1704.04861"]),
    Rule(rule_id="AIML310", name="squeezenet-fire-module-injection", description="SqueezeNet fire module injection", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="SqueezeNet - validate fire module parameters", explanation="Fire modules can be exploited through parameter manipulation", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "cv", "squeezenet", "fire-module"}, references=["https://arxiv.org/abs/1602.07360"]),
    Rule(rule_id="AIML311", name="neural-architecture-search-poisoning", description="Neural architecture search poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="NAS - validate search space and architecture selection to prevent poisoning", explanation="Neural architecture search can be poisoned to select vulnerable architectures", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "cv", "nas", "poisoning"}, references=["https://arxiv.org/abs/1808.05377"]),
    Rule(rule_id="AIML312", name="activation-function-vulnerabilities", description="Activation function vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Custom activation functions - validate for numerical stability and gradient flow", explanation="Custom activations can introduce numerical vulnerabilities", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-754", owasp_mapping="ML03", tags={"ai", "ml", "cv", "activation", "vulnerability"}, references=["https://arxiv.org/abs/1710.05941"]),
    Rule(rule_id="AIML313", name="pooling-layer-manipulation", description="Pooling layer manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Pooling layers - validate pooling parameters to prevent manipulation", explanation="Pooling operations can be exploited for adversarial attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML04", tags={"ai", "ml", "cv", "pooling", "manipulation"}, references=["https://arxiv.org/abs/1412.6806"]),
    Rule(rule_id="AIML314", name="dropout-bypass-techniques", description="Dropout bypass techniques", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Dropout layers - ensure proper training/inference mode to prevent bypass", explanation="Dropout can be bypassed by manipulating training/inference mode", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-754", owasp_mapping="ML03", tags={"ai", "ml", "cv", "dropout", "bypass"}, references=["https://arxiv.org/abs/1207.0580"]),
    Rule(rule_id="AIML315", name="batch-normalization-attacks", description="Batch normalization attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Batch normalization - validate statistics to prevent poisoning attacks", explanation="Batch norm statistics can be poisoned during training", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "cv", "batchnorm", "poisoning"}, references=["https://arxiv.org/abs/1502.03167"]),
    # Phase 3.2: Natural Language Processing Security (35 checks - AIML316-AIML350)
    # Phase 3.2.1: Text Processing Security (AIML316-AIML330)
    Rule(rule_id="AIML316", name="tokenization-injection", description="Tokenization injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Tokenization - validate input text to prevent injection attacks", explanation="Tokenization should validate inputs to prevent malicious token injection", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-74", owasp_mapping="ML01", tags={"ai", "ml", "nlp", "tokenization", "injection"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML317", name="subword-tokenization-bypass", description="Subword tokenization bypass", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Subword tokenization - sanitize inputs to prevent bypass attacks", explanation="Subword tokenization can be bypassed with crafted inputs", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML01", tags={"ai", "ml", "nlp", "subword", "bypass"}, references=["https://aclanthology.org/2020.emnlp-main.463/"]),
    Rule(rule_id="AIML318", name="bpe-manipulation", description="BPE manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="BPE encoding - validate encoding parameters to prevent manipulation", explanation="Byte Pair Encoding can be manipulated through parameter tampering", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "bpe", "manipulation"}, references=["https://arxiv.org/abs/1508.07909"]),
    Rule(rule_id="AIML319", name="wordpiece-attack-vectors", description="WordPiece attack vectors", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="WordPiece tokenization - validate tokens to prevent attack vectors", explanation="WordPiece tokenization vulnerable to adversarial token sequences", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "wordpiece", "attack"}, references=["https://arxiv.org/abs/1609.08144"]),
    Rule(rule_id="AIML320", name="sentencepiece-vulnerabilities", description="SentencePiece vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="SentencePiece - validate model and inputs to prevent vulnerabilities", explanation="SentencePiece models should be validated for integrity", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "sentencepiece", "vulnerability"}, references=["https://arxiv.org/abs/1808.06226"]),
    Rule(rule_id="AIML321", name="text-normalization-bypass", description="Text normalization bypass", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Text normalization - validate Unicode handling to prevent bypass", explanation="Text normalization can be bypassed using Unicode tricks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML01", tags={"ai", "ml", "nlp", "normalization", "bypass"}, references=["https://unicode.org/reports/tr15/"]),
    Rule(rule_id="AIML322", name="stopword-removal-manipulation", description="Stop word removal manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Stop word removal - validate custom stop word lists to prevent manipulation", explanation="Custom stop word lists can be manipulated to affect model behavior", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "stopwords", "manipulation"}, references=["https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html"]),
    Rule(rule_id="AIML323", name="stemming-lemmatization-attacks", description="Stemming/lemmatization attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Stemming/lemmatization - validate models to prevent attacks", explanation="Stemming and lemmatization models can be manipulated", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "stemming", "lemmatization"}, references=["https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"]),
    Rule(rule_id="AIML324", name="ner-injection", description="NER injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="NER - validate inputs to prevent entity injection attacks", explanation="Named Entity Recognition can be manipulated with crafted inputs", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-74", owasp_mapping="ML01", tags={"ai", "ml", "nlp", "ner", "injection"}, references=["https://arxiv.org/abs/2004.05986"]),
    Rule(rule_id="AIML325", name="pos-tagging-manipulation", description="POS tagging manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="POS tagging - validate tags to prevent manipulation", explanation="Part-of-speech tags can be manipulated through adversarial inputs", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "pos", "manipulation"}, references=["https://aclanthology.org/N19-1165/"]),
    Rule(rule_id="AIML326", name="dependency-parsing-poisoning", description="Dependency parsing poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Dependency parsing - validate parse trees to prevent poisoning", explanation="Dependency parsing can be poisoned through crafted sentences", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "dependency", "poisoning"}, references=["https://arxiv.org/abs/1901.10513"]),
    Rule(rule_id="AIML327", name="sentiment-analysis-bias", description="Sentiment analysis bias", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Sentiment analysis - validate for bias and fairness", explanation="Sentiment models should be validated for demographic bias", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="ML06", tags={"ai", "ml", "nlp", "sentiment", "bias"}, references=["https://arxiv.org/abs/1805.04508"]),
    Rule(rule_id="AIML328", name="text-classification-backdoors", description="Text classification backdoors", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Text classification - validate training data to prevent backdoor attacks", explanation="Text classifiers vulnerable to backdoor poisoning", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-912", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "classification", "backdoor"}, references=["https://arxiv.org/abs/1809.00152"]),
    Rule(rule_id="AIML329", name="sequence-labeling-attacks", description="Sequence labeling attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Sequence labeling - validate labels to prevent attacks", explanation="Sequence labeling models can be manipulated through adversarial sequences", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "sequence", "attack"}, references=["https://arxiv.org/abs/1908.05616"]),
    Rule(rule_id="AIML330", name="coreference-resolution-manipulation", description="Coreference resolution manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Coreference resolution - validate resolutions to prevent manipulation", explanation="Coreference resolution can be manipulated to change meaning", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "coreference", "manipulation"}, references=["https://arxiv.org/abs/1906.07045"]),
    # Phase 3.2.2: Transformer Architectures (AIML331-AIML342)
    Rule(rule_id="AIML331", name="bert-finetuning-injection", description="BERT fine-tuning injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="BERT fine-tuning - validate training data to prevent injection attacks", explanation="BERT fine-tuning vulnerable to data poisoning", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "bert", "finetuning"}, references=["https://arxiv.org/abs/1810.04805"]),
    Rule(rule_id="AIML332", name="gpt-prompt-engineering-attacks", description="GPT prompt engineering attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="GPT prompt engineering - sanitize prompts to prevent attacks", explanation="GPT models vulnerable to prompt injection and jailbreaking", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-74", owasp_mapping="ML01", tags={"ai", "ml", "nlp", "gpt", "prompt"}, references=["https://arxiv.org/abs/2302.12173"]),
    Rule(rule_id="AIML333", name="t5-encoder-decoder-manipulation", description="T5 encoder-decoder manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="T5 encoder-decoder - validate inputs to prevent manipulation", explanation="T5 encoder-decoder architecture vulnerable to input manipulation", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "t5", "manipulation"}, references=["https://arxiv.org/abs/1910.10683"]),
    Rule(rule_id="AIML334", name="bart-denoising-poisoning", description="BART denoising poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="BART denoising - validate training data to prevent poisoning", explanation="BART denoising can be poisoned through corrupted training data", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "bart", "poisoning"}, references=["https://arxiv.org/abs/1910.13461"]),
    Rule(rule_id="AIML335", name="roberta-masked-lm", description="RoBERTa masked LM", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="RoBERTa MLM - validate masked tokens to prevent manipulation", explanation="RoBERTa masked language modeling vulnerable to token manipulation", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "roberta", "mlm"}, references=["https://arxiv.org/abs/1907.11692"]),
    Rule(rule_id="AIML336", name="electra-attacks", description="ELECTRA attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="ELECTRA - validate discriminator/generator to prevent attacks", explanation="ELECTRA's discriminator-generator architecture vulnerable to adversarial attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "electra", "attack"}, references=["https://arxiv.org/abs/2003.10555"]),
    Rule(rule_id="AIML337", name="xlnet-permutation-lm", description="XLNet permutation LM", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="XLNet permutation LM - validate permutations to prevent manipulation", explanation="XLNet permutation language modeling can be manipulated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "xlnet", "permutation"}, references=["https://arxiv.org/abs/1906.08237"]),
    Rule(rule_id="AIML338", name="albert-parameter-sharing-risks", description="ALBERT parameter sharing risks", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="ALBERT parameter sharing - validate shared parameters for security", explanation="ALBERT's parameter sharing can propagate vulnerabilities across layers", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "albert", "parameter"}, references=["https://arxiv.org/abs/1909.11942"]),
    Rule(rule_id="AIML339", name="distilbert-knowledge-distillation", description="DistilBERT knowledge distillation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="DistilBERT - validate teacher model to prevent vulnerability inheritance", explanation="DistilBERT can inherit vulnerabilities from teacher model", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "distilbert", "distillation"}, references=["https://arxiv.org/abs/1910.01108"]),
    Rule(rule_id="AIML340", name="deberta-disentangled-attention", description="DeBERTa disentangled attention", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="DeBERTa disentangled attention - validate attention mechanisms", explanation="DeBERTa's disentangled attention vulnerable to manipulation", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML04", tags={"ai", "ml", "nlp", "deberta", "attention"}, references=["https://arxiv.org/abs/2006.03654"]),
    Rule(rule_id="AIML341", name="longformer-sliding-window-attacks", description="Longformer sliding window attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Longformer sliding window - validate window size to prevent attacks", explanation="Longformer's sliding window mechanism vulnerable to resource exhaustion", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-400", owasp_mapping="ML09", tags={"ai", "ml", "nlp", "longformer", "window"}, references=["https://arxiv.org/abs/2004.05150"]),
    Rule(rule_id="AIML342", name="bigbird-sparse-attention-manipulation", description="BigBird sparse attention manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="BigBird sparse attention - validate attention patterns to prevent manipulation", explanation="BigBird's sparse attention patterns can be manipulated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML04", tags={"ai", "ml", "nlp", "bigbird", "attention"}, references=["https://arxiv.org/abs/2007.14062"]),
    # Phase 3.2.3: Embeddings & Representations (AIML343-AIML350)
    Rule(rule_id="AIML343", name="word2vec-poisoning", description="Word2Vec poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Word2Vec - validate training corpus to prevent poisoning attacks", explanation="Word2Vec embeddings can be poisoned through corrupted training data", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "word2vec", "poisoning"}, references=["https://arxiv.org/abs/1301.3781"]),
    Rule(rule_id="AIML344", name="glove-embedding-manipulation", description="GloVe embedding manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="GloVe embeddings - verify integrity before loading to prevent manipulation", explanation="GloVe embeddings should be verified for integrity", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "glove", "manipulation"}, references=["https://nlp.stanford.edu/projects/glove/"]),
    Rule(rule_id="AIML345", name="fasttext-subword-attacks", description="FastText subword attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="FastText subword - validate subword embeddings to prevent attacks", explanation="FastText subword embeddings vulnerable to adversarial subwords", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "fasttext", "subword"}, references=["https://arxiv.org/abs/1607.04606"]),
    Rule(rule_id="AIML346", name="elmo-contextualized-embedding-injection", description="ELMo contextualized embedding injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="ELMo embeddings - validate context to prevent injection attacks", explanation="ELMo contextualized embeddings vulnerable to context injection", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-74", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "elmo", "injection"}, references=["https://arxiv.org/abs/1802.05365"]),
    Rule(rule_id="AIML347", name="sentence-bert-manipulation", description="Sentence-BERT manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Sentence-BERT - validate sentence encodings to prevent manipulation", explanation="Sentence-BERT encodings can be manipulated through adversarial sentences", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "sbert", "manipulation"}, references=["https://arxiv.org/abs/1908.10084"]),
    Rule(rule_id="AIML348", name="universal-sentence-encoder-risks", description="Universal Sentence Encoder risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Universal Sentence Encoder - validate inputs to prevent risks", explanation="Universal Sentence Encoder vulnerable to adversarial inputs", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "use", "risk"}, references=["https://arxiv.org/abs/1803.11175"]),
    Rule(rule_id="AIML349", name="doc2vec-document-poisoning", description="Doc2Vec document poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Doc2Vec - validate document corpus to prevent poisoning attacks", explanation="Doc2Vec embeddings can be poisoned through document injection", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "doc2vec", "poisoning"}, references=["https://arxiv.org/abs/1405.4053"]),
    Rule(rule_id="AIML350", name="graph-embedding-attacks", description="Graph embedding attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Graph embeddings - validate graph structure to prevent attacks", explanation="Graph embeddings vulnerable to graph structure manipulation", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "nlp", "graph", "embedding"}, references=["https://arxiv.org/abs/1709.05584"]),
    
    # Phase 3.3: Reinforcement Learning Security (AIML351-370)
    # Phase 3.3.1: RL Algorithm Vulnerabilities (12 checks)
    Rule(rule_id="AIML351", name="q-learning-poisoning", description="Q-learning poisoning vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Q-learning poisoning risk - validate Q-value updates to prevent manipulation", explanation="Q-learning algorithms vulnerable to poisoning through manipulated Q-value updates", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "rl", "q-learning", "poisoning"}, references=["https://arxiv.org/abs/1906.11852"]),
    Rule(rule_id="AIML352", name="dqn-replay-buffer-manipulation", description="DQN replay buffer manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="DQN replay buffer manipulation risk - validate experiences before adding to buffer", explanation="DQN replay buffers can be poisoned with malicious experiences", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "rl", "dqn", "replay-buffer"}, references=["https://arxiv.org/abs/1906.11852"]),
    Rule(rule_id="AIML353", name="policy-gradient-attacks", description="Policy gradient attack vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Policy gradient attack risk - add gradient clipping to prevent manipulation", explanation="Policy gradient methods vulnerable to adversarial gradient manipulation", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "rl", "policy-gradient", "attack"}, references=["https://arxiv.org/abs/1702.02284"]),
    Rule(rule_id="AIML354", name="actor-critic-tampering", description="Actor-critic tampering vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Actor-critic tampering risk - validate network updates to prevent manipulation", explanation="Actor-critic methods vulnerable to network parameter tampering", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "rl", "actor-critic", "tampering"}, references=["https://arxiv.org/abs/1509.02971"]),
    Rule(rule_id="AIML355", name="ppo-injection", description="PPO injection vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="PPO injection risk - ensure proper probability ratio clipping", explanation="PPO algorithms vulnerable to probability ratio manipulation", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "rl", "ppo", "injection"}, references=["https://arxiv.org/abs/1707.06347"]),
    Rule(rule_id="AIML356", name="a3c-risks", description="A3C asynchronous training risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="A3C risk - validate asynchronous updates to prevent race conditions", explanation="A3C vulnerable to race conditions in asynchronous gradient updates", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-362", owasp_mapping="ML03", tags={"ai", "ml", "rl", "a3c", "async"}, references=["https://arxiv.org/abs/1602.01783"]),
    Rule(rule_id="AIML357", name="ddpg-attacks", description="DDPG attack vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="DDPG attack risk - clip actions to valid bounds to prevent exploitation", explanation="DDPG vulnerable to action space exploitation without proper clipping", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "rl", "ddpg", "attack"}, references=["https://arxiv.org/abs/1509.02971"]),
    Rule(rule_id="AIML358", name="sac-vulnerabilities", description="SAC vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="SAC vulnerability - validate entropy coefficient to prevent exploitation", explanation="SAC vulnerable to entropy coefficient manipulation", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "rl", "sac", "vulnerability"}, references=["https://arxiv.org/abs/1801.01290"]),
    Rule(rule_id="AIML359", name="td3-manipulation", description="TD3 manipulation vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="TD3 manipulation risk - clip target policy noise to prevent attacks", explanation="TD3 vulnerable to target policy noise manipulation", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "rl", "td3", "manipulation"}, references=["https://arxiv.org/abs/1802.09477"]),
    Rule(rule_id="AIML360", name="trpo-bypass", description="TRPO bypass vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="TRPO bypass risk - enforce KL divergence constraint to prevent policy deviation", explanation="TRPO vulnerable to trust region constraint bypass", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "rl", "trpo", "bypass"}, references=["https://arxiv.org/abs/1502.05477"]),
    Rule(rule_id="AIML361", name="reward-function-poisoning", description="Reward function poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Reward function poisoning risk - validate and clip reward values", explanation="Reward functions vulnerable to poisoning through manipulated values", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "rl", "reward", "poisoning"}, references=["https://arxiv.org/abs/1906.11852"]),
    Rule(rule_id="AIML362", name="reward-shaping-attacks", description="Reward shaping attack vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Reward shaping attack risk - bound shaped rewards to prevent exploitation", explanation="Reward shaping vulnerable to exploitation without proper bounds", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "rl", "reward-shaping", "attack"}, references=["https://people.eecs.berkeley.edu/~pabbeel/papers/reward_shaping.pdf"]),
    
    # Phase 3.3.2: RL Environment Security (8 checks)
    Rule(rule_id="AIML363", name="gym-environment-injection", description="OpenAI Gym environment injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Gym environment injection risk - validate environment ID before creation", explanation="Gym environments vulnerable to injection through untrusted environment IDs", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="ML03", tags={"ai", "ml", "rl", "gym", "injection"}, references=["https://github.com/openai/gym"]),
    Rule(rule_id="AIML364", name="gymnasium-api-manipulation", description="Gymnasium API manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Gymnasium API manipulation risk - validate environment kwargs", explanation="Gymnasium API vulnerable to manipulation through unvalidated parameters", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "rl", "gymnasium", "manipulation"}, references=["https://github.com/Farama-Foundation/Gymnasium"]),
    Rule(rule_id="AIML365", name="custom-environment-backdoors", description="Custom environment backdoors", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Custom environment backdoor risk - validate environment behavior", explanation="Custom RL environments vulnerable to backdoor insertion", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-506", owasp_mapping="ML03", tags={"ai", "ml", "rl", "custom-env", "backdoor"}, references=["https://arxiv.org/abs/1906.11852"]),
    Rule(rule_id="AIML366", name="state-space-poisoning", description="State space poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="State space poisoning risk - enforce observation space bounds", explanation="State spaces vulnerable to poisoning without proper bounds", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "rl", "state-space", "poisoning"}, references=["https://arxiv.org/abs/1906.11852"]),
    Rule(rule_id="AIML367", name="action-space-tampering", description="Action space tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Action space tampering risk - validate actions against defined space", explanation="Action spaces vulnerable to tampering without validation", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "rl", "action-space", "tampering"}, references=["https://arxiv.org/abs/1906.11852"]),
    Rule(rule_id="AIML368", name="observation-function-attacks", description="Observation function attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Observation function attack risk - sanitize observations before return", explanation="Observation functions vulnerable to attacks without sanitization", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "rl", "observation", "attack"}, references=["https://arxiv.org/abs/1906.11852"]),
    Rule(rule_id="AIML369", name="reward-function-manipulation", description="Reward function manipulation in environments", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Reward function manipulation risk - clip reward values to prevent gaming", explanation="Environment reward functions vulnerable to manipulation for reward gaming", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "rl", "reward", "manipulation"}, references=["https://arxiv.org/abs/1906.11852"]),
    Rule(rule_id="AIML370", name="multiagent-rl-vulnerabilities", description="Multi-agent RL vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Multi-agent RL vulnerability - implement adversarial robustness checks", explanation="Multi-agent RL systems vulnerable to adversarial agents", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "rl", "multi-agent", "vulnerability"}, references=["https://arxiv.org/abs/1911.10635"]),
    
    # Phase 3.4: Specialized ML Libraries (AIML371-380)
    # Phase 3.4.1: AutoML & Hyperparameter Tuning (5 checks)
    Rule(rule_id="AIML371", name="optuna-trial-manipulation", description="Optuna trial manipulation vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Optuna trial manipulation risk - validate hyperparameter suggestions", explanation="Optuna trials vulnerable to malicious hyperparameter manipulation", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "automl", "optuna", "manipulation"}, references=["https://optuna.org/"]),
    Rule(rule_id="AIML372", name="ray-tune-search-space-poisoning", description="Ray Tune search space poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Ray Tune search space poisoning risk - validate search configuration", explanation="Ray Tune search spaces vulnerable to poisoning attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "automl", "ray-tune", "poisoning"}, references=["https://docs.ray.io/en/latest/tune/index.html"]),
    Rule(rule_id="AIML373", name="hyperopt-objective-injection", description="Hyperopt objective injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Hyperopt objective injection risk - validate search space and objective", explanation="Hyperopt objectives vulnerable to code injection attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="ML03", tags={"ai", "ml", "automl", "hyperopt", "injection"}, references=["http://hyperopt.github.io/hyperopt/"]),
    Rule(rule_id="AIML374", name="autosklearn-pipeline-tampering", description="Auto-sklearn pipeline tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Auto-sklearn pipeline tampering risk - validate generated pipelines", explanation="Auto-sklearn pipelines vulnerable to component tampering", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "automl", "auto-sklearn", "tampering"}, references=["https://automl.github.io/auto-sklearn/"]),
    Rule(rule_id="AIML375", name="autokeras-architecture-injection", description="AutoKeras architecture injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="AutoKeras architecture injection risk - validate generated architectures", explanation="AutoKeras architectures vulnerable to malicious layer injection", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "automl", "autokeras", "injection"}, references=["https://autokeras.com/"]),
    
    # Phase 3.4.2: Graph Neural Networks (5 checks)
    Rule(rule_id="AIML376", name="pytorch-geometric-injection", description="PyTorch Geometric injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="PyTorch Geometric injection risk - validate graph data before processing", explanation="PyTorch Geometric vulnerable to malicious graph data injection", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "gnn", "pytorch-geometric", "injection"}, references=["https://pytorch-geometric.readthedocs.io/"]),
    Rule(rule_id="AIML377", name="dgl-manipulation", description="DGL manipulation vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="DGL manipulation risk - validate graph structure before use", explanation="DGL graphs vulnerable to structure manipulation attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "gnn", "dgl", "manipulation"}, references=["https://www.dgl.ai/"]),
    Rule(rule_id="AIML378", name="graph-structure-poisoning", description="Graph structure poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Graph structure poisoning risk - validate graph topology from untrusted sources", explanation="Graph structures vulnerable to poisoning through malicious topology", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "gnn", "graph", "poisoning"}, references=["https://arxiv.org/abs/1806.02371"]),
    Rule(rule_id="AIML379", name="node-feature-injection", description="Node feature injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Node feature injection risk - sanitize node features before use", explanation="Node features vulnerable to adversarial injection attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "gnn", "node-features", "injection"}, references=["https://arxiv.org/abs/1806.02371"]),
    Rule(rule_id="AIML380", name="message-passing-attacks", description="Message passing attack vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Message passing attack risk - bound message values to prevent overflow", explanation="GNN message passing vulnerable to overflow and poisoning attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "gnn", "message-passing", "attack"}, references=["https://arxiv.org/abs/1806.02371"]),
    
    # Phase 4: AI/ML Supply Chain & Infrastructure Security (AIML381-460)
    # Phase 4.1: Jupyter & Notebook Security (25 checks - AIML381-405)
    # Phase 4.1.1: Notebook Execution Risks (12 checks - AIML381-392)
    Rule(rule_id="AIML381", name="untrusted-notebook-execution", description="Untrusted notebook execution", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="Executing untrusted notebook - validate source and content before execution", explanation="Executing untrusted notebooks can lead to arbitrary code execution", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="A03", tags={"ai", "ml", "jupyter", "notebook", "execution"}, references=["https://jupyter-notebook.readthedocs.io/en/stable/security.html"]),
    Rule(rule_id="AIML382", name="output-cell-code-injection", description="Output cell code injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Output cell may contain executable code - sanitize notebook outputs", explanation="Notebook output cells can contain malicious JavaScript or HTML", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-79", owasp_mapping="A03", tags={"ai", "ml", "jupyter", "notebook", "output-injection"}, references=["https://jupyter-notebook.readthedocs.io/en/stable/security.html"]),
    Rule(rule_id="AIML383", name="matplotlib-backend-exploitation", description="Matplotlib backend exploitation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Matplotlib backend configuration - use safe backend to prevent exploitation", explanation="Matplotlib backends can be exploited in notebook environments", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="ML09", tags={"ai", "ml", "jupyter", "matplotlib", "backend"}, references=["https://matplotlib.org/stable/users/explain/backends.html"]),
    Rule(rule_id="AIML384", name="ipython-magic-injection", description="IPython magic command injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="IPython magic with user input - validate commands to prevent injection", explanation="IPython magic commands with untrusted input enable code execution", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-78", owasp_mapping="A03", tags={"ai", "ml", "jupyter", "ipython", "magic"}, references=["https://ipython.readthedocs.io/en/stable/interactive/magics.html"]),
    Rule(rule_id="AIML385", name="kernel-manipulation-attacks", description="Kernel manipulation attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Jupyter kernel manipulation - validate kernel operations", explanation="Jupyter kernels can be manipulated to execute malicious code", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="A03", tags={"ai", "ml", "jupyter", "kernel", "manipulation"}, references=["https://jupyter-client.readthedocs.io/en/stable/"]),
    Rule(rule_id="AIML386", name="widget-state-poisoning", description="Widget state poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Jupyter widget state - validate state data to prevent poisoning", explanation="Jupyter widget state can be poisoned through untrusted notebooks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "jupyter", "widget", "poisoning"}, references=["https://ipywidgets.readthedocs.io/"]),
    Rule(rule_id="AIML387", name="display-object-injection", description="Display object injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Display object with untrusted data - sanitize before rendering", explanation="Display objects can inject malicious content in notebook outputs", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-79", owasp_mapping="A03", tags={"ai", "ml", "jupyter", "display", "injection"}, references=["https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html"]),
    Rule(rule_id="AIML388", name="markdown-cell-xss", description="Markdown cell XSS", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Markdown cell with user input - sanitize to prevent XSS", explanation="Markdown cells can contain XSS payloads via HTML injection", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-79", owasp_mapping="A03", tags={"ai", "ml", "jupyter", "markdown", "xss"}, references=["https://jupyter-notebook.readthedocs.io/en/stable/security.html"]),
    Rule(rule_id="AIML389", name="latex-injection", description="LaTeX injection in outputs", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="LaTeX rendering with user input - sanitize to prevent injection", explanation="LaTeX in notebook outputs can execute arbitrary commands", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="A03", tags={"ai", "ml", "jupyter", "latex", "injection"}, references=["https://en.wikipedia.org/wiki/LaTeX_injection"]),
    Rule(rule_id="AIML390", name="svg-code-execution", description="SVG code execution", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="SVG output - sanitize to prevent JavaScript execution", explanation="SVG images can contain embedded JavaScript for code execution", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-79", owasp_mapping="A03", tags={"ai", "ml", "jupyter", "svg", "execution"}, references=["https://owasp.org/www-community/attacks/SVG_Embedding"]),
    Rule(rule_id="AIML391", name="html-display-risks", description="HTML display risks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="HTML display with untrusted content - sanitize to prevent injection", explanation="HTML display objects can inject malicious scripts", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-79", owasp_mapping="A03", tags={"ai", "ml", "jupyter", "html", "display"}, references=["https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.HTML"]),
    Rule(rule_id="AIML392", name="javascript-execution-notebooks", description="JavaScript execution in notebooks", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="JavaScript in notebook - avoid or sanitize to prevent malicious execution", explanation="JavaScript execution in notebooks enables XSS and data exfiltration", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-79", owasp_mapping="A03", tags={"ai", "ml", "jupyter", "javascript", "execution"}, references=["https://jupyter-notebook.readthedocs.io/en/stable/security.html"]),
    
    # Phase 4.1.2: Collaboration & Sharing Risks (8 checks - AIML393-400)
    Rule(rule_id="AIML393", name="nbviewer-security-gaps", description="nbviewer security gaps", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="nbviewer sharing - ensure notebooks are safe before public sharing", explanation="nbviewer can execute malicious notebooks if not properly sanitized", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-79", owasp_mapping="A03", tags={"ai", "ml", "jupyter", "nbviewer", "sharing"}, references=["https://nbviewer.jupyter.org/"]),
    Rule(rule_id="AIML394", name="binder-config-injection", description="Binder configuration injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Binder configuration - validate repo2docker config to prevent injection", explanation="Binder configuration files can inject malicious commands", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="A03", tags={"ai", "ml", "jupyter", "binder", "injection"}, references=["https://mybinder.readthedocs.io/en/latest/using/config_files.html"]),
    Rule(rule_id="AIML395", name="jupyterhub-auth-bypass", description="JupyterHub authentication bypass", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="JupyterHub authentication - implement proper access controls", explanation="JupyterHub authentication bypasses can expose sensitive notebooks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-287", owasp_mapping="A07", tags={"ai", "ml", "jupyter", "jupyterhub", "auth"}, references=["https://jupyterhub.readthedocs.io/en/stable/reference/authenticators.html"]),
    Rule(rule_id="AIML396", name="notebook-credential-leakage", description="Notebook sharing credential leakage", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="Notebook sharing - check for hardcoded credentials before sharing", explanation="Shared notebooks often contain leaked credentials and API keys", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-798", owasp_mapping="A07", tags={"ai", "ml", "jupyter", "credentials", "sharing"}, references=["https://jupyter-notebook.readthedocs.io/en/stable/security.html"]),
    Rule(rule_id="AIML397", name="git-ipynb-risks", description="Git integration risks (.ipynb in repos)", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Notebook in git repository - review outputs and metadata for sensitive data", explanation="Notebooks in git repos often contain leaked secrets in outputs/metadata", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-312", owasp_mapping="A02", tags={"ai", "ml", "jupyter", "git", "metadata"}, references=["https://docs.github.com/en/code-security/secret-scanning"]),
    Rule(rule_id="AIML398", name="colab-sharing-vulnerabilities", description="Colab notebook sharing vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Google Colab sharing - validate permissions and clean sensitive data", explanation="Colab notebooks may expose sensitive data through sharing links", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-200", owasp_mapping="A01", tags={"ai", "ml", "jupyter", "colab", "sharing"}, references=["https://research.google.com/colaboratory/faq.html"]),
    Rule(rule_id="AIML399", name="kaggle-notebook-injection", description="Kaggle notebook injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Kaggle notebook - validate external code before execution", explanation="Kaggle notebooks can contain malicious code from competitions", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="A03", tags={"ai", "ml", "jupyter", "kaggle", "injection"}, references=["https://www.kaggle.com/code"]),
    Rule(rule_id="AIML400", name="paperspace-gradient-risks", description="Paperspace Gradient risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Paperspace Gradient - validate notebook environment and dependencies", explanation="Paperspace notebooks may have insecure default configurations", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-1188", owasp_mapping="A05", tags={"ai", "ml", "jupyter", "paperspace", "config"}, references=["https://docs.paperspace.com/gradient/"]),
    
    # Phase 4.1.3: ML-Specific Notebook Risks (5 checks - AIML401-405)
    Rule(rule_id="AIML401", name="model-checkpoints-notebooks", description="Model checkpoints in notebooks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model checkpoint in notebook - extract to separate file and use secure loading", explanation="Model checkpoints in notebooks create large files and security risks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-502", owasp_mapping="ML05", tags={"ai", "ml", "jupyter", "checkpoint", "model"}, references=["https://pytorch.org/tutorials/beginner/saving_loading_models.html"]),
    Rule(rule_id="AIML402", name="credentials-notebook-metadata", description="Credentials in notebook metadata", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="Credentials in notebook metadata - remove before committing", explanation="Notebook metadata can store credentials that persist in version control", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-798", owasp_mapping="A07", tags={"ai", "ml", "jupyter", "metadata", "credentials"}, references=["https://nbformat.readthedocs.io/en/latest/format_description.html"]),
    Rule(rule_id="AIML403", name="large-model-loading-dos", description="Large model loading (DoS)", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Large model loading - implement memory limits to prevent DoS", explanation="Loading large models in notebooks can cause memory exhaustion", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-400", owasp_mapping="ML09", tags={"ai", "ml", "jupyter", "model", "dos"}, references=["https://pytorch.org/docs/stable/notes/large_scale_deployments.html"]),
    Rule(rule_id="AIML404", name="gpu-memory-exhaustion-notebooks", description="GPU memory exhaustion in notebooks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="GPU operations - implement memory limits and cleanup", explanation="GPU memory exhaustion in notebooks can cause kernel crashes", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-400", owasp_mapping="ML09", tags={"ai", "ml", "jupyter", "gpu", "memory"}, references=["https://pytorch.org/docs/stable/notes/cuda.html#best-practices"]),
    Rule(rule_id="AIML405", name="dataset-download-vulnerabilities", description="Dataset downloading vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Dataset download - validate source and verify integrity", explanation="Downloading datasets in notebooks from untrusted sources risks data poisoning", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML03", tags={"ai", "ml", "jupyter", "dataset", "download"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    
    # Phase 4.2: Dataset & Data Pipeline Security (25 checks - AIML406-430)
    # Phase 4.2.1: Dataset Repositories (10 checks - AIML406-415)
    Rule(rule_id="AIML406", name="huggingface-datasets-poisoning", description="Hugging Face Datasets poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Hugging Face dataset - verify source and integrity before use", explanation="Hugging Face datasets from untrusted sources may contain poisoned data", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML03", tags={"ai", "ml", "dataset", "huggingface", "poisoning"}, references=["https://huggingface.co/docs/datasets/security"]),
    Rule(rule_id="AIML407", name="kaggle-dataset-injection", description="Kaggle dataset injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Kaggle dataset - validate data integrity and source", explanation="Kaggle datasets may contain malicious or poisoned data", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML03", tags={"ai", "ml", "dataset", "kaggle", "injection"}, references=["https://www.kaggle.com/datasets"]),
    Rule(rule_id="AIML408", name="tensorflow-datasets-manipulation", description="TensorFlow Datasets manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="TensorFlow dataset - verify checksum and source integrity", explanation="TensorFlow datasets should be verified for integrity", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML03", tags={"ai", "ml", "dataset", "tensorflow", "manipulation"}, references=["https://www.tensorflow.org/datasets/overview"]),
    Rule(rule_id="AIML409", name="torchvision-datasets-tampering", description="Torchvision datasets tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Torchvision dataset - validate dataset source and checksums", explanation="Torchvision datasets should be verified for tampering", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML03", tags={"ai", "ml", "dataset", "torchvision", "tampering"}, references=["https://pytorch.org/vision/stable/datasets.html"]),
    Rule(rule_id="AIML410", name="paperswithcode-dataset-risks", description="Papers with Code dataset risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Papers with Code dataset - verify source and data integrity", explanation="Research datasets may lack security verification", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-494", owasp_mapping="ML03", tags={"ai", "ml", "dataset", "paperswithcode", "risk"}, references=["https://paperswithcode.com/datasets"]),
    Rule(rule_id="AIML411", name="google-dataset-search-injection", description="Google Dataset Search injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Google Dataset Search - validate dataset source and provenance", explanation="Datasets from search results should be verified", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-494", owasp_mapping="ML03", tags={"ai", "ml", "dataset", "google", "search"}, references=["https://datasetsearch.research.google.com/"]),
    Rule(rule_id="AIML412", name="uci-ml-repository-vulnerabilities", description="UCI ML Repository vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="UCI ML Repository dataset - validate data format and integrity", explanation="Legacy datasets may have integrity issues", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-494", owasp_mapping="ML03", tags={"ai", "ml", "dataset", "uci", "repository"}, references=["https://archive.ics.uci.edu/ml/"]),
    Rule(rule_id="AIML413", name="common-crawl-poisoning", description="Common Crawl poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Common Crawl data - implement filtering and validation", explanation="Common Crawl data contains untrusted web content", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "dataset", "common-crawl", "poisoning"}, references=["https://commoncrawl.org/"]),
    Rule(rule_id="AIML414", name="imagenet-distribution-attacks", description="ImageNet distribution attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="ImageNet dataset - verify official source and checksums", explanation="Unofficial ImageNet distributions may be tampered", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML03", tags={"ai", "ml", "dataset", "imagenet", "attack"}, references=["https://image-net.org/"]),
    Rule(rule_id="AIML415", name="custom-dataset-loader-risks", description="Custom dataset loader risks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Custom dataset loader - validate data source and implement integrity checks", explanation="Custom loaders may bypass security checks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "dataset", "custom", "loader"}, references=["https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"]),
    
    # Phase 4.2.2: Data Loading & Preprocessing (10 checks - AIML416-425)
    Rule(rule_id="AIML416", name="pytorch-dataloader-injection", description="PyTorch DataLoader injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="PyTorch DataLoader - validate data source and worker processes", explanation="DataLoader with untrusted data sources can execute arbitrary code", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="ML03", tags={"ai", "ml", "pytorch", "dataloader", "injection"}, references=["https://pytorch.org/docs/stable/data.html"]),
    Rule(rule_id="AIML417", name="tensorflow-tfdata-manipulation", description="TensorFlow tf.data manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="TensorFlow tf.data - validate pipeline operations and data sources", explanation="tf.data pipelines can be manipulated through untrusted operations", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "tensorflow", "tfdata", "manipulation"}, references=["https://www.tensorflow.org/guide/data"]),
    Rule(rule_id="AIML418", name="pandas-data-loading-risks", description="Pandas data loading risks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Pandas read operations - validate file paths and disable dangerous features", explanation="Pandas can execute arbitrary code through pickle and eval", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-502", owasp_mapping="A08", tags={"ai", "ml", "pandas", "loading", "risk"}, references=["https://pandas.pydata.org/docs/user_guide/io.html"]),
    Rule(rule_id="AIML419", name="numpy-data-loading-vulnerabilities", description="NumPy data loading vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="NumPy load operations - use allow_pickle=False for untrusted data", explanation="NumPy can execute arbitrary code through pickle deserialization", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-502", owasp_mapping="A08", tags={"ai", "ml", "numpy", "loading", "vulnerability"}, references=["https://numpy.org/doc/stable/reference/generated/numpy.load.html"]),
    Rule(rule_id="AIML420", name="h5py-file-injection", description="H5PY file injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="H5PY file operations - validate file source and structure", explanation="HDF5 files can contain malicious data structures", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "h5py", "hdf5", "injection"}, references=["https://docs.h5py.org/en/stable/"]),
    Rule(rule_id="AIML421", name="zarr-array-poisoning", description="Zarr array poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Zarr array operations - validate array metadata and sources", explanation="Zarr arrays can be poisoned through metadata manipulation", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "zarr", "array", "poisoning"}, references=["https://zarr.readthedocs.io/"]),
    Rule(rule_id="AIML422", name="parquet-file-tampering", description="Parquet file tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Parquet file operations - validate file integrity and metadata", explanation="Parquet files can be tampered to inject malicious data", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "parquet", "tampering", "file"}, references=["https://arrow.apache.org/docs/python/parquet.html"]),
    Rule(rule_id="AIML423", name="arrow-data-format-risks", description="Arrow data format risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Apache Arrow operations - validate data sources and schema", explanation="Arrow format can contain malicious schema definitions", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "arrow", "format", "risk"}, references=["https://arrow.apache.org/docs/python/"]),
    Rule(rule_id="AIML424", name="augmentation-library-vulnerabilities", description="Data augmentation library vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Data augmentation - validate augmentation parameters and operations", explanation="Augmentation operations can be exploited for adversarial attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "augmentation", "vulnerability", "data"}, references=["https://arxiv.org/abs/1912.02781"]),
    Rule(rule_id="AIML425", name="albumentations-injection", description="Albumentations injection", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Albumentations transforms - validate transform parameters", explanation="Albumentations transforms can be manipulated for adversarial purposes", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "ml", "albumentations", "injection", "transform"}, references=["https://albumentations.ai/"]),
    
    # Phase 4.2.3: Data Versioning & Tracking (5 checks - AIML426-430)
    Rule(rule_id="AIML426", name="dvc-injection", description="DVC (Data Version Control) injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="DVC operations - validate remote storage and pipeline configurations", explanation="DVC can be exploited through malicious remote storage or pipeline configs", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="ML03", tags={"ai", "ml", "dvc", "version-control", "injection"}, references=["https://dvc.org/doc/user-guide/security-and-privacy"]),
    Rule(rule_id="AIML427", name="lakefs-tampering", description="LakeFS tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="LakeFS operations - implement access controls and audit logging", explanation="LakeFS data lake can be tampered without proper access controls", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-285", owasp_mapping="A01", tags={"ai", "ml", "lakefs", "tampering", "data-lake"}, references=["https://docs.lakefs.io/"]),
    Rule(rule_id="AIML428", name="delta-lake-poisoning", description="Delta Lake poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Delta Lake operations - validate transaction logs and data integrity", explanation="Delta Lake transaction logs can be poisoned", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "delta-lake", "poisoning", "transaction"}, references=["https://docs.delta.io/latest/delta-intro.html"]),
    Rule(rule_id="AIML429", name="iceberg-table-manipulation", description="Iceberg table manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Iceberg table operations - validate metadata and snapshots", explanation="Iceberg table metadata can be manipulated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "iceberg", "manipulation", "table"}, references=["https://iceberg.apache.org/"]),
    Rule(rule_id="AIML430", name="hudi-versioning-risks", description="Hudi data versioning risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Hudi operations - validate timeline and commit integrity", explanation="Hudi timeline and commits can be manipulated", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "hudi", "versioning", "risk"}, references=["https://hudi.apache.org/"]),
    
    # Phase 4.3: Model Registry & Versioning Security (20 checks - AIML431-450)
    # Phase 4.3.1: Model Registry Security (12 checks - AIML431-442)
    Rule(rule_id="AIML431", name="mlflow-registry-injection", description="MLflow Model Registry injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="MLflow Model Registry - validate model source and metadata to prevent injection", explanation="MLflow registry operations can be exploited through malicious model metadata", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="ML03", tags={"ai", "ml", "mlflow", "registry", "injection"}, references=["https://mlflow.org/docs/latest/model-registry.html"]),
    Rule(rule_id="AIML432", name="wandb-artifact-tampering", description="Weights & Biases artifact tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Weights & Biases artifact - verify integrity to prevent tampering", explanation="W&B artifacts should be verified for integrity before use", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "wandb", "artifact", "tampering"}, references=["https://docs.wandb.ai/guides/artifacts"]),
    Rule(rule_id="AIML433", name="neptune-model-poisoning", description="Neptune.ai model poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Neptune.ai model operations - validate model integrity to prevent poisoning", explanation="Neptune.ai models should be validated for integrity", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "neptune", "poisoning", "model"}, references=["https://docs.neptune.ai/"]),
    Rule(rule_id="AIML434", name="comet-registry-manipulation", description="Comet.ml registry manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Comet.ml registry - validate model registration to prevent manipulation", explanation="Comet.ml registry can be manipulated without proper validation", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "comet", "registry", "manipulation"}, references=["https://www.comet.com/docs/v2/guides/model-registry/"]),
    Rule(rule_id="AIML435", name="sagemaker-registry-risks", description="AWS SageMaker model registry risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="AWS SageMaker model registry - implement approval workflows and validation", explanation="SageMaker registry requires proper approval and validation workflows", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-285", owasp_mapping="ML03", tags={"ai", "ml", "sagemaker", "registry", "risk"}, references=["https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html"]),
    Rule(rule_id="AIML436", name="azure-ml-registry-gaps", description="Azure ML model registry gaps", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Azure ML model registry - implement validation and approval workflows", explanation="Azure ML registry needs proper validation and approval processes", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-285", owasp_mapping="ML03", tags={"ai", "ml", "azure", "registry", "gap"}, references=["https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-management-and-deployment"]),
    Rule(rule_id="AIML437", name="vertex-ai-registry", description="Google Vertex AI model registry", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Google Vertex AI model registry - validate models before deployment", explanation="Vertex AI models should be validated before deployment", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-285", owasp_mapping="ML03", tags={"ai", "ml", "vertex", "registry", "google"}, references=["https://cloud.google.com/vertex-ai/docs/model-registry/introduction"]),
    Rule(rule_id="AIML438", name="custom-registry-vulnerabilities", description="Custom registry vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Custom model registry - implement validation and integrity checks", explanation="Custom registries may lack security controls", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-494", owasp_mapping="ML03", tags={"ai", "ml", "custom", "registry", "vulnerability"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML439", name="model-metadata-injection", description="Model metadata injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model metadata operations - sanitize inputs to prevent injection", explanation="Model metadata can be exploited for injection attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-94", owasp_mapping="ML03", tags={"ai", "ml", "metadata", "injection", "model"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML440", name="model-versioning-bypass", description="Model versioning bypass", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model versioning - avoid 'latest' tag, use specific version for reproducibility", explanation="Using 'latest' tag creates security and reproducibility risks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML03", tags={"ai", "ml", "versioning", "bypass", "model"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML441", name="model-lineage-tampering", description="Model lineage tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model lineage operations - implement audit logging to prevent tampering", explanation="Model lineage should be protected with audit logging", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "lineage", "tampering", "model"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML442", name="model-approval-workflow-gaps", description="Model approval workflow gaps", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model deployment - implement approval workflow before production", explanation="Production deployments should require approval workflows", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-285", owasp_mapping="ML03", tags={"ai", "ml", "approval", "workflow", "model"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    
    # Phase 4.3.2: Model Packaging & Distribution (8 checks - AIML443-450)
    Rule(rule_id="AIML443", name="docker-image-model-injection", description="Docker image model injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Docker image with model - scan for vulnerabilities before deployment", explanation="Docker images containing models should be scanned for vulnerabilities", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML03", tags={"ai", "ml", "docker", "injection", "model"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML444", name="maas-risks", description="Model as a service (MaaS) risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model serving endpoint - implement authentication and rate limiting", explanation="Model APIs should have authentication and rate limiting", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-306", owasp_mapping="ML09", tags={"ai", "ml", "maas", "serving", "risk"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML445", name="model-marketplace-vulnerabilities", description="Model marketplace vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Model marketplace download - verify model source and integrity", explanation="Models from marketplaces should be verified before use", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-494", owasp_mapping="ML05", tags={"ai", "ml", "marketplace", "vulnerability", "model"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML446", name="model-license-bypass", description="Model license bypass", category=RuleCategory.CONVENTION, severity=RuleSeverity.MEDIUM, message_template="Model license - ensure compliance with licensing terms", explanation="Model usage should comply with license terms", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-1059", owasp_mapping="ML03", tags={"ai", "ml", "license", "bypass", "model"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    Rule(rule_id="AIML447", name="model-watermark-removal", description="Model watermark removal", category=RuleCategory.CONVENTION, severity=RuleSeverity.MEDIUM, message_template="Model watermark removal - respect model authorship and provenance", explanation="Model watermarks should be preserved to maintain provenance", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "watermark", "removal", "model"}, references=["https://arxiv.org/abs/1701.04082"]),
    Rule(rule_id="AIML448", name="model-fingerprinting-attacks", description="Model fingerprinting attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model extraction risk - implement query limits to prevent fingerprinting", explanation="Model APIs should limit queries to prevent extraction attacks", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-200", owasp_mapping="ML07", tags={"ai", "ml", "fingerprinting", "attack", "model"}, references=["https://arxiv.org/abs/1609.02943"]),
    Rule(rule_id="AIML449", name="model-compression-tampering", description="Model compression tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model compression - validate model integrity after compression", explanation="Compressed models should be validated for integrity", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "compression", "tampering", "model"}, references=["https://arxiv.org/abs/1710.09282"]),
    Rule(rule_id="AIML450", name="model-conversion-vulnerabilities", description="Model conversion vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Model conversion - validate converted model for equivalence and security", explanation="Model conversions should be validated for equivalence", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "conversion", "vulnerability", "model"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    
    # Phase 4.4: Cloud & Infrastructure Security (10 checks - AIML451-460)
    # Phase 4.4.1: Cloud ML Services (10 checks - AIML451-460)
    Rule(rule_id="AIML451", name="sagemaker-notebook-injection", description="AWS SageMaker notebook injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="AWS SageMaker notebook - disable root access and direct internet access", explanation="SageMaker notebooks should have root access disabled and no direct internet access", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="ML09", tags={"ai", "ml", "sagemaker", "aws", "notebook"}, references=["https://docs.aws.amazon.com/sagemaker/latest/dg/nbi-root-access.html"]),
    Rule(rule_id="AIML452", name="azure-ml-workspace-tampering", description="Azure ML workspace tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Azure ML workspace - implement proper authentication and RBAC", explanation="Azure ML workspaces should use proper authentication and role-based access control", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-287", owasp_mapping="ML03", tags={"ai", "ml", "azure", "workspace", "tampering"}, references=["https://learn.microsoft.com/en-us/azure/machine-learning/concept-workspace-security"]),
    Rule(rule_id="AIML453", name="vertex-ai-pipeline-manipulation", description="Google Vertex AI pipeline manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Google Vertex AI pipeline - configure service account and encryption", explanation="Vertex AI pipelines should use dedicated service accounts and encryption", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-311", owasp_mapping="ML03", tags={"ai", "ml", "vertex", "google", "pipeline"}, references=["https://cloud.google.com/vertex-ai/docs/pipelines/configure-project"]),
    Rule(rule_id="AIML454", name="databricks-ml-runtime-risks", description="Databricks ML runtime risks", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="Databricks ML runtime - use secrets API instead of hardcoded tokens", explanation="Databricks should use secrets API for credentials, not hardcoded tokens", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-798", owasp_mapping="A07", tags={"ai", "ml", "databricks", "secrets", "token"}, references=["https://docs.databricks.com/security/secrets/index.html"]),
    Rule(rule_id="AIML455", name="snowflake-ml-vulnerabilities", description="Snowflake ML vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Snowflake ML - validate model before registration", explanation="Snowflake ML models should be validated before registration", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "ml", "snowflake", "model", "validation"}, references=["https://docs.snowflake.com/en/developer-guide/snowpark-ml/index"]),
    Rule(rule_id="AIML456", name="bigquery-ml-injection", description="BigQuery ML injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="BigQuery ML - use parameterized queries to prevent SQL injection", explanation="BigQuery ML operations should use parameterized queries", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-89", owasp_mapping="A03", tags={"ai", "ml", "bigquery", "sql", "injection"}, references=["https://cloud.google.com/bigquery/docs/parameterized-queries"]),
    Rule(rule_id="AIML457", name="redshift-ml-tampering", description="Redshift ML tampering", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Redshift ML - use parameterized queries to prevent SQL injection", explanation="Redshift ML operations should use parameterized queries", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-89", owasp_mapping="A03", tags={"ai", "ml", "redshift", "sql", "tampering"}, references=["https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_MODEL.html"]),
    Rule(rule_id="AIML458", name="lambda-ml-inference-risks", description="Lambda ML inference risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Lambda ML inference - configure timeout, memory limits, and VPC", explanation="Lambda ML functions should have proper resource limits and VPC configuration", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-400", owasp_mapping="ML09", tags={"ai", "ml", "lambda", "aws", "inference"}, references=["https://docs.aws.amazon.com/lambda/latest/dg/lambda-security.html"]),
    Rule(rule_id="AIML459", name="cloud-functions-ml-serving-gaps", description="Cloud Functions ML serving gaps", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Cloud Functions ML serving - implement authentication and IAM", explanation="Cloud Functions ML serving should use proper authentication and IAM", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-306", owasp_mapping="A07", tags={"ai", "ml", "cloud-functions", "google", "serving"}, references=["https://cloud.google.com/functions/docs/securing"]),
    Rule(rule_id="AIML460", name="serverless-ml-vulnerabilities", description="Serverless ML vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Serverless ML - implement rate limiting and resource quotas", explanation="Serverless ML deployments should have rate limiting and resource quotas", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-770", owasp_mapping="ML09", tags={"ai", "ml", "serverless", "faas", "rate-limit"}, references=["https://owasp.org/www-project-machine-learning-security-top-10/"]),
    # Phase 5.1: Generative AI Security (20 rules - AIML461-480)
    # Phase 5.1.1: Text Generation Security (10 rules - AIML461-470)
    Rule(rule_id="AIML461", name="prompt-leaking-attacks", description="Prompt leaking attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="LLM prompt leaking - implement prompt protection to prevent system prompt extraction", explanation="LLM applications should protect system prompts from being leaked through carefully crafted user inputs", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-200", owasp_mapping="LLM01", tags={"ai", "llm", "prompt", "leaking", "extraction"}, references=["https://owasp.org/www-project-top-10-for-large-language-model-applications/", "https://arxiv.org/abs/2306.05499"]),
    Rule(rule_id="AIML462", name="training-data-extraction", description="Training data extraction", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="LLM training data extraction - implement output filtering to prevent training data leakage", explanation="LLM applications should filter outputs to prevent extraction of memorized training data", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-200", owasp_mapping="LLM06", tags={"ai", "llm", "training", "extraction", "privacy"}, references=["https://arxiv.org/abs/2012.07805", "https://owasp.org/www-project-top-10-for-large-language-model-applications/"]),
    Rule(rule_id="AIML463", name="memorization-exploitation", description="Memorization exploitation", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="LLM memorization exploitation - implement output validation to prevent memorized content extraction", explanation="LLM applications should validate outputs to prevent exploitation of memorized training data", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-200", owasp_mapping="LLM06", tags={"ai", "llm", "memorization", "exploitation", "privacy"}, references=["https://arxiv.org/abs/2202.07646", "https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extraction"]),
    Rule(rule_id="AIML464", name="copyright-infringement", description="Copyright infringement detection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="LLM copyright infringement - implement content filtering to prevent copyrighted content generation", explanation="LLM applications should filter outputs to prevent generation of copyrighted or licensed content", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-200", owasp_mapping="LLM09", tags={"ai", "llm", "copyright", "legal", "compliance"}, references=["https://arxiv.org/abs/2305.15728", "https://openai.com/blog/chatgpt-plugins"]),
    Rule(rule_id="AIML465", name="generated-code-security", description="Generated code security (Copilot-style)", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="LLM code generation - validate and sanitize generated code before execution", explanation="LLM-generated code should be validated for security vulnerabilities before execution", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="LLM08", tags={"ai", "llm", "code-generation", "security", "copilot"}, references=["https://arxiv.org/abs/2108.09293", "https://github.blog/2023-04-17-new-code-scanning-rules-for-github-copilot/"]),
    Rule(rule_id="AIML466", name="jailbreak-detection", description="Jailbreak detection", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="LLM jailbreak - implement input filtering to prevent jailbreak attempts", explanation="LLM applications should detect and block jailbreak attempts that bypass safety guardrails", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-863", owasp_mapping="LLM01", tags={"ai", "llm", "jailbreak", "safety", "guardrails"}, references=["https://arxiv.org/abs/2308.03825", "https://www.jailbreakchat.com/"]),
    Rule(rule_id="AIML467", name="toxicity-generation", description="Toxicity generation risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="LLM toxicity generation - implement content moderation to prevent toxic output", explanation="LLM applications should use content moderation APIs to prevent generation of toxic or harmful content", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-1284", owasp_mapping="LLM09", tags={"ai", "llm", "toxicity", "safety", "moderation"}, references=["https://platform.openai.com/docs/guides/moderation", "https://www.anthropic.com/product"]),
    Rule(rule_id="AIML468", name="bias-amplification", description="Bias amplification detection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="LLM bias amplification - implement bias detection and mitigation strategies", explanation="LLM applications should monitor and mitigate bias amplification in generated content", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-1229", owasp_mapping="LLM09", tags={"ai", "llm", "bias", "fairness", "ethics"}, references=["https://arxiv.org/abs/2203.11933", "https://www.nist.gov/itl/ai-risk-management-framework"]),
    Rule(rule_id="AIML469", name="hallucination-exploitation", description="Hallucination exploitation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="LLM hallucination exploitation - implement fact-checking and source verification", explanation="LLM applications should validate factual claims and provide source verification to prevent hallucination exploitation", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="LLM09", tags={"ai", "llm", "hallucination", "verification", "accuracy"}, references=["https://arxiv.org/abs/2305.14552", "https://www.anthropic.com/index/core-views-on-ai-safety"]),
    Rule(rule_id="AIML470", name="output-filtering-bypass", description="Output filtering bypass", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="LLM output filtering - implement robust content filtering that cannot be bypassed", explanation="LLM output filtering should use defense-in-depth approach to prevent bypass attempts", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-116", owasp_mapping="LLM01", tags={"ai", "llm", "filtering", "bypass", "security"}, references=["https://arxiv.org/abs/2308.03825", "https://owasp.org/www-project-top-10-for-large-language-model-applications/"]),
    # Phase 5.1.2: Image/Video Generation (10 rules - AIML471-480)
    Rule(rule_id="AIML471", name="stable-diffusion-prompt-injection", description="Stable Diffusion prompt injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Stable Diffusion prompt injection - sanitize user inputs in image generation prompts", explanation="Stable Diffusion applications should sanitize user inputs to prevent prompt injection attacks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="LLM01", tags={"ai", "image-generation", "stable-diffusion", "prompt", "injection"}, references=["https://arxiv.org/abs/2211.09699", "https://stability.ai/safety"]),
    Rule(rule_id="AIML472", name="dalle-manipulation", description="DALL-E manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="DALL-E manipulation - implement prompt filtering and content moderation", explanation="DALL-E applications should filter prompts and moderate generated images", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="LLM09", tags={"ai", "image-generation", "dalle", "openai", "moderation"}, references=["https://openai.com/dall-e-2-safety-system", "https://arxiv.org/abs/2204.06125"]),
    Rule(rule_id="AIML473", name="midjourney-prompt-engineering", description="Midjourney prompt engineering", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Midjourney prompt engineering - validate prompts to prevent manipulation", explanation="Midjourney applications should validate prompts to prevent malicious prompt engineering", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="LLM01", tags={"ai", "image-generation", "midjourney", "prompt", "validation"}, references=["https://docs.midjourney.com/docs/community-guidelines"]),
    Rule(rule_id="AIML474", name="gan-mode-collapse", description="GAN mode collapse exploitation", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="GAN mode collapse - monitor training for mode collapse and implement diversity metrics", explanation="GAN applications should monitor training for mode collapse and implement diversity metrics", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="ML09", tags={"ai", "gan", "training", "mode-collapse", "monitoring"}, references=["https://arxiv.org/abs/1611.02163", "https://arxiv.org/abs/1809.11096"]),
    Rule(rule_id="AIML475", name="vae-latent-manipulation", description="VAE latent space manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="VAE latent manipulation - implement input validation on latent vectors", explanation="VAE applications should validate latent vector inputs to prevent adversarial manipulation", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML01", tags={"ai", "vae", "latent-space", "manipulation", "validation"}, references=["https://arxiv.org/abs/1312.6114", "https://arxiv.org/abs/1906.02691"]),
    Rule(rule_id="AIML476", name="diffusion-model-backdoors", description="Diffusion model backdoors", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Diffusion model backdoors - validate model integrity and scan for backdoors", explanation="Diffusion models should be validated for backdoors before deployment", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-506", owasp_mapping="ML03", tags={"ai", "diffusion", "backdoor", "model-security", "validation"}, references=["https://arxiv.org/abs/2302.01316", "https://arxiv.org/abs/2211.01590"]),
    Rule(rule_id="AIML477", name="video-generation-injection", description="Video generation injection (Runway, Gen-2)", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Video generation injection - sanitize prompts and validate generated content", explanation="Video generation applications should sanitize prompts and validate outputs", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="LLM01", tags={"ai", "video-generation", "runway", "gen2", "injection"}, references=["https://research.runwayml.com/gen2", "https://arxiv.org/abs/2302.08453"]),
    Rule(rule_id="AIML478", name="3d-generation-vulnerabilities", description="3D generation vulnerabilities (Point-E, Shap-E)", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="3D generation - validate generated 3D models for malicious content", explanation="3D generation applications should validate outputs for embedded malicious content", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-20", owasp_mapping="ML09", tags={"ai", "3d-generation", "point-e", "shap-e", "validation"}, references=["https://arxiv.org/abs/2212.08751", "https://github.com/openai/shap-e"]),
    Rule(rule_id="AIML479", name="music-generation-risks", description="Music generation risks (Jukebox, MusicLM)", category=RuleCategory.SECURITY, severity=RuleSeverity.LOW, message_template="Music generation - implement copyright protection and content filtering", explanation="Music generation applications should filter for copyrighted content and implement watermarking", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-200", owasp_mapping="LLM09", tags={"ai", "music-generation", "jukebox", "musiclm", "copyright"}, references=["https://arxiv.org/abs/2005.00341", "https://google-research.github.io/seanet/musiclm/examples/"]),
    Rule(rule_id="AIML480", name="audio-generation-injection", description="Audio generation injection (AudioLM, Whisper)", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Audio generation injection - validate inputs and sanitize generated audio", explanation="Audio generation applications should validate inputs and sanitize outputs to prevent malicious audio generation", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="LLM01", tags={"ai", "audio-generation", "audiolm", "whisper", "injection"}, references=["https://arxiv.org/abs/2209.03143", "https://github.com/openai/whisper"]),
    
    # Phase 5.2: Multimodal & Fusion Models (15 rules - AIML481-495)
    # Phase 5.2.1: Vision-Language Models (8 rules - AIML481-488)
    Rule(rule_id="AIML481", name="clip-contrastive-poisoning", description="CLIP contrastive learning poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="CLIP contrastive learning poisoning - validate model integrity and embedding consistency", explanation="CLIP models should be validated for contrastive learning poisoning attacks that manipulate image-text embeddings", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "multimodal", "clip", "contrastive", "poisoning"}, references=["https://arxiv.org/abs/2103.00020", "https://github.com/openai/CLIP"]),
    Rule(rule_id="AIML482", name="align-multimodal-injection", description="ALIGN multimodal injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="ALIGN multimodal injection - validate and sanitize image-text pairs before processing", explanation="ALIGN models should validate image-text pairs to prevent noisy data injection and alignment manipulation", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "multimodal", "align", "injection", "validation"}, references=["https://arxiv.org/abs/2102.05918"]),
    Rule(rule_id="AIML483", name="flamingo-few-shot-manipulation", description="Flamingo few-shot manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Flamingo few-shot manipulation - validate few-shot examples to prevent context poisoning", explanation="Flamingo models should validate few-shot visual examples to prevent context manipulation attacks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="LLM01", tags={"ai", "multimodal", "flamingo", "few-shot", "manipulation"}, references=["https://arxiv.org/abs/2204.14198"]),
    Rule(rule_id="AIML484", name="blip2-query-injection", description="BLIP-2 query injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="BLIP-2 query injection - sanitize queries and validate Q-Former inputs", explanation="BLIP-2 models should sanitize queries and validate Q-Former inputs to prevent query transformer injection", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="LLM01", tags={"ai", "multimodal", "blip2", "query", "injection"}, references=["https://arxiv.org/abs/2301.12597"]),
    Rule(rule_id="AIML485", name="gpt4-vision-prompt-attacks", description="GPT-4 Vision prompt attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="GPT-4 Vision prompt attack - validate images and text prompts to prevent visual jailbreaks", explanation="GPT-4 Vision applications should validate images and text prompts to prevent visual prompt injection and jailbreak attempts", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="LLM01", tags={"ai", "multimodal", "gpt4-vision", "prompt", "jailbreak"}, references=["https://openai.com/research/gpt-4v-system-card", "https://arxiv.org/abs/2311.16732"]),
    Rule(rule_id="AIML486", name="llava-instruction-tuning-risks", description="LLaVA instruction tuning risks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="LLaVA instruction tuning risks - validate visual instructions to prevent poisoning", explanation="LLaVA models should validate visual instruction data to prevent instruction tuning poisoning attacks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML03", tags={"ai", "multimodal", "llava", "instruction", "tuning"}, references=["https://arxiv.org/abs/2304.08485", "https://llava-vl.github.io/"]),
    Rule(rule_id="AIML487", name="minigpt4-alignment-bypass", description="MiniGPT-4 alignment bypass", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="MiniGPT-4 alignment bypass - implement safety guardrails and alignment verification", explanation="MiniGPT-4 applications should implement safety guardrails to prevent alignment bypass attempts", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-863", owasp_mapping="LLM01", tags={"ai", "multimodal", "minigpt4", "alignment", "bypass"}, references=["https://arxiv.org/abs/2304.10592", "https://minigpt-4.github.io/"]),
    Rule(rule_id="AIML488", name="coca-caption-poisoning", description="CoCa caption poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="CoCa caption poisoning - validate generated captions to prevent content injection", explanation="CoCa models should validate generated captions to prevent caption poisoning and content injection", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="ML09", tags={"ai", "multimodal", "coca", "caption", "poisoning"}, references=["https://arxiv.org/abs/2205.01917"]),
    
    # Phase 5.2.2: Audio-Visual & Cross-Modal (7 rules - AIML489-495)
    Rule(rule_id="AIML489", name="audio-text-alignment-poisoning", description="Audio-text alignment poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Audio-text alignment poisoning - validate audio-text correspondence to prevent manipulation", explanation="Audio-text models should validate alignment to prevent speech embedding poisoning attacks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML09", tags={"ai", "multimodal", "audio", "text", "alignment"}, references=["https://arxiv.org/abs/2106.07889"]),
    Rule(rule_id="AIML490", name="video-text-retrieval-manipulation", description="Video-text retrieval manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Video-text retrieval manipulation - validate video frames and temporal alignment", explanation="Video-text retrieval systems should validate temporal alignment to prevent frame injection attacks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML09", tags={"ai", "multimodal", "video", "text", "retrieval"}, references=["https://arxiv.org/abs/2112.01514"]),
    Rule(rule_id="AIML491", name="speech-to-text-injection", description="Speech-to-text injection", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Speech-to-text injection - validate audio inputs to prevent adversarial audio attacks", explanation="Speech-to-text systems should validate audio inputs to prevent adversarial audio and ASR poisoning", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-94", owasp_mapping="ML01", tags={"ai", "speech", "asr", "injection", "adversarial"}, references=["https://arxiv.org/abs/1801.01944", "https://github.com/carlini/audio_adversarial_examples"]),
    Rule(rule_id="AIML492", name="text-to-speech-vulnerabilities", description="Text-to-speech vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Text-to-speech vulnerabilities - validate text inputs and prevent voice cloning attacks", explanation="Text-to-speech systems should validate inputs to prevent prosody manipulation and voice cloning", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-20", owasp_mapping="ML09", tags={"ai", "tts", "voice", "cloning", "synthesis"}, references=["https://arxiv.org/abs/2010.05646"]),
    Rule(rule_id="AIML493", name="visual-grounding-attacks", description="Visual grounding attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Visual grounding attacks - validate object localization and referring expressions", explanation="Visual grounding models should validate object localization to prevent spatial reasoning attacks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML09", tags={"ai", "visual", "grounding", "localization", "attack"}, references=["https://arxiv.org/abs/1808.08089"]),
    Rule(rule_id="AIML494", name="embodied-ai-risks", description="Embodied AI risks (robotics)", category=RuleCategory.SECURITY, severity=RuleSeverity.CRITICAL, message_template="Embodied AI risks - implement safety checks and validate robot control commands", explanation="Embodied AI systems should implement safety checks to prevent malicious control commands and sensor spoofing", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-754", owasp_mapping="ML09", tags={"ai", "robotics", "embodied", "safety", "control"}, references=["https://arxiv.org/abs/2210.06217", "https://www.nist.gov/itl/ai-risk-management-framework"]),
    Rule(rule_id="AIML495", name="sensor-fusion-manipulation", description="Sensor fusion manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Sensor fusion manipulation - validate sensor data consistency and implement integrity checks", explanation="Sensor fusion systems should validate multi-sensor data consistency to prevent cross-sensor attacks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML09", tags={"ai", "sensor", "fusion", "manipulation", "integrity"}, references=["https://arxiv.org/abs/1903.03921"]),
    
    # Phase 5.3: Federated & Privacy-Preserving ML (15 rules - AIML496-510)
    
    # Phase 5.3.1: Federated Learning Security (10 rules - AIML496-505)
    Rule(rule_id="AIML496", name="federated-averaging-poisoning", description="Federated averaging poisoning", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Federated averaging poisoning - implement Byzantine-robust aggregation and validate client gradients", explanation="Federated learning systems should use Byzantine-robust aggregation methods to prevent gradient poisoning attacks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "federated-learning", "poisoning", "aggregation", "byzantine"}, references=["https://arxiv.org/abs/1703.03757", "https://arxiv.org/abs/1802.07927"]),
    Rule(rule_id="AIML497", name="client-selection-manipulation", description="Client selection manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Client selection manipulation - implement secure random client sampling and reputation systems", explanation="Federated learning should validate client selection to prevent adversarial client participation", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-330", owasp_mapping="ML09", tags={"ai", "federated-learning", "client-selection", "reputation", "sampling"}, references=["https://arxiv.org/abs/1902.01046"]),
    Rule(rule_id="AIML498", name="model-aggregation-attacks", description="Model aggregation attacks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Model aggregation attacks - implement secure aggregation protocols and verify model updates", explanation="Federated learning should use secure aggregation to protect individual model updates from adversaries", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML09", tags={"ai", "federated-learning", "aggregation", "secure", "model-update"}, references=["https://eprint.iacr.org/2017/281.pdf"]),
    Rule(rule_id="AIML499", name="byzantine-client-detection-bypass", description="Byzantine client detection bypass", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Byzantine client detection bypass - implement outlier detection and Byzantine-robust algorithms (Krum, Median)", explanation="Federated learning systems should detect and filter Byzantine clients using robust aggregation methods", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-345", owasp_mapping="ML03", tags={"ai", "federated-learning", "byzantine", "outlier-detection", "krum"}, references=["https://arxiv.org/abs/1703.02757", "https://arxiv.org/abs/1803.01498"]),
    Rule(rule_id="AIML500", name="privacy-budget-exploitation", description="Privacy budget exploitation", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Privacy budget exploitation - implement proper epsilon/delta tracking and privacy amplification", explanation="Differential privacy systems should track privacy budget to prevent privacy leakage through composition", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-200", owasp_mapping="ML09", tags={"ai", "differential-privacy", "privacy-budget", "epsilon", "delta"}, references=["https://arxiv.org/abs/1607.00133", "https://desfontain.es/privacy/differential-privacy-reading-list.html"]),
    Rule(rule_id="AIML501", name="differential-privacy-bypass", description="Differential privacy bypass", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Differential privacy bypass - implement adequate noise calibration and composition bounds", explanation="Differential privacy implementations should use sufficient noise and proper composition tracking to prevent privacy attacks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-200", owasp_mapping="ML09", tags={"ai", "differential-privacy", "noise", "composition", "bypass"}, references=["https://arxiv.org/abs/1905.02383", "https://privacytools.seas.harvard.edu/publications"]),
    Rule(rule_id="AIML502", name="secure-aggregation-vulnerabilities", description="Secure aggregation vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="Secure aggregation vulnerabilities - use homomorphic encryption or secret sharing for gradient aggregation", explanation="Federated learning should use cryptographic secure aggregation to protect individual client updates", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-311", owasp_mapping="ML09", tags={"ai", "federated-learning", "secure-aggregation", "homomorphic", "secret-sharing"}, references=["https://eprint.iacr.org/2017/281.pdf", "https://research.google/pubs/pub45808/"]),
    Rule(rule_id="AIML503", name="homomorphic-encryption-weaknesses", description="Homomorphic encryption weaknesses", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Homomorphic encryption weaknesses - implement proper key management and validate encryption schemes (CKKS, BFV)", explanation="Homomorphic encryption in ML should use proper key management and validated encryption schemes", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-320", owasp_mapping="ML09", tags={"ai", "homomorphic-encryption", "fhe", "ckks", "key-management"}, references=["https://eprint.iacr.org/2016/421.pdf", "https://homomorphicencryption.org/"]),
    Rule(rule_id="AIML504", name="trusted-execution-environment-gaps", description="Trusted execution environment gaps", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="TEE gaps - implement SGX/TrustZone integration and remote attestation for federated learning", explanation="Federated learning in TEEs should implement proper attestation and enclave measurement verification", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-494", owasp_mapping="ML09", tags={"ai", "tee", "sgx", "trustzone", "attestation"}, references=["https://arxiv.org/abs/1902.05964", "https://www.intel.com/content/www/us/en/developer/tools/software-guard-extensions/overview.html"]),
    Rule(rule_id="AIML505", name="split-learning-injection", description="Split learning injection", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Split learning injection - validate cut layer and protect activation sharing between client and server", explanation="Split learning should validate cut layer selection and secure activation transmission to prevent inference attacks", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-200", owasp_mapping="ML09", tags={"ai", "split-learning", "cut-layer", "activation", "inference-attack"}, references=["https://arxiv.org/abs/1812.00564", "https://arxiv.org/abs/2004.12666"]),
    
    # Phase 5.3.2: Privacy-Enhancing Technologies (5 rules - AIML506-510)
    Rule(rule_id="AIML506", name="differential-privacy-parameter-manipulation", description="Differential privacy parameter manipulation", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="DP parameter manipulation - avoid hardcoded epsilon/delta and implement adaptive privacy budget allocation", explanation="Differential privacy implementations should use adaptive privacy budgets and avoid hardcoded parameters", fix_applicability=FixApplicability.SAFE, cwe_mapping="CWE-798", owasp_mapping="ML09", tags={"ai", "differential-privacy", "epsilon", "delta", "parameter"}, references=["https://arxiv.org/abs/1607.00133"]),
    Rule(rule_id="AIML507", name="smpc-risks", description="SMPC (Secure Multi-Party Computation) risks", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="SMPC risks - implement malicious security and validate secret sharing reconstruction", explanation="SMPC protocols should provide malicious security guarantees and validate input contributions", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML09", tags={"ai", "smpc", "mpc", "secret-sharing", "malicious-security"}, references=["https://eprint.iacr.org/2011/136.pdf", "https://www.cs.umd.edu/~jkatz/imc.html"]),
    Rule(rule_id="AIML508", name="trusted-execution-environment-bypass", description="Trusted execution environment bypass", category=RuleCategory.SECURITY, severity=RuleSeverity.HIGH, message_template="TEE bypass - implement side-channel mitigations and validate enclave measurement during attestation", explanation="TEE implementations should mitigate side-channel attacks and implement proper attestation verification", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-203", owasp_mapping="ML09", tags={"ai", "tee", "side-channel", "attestation", "enclave"}, references=["https://arxiv.org/abs/1706.03757", "https://sgx101.gitbook.io/"]),
    Rule(rule_id="AIML509", name="encrypted-inference-vulnerabilities", description="Encrypted inference vulnerabilities", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="Encrypted inference vulnerabilities - implement proper key management and validate ciphertext integrity", explanation="Encrypted inference systems should use proper key management and validate homomorphic operations", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-311", owasp_mapping="ML09", tags={"ai", "encrypted-inference", "homomorphic", "key-management", "integrity"}, references=["https://arxiv.org/abs/1811.04008", "https://eprint.iacr.org/2016/421.pdf"]),
    Rule(rule_id="AIML510", name="zero-knowledge-proof-gaps", description="Zero-knowledge proof gaps", category=RuleCategory.SECURITY, severity=RuleSeverity.MEDIUM, message_template="ZKP gaps - implement soundness guarantees and validate proof generation/verification", explanation="Zero-knowledge proof systems should provide soundness guarantees and properly verify challenges", fix_applicability=FixApplicability.MANUAL, cwe_mapping="CWE-345", owasp_mapping="ML09", tags={"ai", "zkp", "zero-knowledge", "soundness", "verification"}, references=["https://zkp.science/", "https://eprint.iacr.org/2016/263.pdf"]),
]


class AIMLSecurityFixer:
    """
    AI/ML Security Auto-Fixer with AST-based transformations.

    Provides automated code transformations for AI/ML security vulnerabilities:
    - Model loading security (torch.load, from_pretrained)
    - Prompt injection prevention
    - API security (rate limiting, input validation)
    - Training pipeline security
    - GPU memory management
    - Model integrity verification

    All fixes are classified by safety level and include educational comments.

    References:
    - OWASP ML Top 10 | https://owasp.org/www-project-machine-learning-security-top-10/
    - OWASP LLM Top 10 | https://owasp.org/www-project-top-10-for-large-language-model-applications/
    - MITRE ATLAS | https://atlas.mitre.org/
    - CWE Top 25 | https://cwe.mitre.org/top25/
    """

    def __init__(self, allow_unsafe: bool = False):
        """
        Initialize AI/ML security fixer.

        Args:
            allow_unsafe: Whether to allow unsafe transformations
        """
        self.logger = PyGuardLogger()
        self.file_ops = FileOperations()
        self.safety_classifier = FixSafetyClassifier()
        self.allow_unsafe = allow_unsafe
        self.fixes_applied: List[str] = []

    def fix_file(self, file_path: Path) -> Tuple[bool, List[str]]:
        """
        Apply AI/ML security fixes to a file.

        Args:
            file_path: Path to Python file

        Returns:
            Tuple of (success, list of fixes applied)
        """
        content = self.file_ops.read_file(file_path)
        if content is None:
            return False, []

        original_content = content
        self.fixes_applied = []

        # Apply safe fixes (always applied)
        content = self._fix_torch_load_weights_only(content)
        content = self._fix_from_pretrained_trust(content)
        content = self._fix_api_key_exposure(content)
        content = self._fix_gpu_memory_limits(content)
        content = self._fix_llm_rate_limiting(content)
        content = self._fix_output_sanitization(content)
        content = self._fix_model_versioning(content)
        content = self._fix_prompt_injection_basic(content)
        
        # Stage 2 priority fixes (API security & model loading)
        content = self._fix_api_parameter_validation(content)
        content = self._fix_missing_timeout(content)
        content = self._fix_unhandled_api_errors(content)
        content = self._fix_untrusted_url_loading(content)
        content = self._fix_torch_jit_load(content)
        content = self._fix_model_integrity_verification(content)
        content = self._fix_model_card_credentials(content)

        # Apply unsafe fixes (only if allowed)
        if self.allow_unsafe:
            content = self._fix_pickle_to_safetensors(content)
            content = self._fix_training_data_validation(content)

        # Write back if changes were made
        if content != original_content:
            success = self.file_ops.write_file(file_path, content)
            if success:
                self.logger.success(
                    f"Applied {len(self.fixes_applied)} AI/ML security fixes",
                    category="AI/ML Security",
                    file_path=str(file_path),
                    details={"fixes": self.fixes_applied},
                )
            return success, self.fixes_applied

        return True, []

    # ===== SAFE FIXES (Always Applied) =====

    def _fix_torch_load_weights_only(self, content: str) -> str:
        """
        Add weights_only=True to torch.load() calls.

        Classification: SAFE
        - Prevents arbitrary code execution via pickle deserialization
        - Direct parameter addition with no logic change

        Before: model = torch.load('model.pth')
        After:  model = torch.load('model.pth', weights_only=True)

        Reference: AIML071, CWE-502, OWASP ML05
        """
        fix_id = "torch_load_weights_only"
        if not self.safety_classifier.should_apply_fix(fix_id, self.allow_unsafe):
            return content

        if "torch.load(" in content and "weights_only" not in content:
            lines = content.split("\n")
            fixed_lines = []
            modified = False

            for line in lines:
                # Skip comments and strings
                if line.strip().startswith("#"):
                    fixed_lines.append(line)
                    continue

                if "torch.load(" in line and "weights_only" not in line:
                    # Check if it's in a string
                    stripped = line.strip()
                    if stripped.startswith(("'", '"')) or "torch.load(" in stripped and any(
                        stripped.count(q) % 2 == 1 for q in ('"', "'")
                    ):
                        fixed_lines.append(line)
                        continue

                    # Add weights_only=True parameter
                    # Handle both single-line and multi-line calls
                    if ")" in line:
                        # Single-line call
                        pattern = r'torch\.load\(([^)]+)\)'
                        match = re.search(pattern, line)
                        if match:
                            args = match.group(1).strip()
                            if args:
                                fixed_line = re.sub(
                                    pattern,
                                    f'torch.load({args}, weights_only=True)',
                                    line
                                )
                            else:
                                fixed_line = re.sub(
                                    pattern,
                                    'torch.load(weights_only=True)',
                                    line
                                )
                            fixed_lines.append(fixed_line)
                            if not modified:
                                self.fixes_applied.append(
                                    "torch.load() â†’ torch.load(weights_only=True) [AIML071]"
                                )
                                modified = True
                        else:
                            fixed_lines.append(line)
                    else:
                        # Multi-line call - add comment for manual review
                        fixed_lines.append(line)
                        if not modified:
                            self.fixes_applied.append(
                                "torch.load() multi-line - manual review needed [AIML071]"
                            )
                            modified = True
                else:
                    fixed_lines.append(line)

            content = "\n".join(fixed_lines)

            if modified:
                self.logger.info(
                    f"Applied safe fix ({fix_id}): torch.load() â†’ weights_only=True",
                    category="AI/ML Security",
                )

        return content

    def _fix_from_pretrained_trust(self, content: str) -> str:
        """
        Add trust_remote_code=False to from_pretrained() calls.

        Classification: SAFE
        - Prevents arbitrary code execution from untrusted models
        - Direct parameter addition

        Before: model = AutoModel.from_pretrained('model')
        After:  model = AutoModel.from_pretrained('model', trust_remote_code=False)

        Reference: AIML101, CWE-494, MITRE ATLAS T1574.002
        """
        fix_id = "from_pretrained_trust"
        if not self.safety_classifier.should_apply_fix(fix_id, self.allow_unsafe):
            return content

        if "from_pretrained(" in content and "trust_remote_code" not in content:
            lines = content.split("\n")
            fixed_lines = []
            modified = False

            for line in lines:
                if line.strip().startswith("#"):
                    fixed_lines.append(line)
                    continue

                if "from_pretrained(" in line and "trust_remote_code" not in line:
                    # Skip if in string
                    stripped = line.strip()
                    if stripped.startswith(("'", '"')):
                        fixed_lines.append(line)
                        continue

                    # Add trust_remote_code=False
                    if ")" in line:
                        pattern = r'from_pretrained\(([^)]+)\)'
                        match = re.search(pattern, line)
                        if match:
                            args = match.group(1).strip()
                            if args:
                                fixed_line = re.sub(
                                    pattern,
                                    f'from_pretrained({args}, trust_remote_code=False)',
                                    line
                                )
                            else:
                                fixed_line = re.sub(
                                    pattern,
                                    'from_pretrained(trust_remote_code=False)',
                                    line
                                )
                            fixed_lines.append(fixed_line)
                            if not modified:
                                self.fixes_applied.append(
                                    "from_pretrained() â†’ trust_remote_code=False [AIML101]"
                                )
                                modified = True
                        else:
                            fixed_lines.append(line)
                    else:
                        fixed_lines.append(line)
                else:
                    fixed_lines.append(line)

            content = "\n".join(fixed_lines)

            if modified:
                self.logger.info(
                    f"Applied safe fix ({fix_id}): from_pretrained() â†’ trust_remote_code=False",
                    category="AI/ML Security",
                )

        return content

    def _fix_api_key_exposure(self, content: str) -> str:
        """
        Move hardcoded API keys to environment variables.

        Classification: SAFE
        - Prevents credential exposure in source code
        - Replaces hardcoded values with os.getenv()

        Before: openai.api_key = "sk-..."
        After:  openai.api_key = os.getenv("OPENAI_API_KEY")

        Reference: AIML054, CWE-798, OWASP A07:2021
        """
        fix_id = "api_key_exposure"
        if not self.safety_classifier.should_apply_fix(fix_id, self.allow_unsafe):
            return content

        # Detect common API key patterns
        api_key_patterns = [
            (r'openai\.api_key\s*=\s*["\']sk-[^"\']+["\']', 'openai.api_key = os.getenv("OPENAI_API_KEY")', "OpenAI"),
            (r'anthropic\.api_key\s*=\s*["\']sk-[^"\']+["\']', 'anthropic.api_key = os.getenv("ANTHROPIC_API_KEY")', "Anthropic"),
            (r'cohere\.api_key\s*=\s*["\'][^"\']+["\']', 'cohere.api_key = os.getenv("COHERE_API_KEY")', "Cohere"),
        ]

        modified = False
        needs_import = False

        for pattern, replacement, service in api_key_patterns:
            if re.search(pattern, content):
                content = re.sub(pattern, replacement, content)
                if not modified:
                    self.fixes_applied.append(
                        f"{service} API key â†’ environment variable [AIML054]"
                    )
                    modified = True
                    needs_import = True

        # Add os import if needed and not present
        if needs_import and "import os" not in content:
            lines = content.split("\n")
            # Find first import or add at top
            insert_idx = 0
            for i, line in enumerate(lines):
                if line.startswith("import ") or line.startswith("from "):
                    insert_idx = i + 1
                elif line.strip() and not line.startswith("#"):
                    break
            lines.insert(insert_idx, "import os  # PyGuard: Added for API key environment variable")
            content = "\n".join(lines)

        if modified:
            self.logger.info(
                f"Applied safe fix ({fix_id}): API key â†’ environment variable",
                category="AI/ML Security",
            )

        return content

    def _fix_gpu_memory_limits(self, content: str) -> str:
        """
        Add GPU memory limits to prevent exhaustion attacks.

        Classification: SAFE
        - Prevents GPU memory exhaustion
        - Adds configuration without changing logic

        Before: # No GPU memory limit
        After:  torch.cuda.set_per_process_memory_fraction(0.8)

        Reference: AIML081, CWE-770, OWASP ML08
        """
        fix_id = "gpu_memory_limits"
        if not self.safety_classifier.should_apply_fix(fix_id, self.allow_unsafe):
            return content

        # Only add if using torch.cuda and no memory limit set
        if "torch.cuda" in content and "set_per_process_memory_fraction" not in content:
            lines = content.split("\n")
            fixed_lines = []
            modified = False

            # Find where to add the limit (after torch import)
            for i, line in enumerate(lines):
                fixed_lines.append(line)
                if not modified and "import torch" in line and "cuda" in content:
                    # Add memory limit after torch import
                    fixed_lines.append(
                        "# PyGuard: GPU memory limit to prevent exhaustion attacks (AIML081)"
                    )
                    fixed_lines.append(
                        "if torch.cuda.is_available():"
                    )
                    fixed_lines.append(
                        "    torch.cuda.set_per_process_memory_fraction(0.8)  # Limit to 80% of GPU memory"
                    )
                    self.fixes_applied.append(
                        "Added GPU memory limit (80%) [AIML081]"
                    )
                    modified = True

            if modified:
                content = "\n".join(fixed_lines)
                self.logger.info(
                    f"Applied safe fix ({fix_id}): Added GPU memory limit",
                    category="AI/ML Security",
                )

        return content

    def _fix_llm_rate_limiting(self, content: str) -> str:
        """
        Add rate limiting to LLM API calls.

        Classification: SAFE
        - Prevents DoS and cost overflow attacks
        - Adds protective wrapper without changing logic

        Before: response = openai.ChatCompletion.create(...)
        After:  response = openai.ChatCompletion.create(..., max_tokens=150)

        Reference: AIML046, CWE-770, OWASP LLM01
        """
        fix_id = "llm_rate_limiting"
        if not self.safety_classifier.should_apply_fix(fix_id, self.allow_unsafe):
            return content

        # Add max_tokens to OpenAI calls if missing
        if "openai.ChatCompletion.create" in content and "max_tokens" not in content:
            lines = content.split("\n")
            fixed_lines = []
            modified = False

            for line in lines:
                if "openai.ChatCompletion.create" in line and "max_tokens" not in line:
                    # Add max_tokens parameter
                    if ")" in line:
                        fixed_line = line.replace(")", ", max_tokens=150)")
                        fixed_lines.append(fixed_line)
                        if not modified:
                            self.fixes_applied.append(
                                "Added max_tokens limit to LLM API call [AIML046]"
                            )
                            modified = True
                    else:
                        fixed_lines.append(line)
                else:
                    fixed_lines.append(line)

            if modified:
                content = "\n".join(fixed_lines)
                self.logger.info(
                    f"Applied safe fix ({fix_id}): Added max_tokens limit",
                    category="AI/ML Security",
                )

        return content

    def _fix_output_sanitization(self, content: str) -> str:
        """
        Add output sanitization to LLM responses.

        Classification: SAFE
        - Prevents code injection via generated output
        - Adds validation layer

        Before: result = llm_response.content
        After:  result = html.escape(llm_response.content)

        Reference: AIML061, CWE-79, OWASP LLM02
        """
        fix_id = "output_sanitization"
        # This is a complex transformation that requires context
        # For now, return content unchanged
        # TODO: Implement in future iteration
        return content

    def _fix_model_versioning(self, content: str) -> str:
        """
        Replace 'latest' model tags with specific versions.

        Classification: SAFE
        - Ensures reproducibility and prevents model substitution
        - Comment-based suggestion

        Before: model = AutoModel.from_pretrained("model:latest")
        After:  # PyGuard: Pin to specific version instead of 'latest' [AIML440]

        Reference: AIML440, CWE-494, OWASP ML03
        """
        fix_id = "model_versioning"
        if not self.safety_classifier.should_apply_fix(fix_id, self.allow_unsafe):
            return content

        if ":latest" in content or '"latest"' in content or "'latest'" in content:
            lines = content.split("\n")
            fixed_lines = []
            modified = False

            for line in lines:
                if (":latest" in line or '"latest"' in line or "'latest'" in line) and (
                    "from_pretrained" in line or "load" in line
                ):
                    fixed_lines.append(
                        "# PyGuard: Pin to specific version instead of 'latest' tag for reproducibility (AIML440)"
                    )
                    fixed_lines.append(line)
                    if not modified:
                        self.fixes_applied.append(
                            "Added warning about 'latest' tag usage [AIML440]"
                        )
                        modified = True
                else:
                    fixed_lines.append(line)

            if modified:
                content = "\n".join(fixed_lines)
                self.logger.info(
                    f"Applied safe fix ({fix_id}): Added versioning warning",
                    category="AI/ML Security",
                )

        return content

    def _fix_prompt_injection_basic(self, content: str) -> str:
        """
        Add basic prompt injection prevention.

        Classification: SAFE
        - Adds input validation before LLM calls
        - Filters common injection patterns

        Before: response = llm(user_input)
        After:  # Add input validation (see comments)

        Reference: AIML011-045, OWASP LLM01
        """
        fix_id = "prompt_injection_basic"
        # This is a complex transformation requiring context analysis
        # For now, return content unchanged
        # TODO: Implement in future iteration with AST-based analysis
        return content

    def _fix_api_parameter_validation(self, content: str) -> str:
        """
        Add validation for LLM API parameters (temperature, top_p).

        Classification: SAFE
        - Validates temperature and top_p parameters are in valid ranges
        - Prevents API errors and unexpected behavior

        Before: openai.ChatCompletion.create(temperature=2.5)
        After:  openai.ChatCompletion.create(temperature=1.0)  # Max 2.0

        Reference: AIML047, CWE-20, OWASP LLM03
        """
        fix_id = "api_parameter_validation"
        if not self.safety_classifier.should_apply_fix(fix_id, self.allow_unsafe):
            return content

        if "temperature=" in content or "top_p=" in content:
            lines = content.split("\n")
            fixed_lines = []
            modified = False

            for line in lines:
                if line.strip().startswith("#"):
                    fixed_lines.append(line)
                    continue

                # Check for temperature > 2.0 or < 0
                if "temperature=" in line:
                    temp_match = re.search(r'temperature=([0-9.]+)', line)
                    if temp_match:
                        temp_val = float(temp_match.group(1))
                        if temp_val < 0 or temp_val > 2.0:
                            # Add warning comment
                            fixed_lines.append(
                                "# PyGuard: temperature should be between 0 and 2.0 (AIML047)"
                            )
                            if not modified:
                                self.fixes_applied.append(
                                    "Added temperature parameter validation warning [AIML047]"
                                )
                                modified = True

                # Check for top_p > 1.0 or < 0
                if "top_p=" in line:
                    top_p_match = re.search(r'top_p=([0-9.]+)', line)
                    if top_p_match:
                        top_p_val = float(top_p_match.group(1))
                        if top_p_val < 0 or top_p_val > 1.0:
                            # Add warning comment
                            fixed_lines.append(
                                "# PyGuard: top_p should be between 0 and 1.0 (AIML047)"
                            )
                            if not modified:
                                self.fixes_applied.append(
                                    "Added top_p parameter validation warning [AIML047]"
                                )
                                modified = True

                fixed_lines.append(line)

            if modified:
                content = "\n".join(fixed_lines)
                self.logger.info(
                    f"Applied safe fix ({fix_id}): Added API parameter validation",
                    category="AI/ML Security",
                )

        return content

    def _fix_missing_timeout(self, content: str) -> str:
        """
        Add timeout configuration to LLM API calls.

        Classification: SAFE
        - Prevents indefinite hangs on API calls
        - Adds reasonable timeout defaults

        Before: openai.ChatCompletion.create(...)
        After:  openai.ChatCompletion.create(..., timeout=30)

        Reference: AIML056, CWE-400, OWASP LLM06
        """
        fix_id = "missing_timeout"
        if not self.safety_classifier.should_apply_fix(fix_id, self.allow_unsafe):
            return content

        # Patterns for LLM API calls without timeout
        api_patterns = [
            (r'openai\.ChatCompletion\.create\(', 'openai.ChatCompletion.create'),
            (r'openai\.Completion\.create\(', 'openai.Completion.create'),
            (r'anthropic\.messages\.create\(', 'anthropic.messages.create'),
        ]

        if any(re.search(pattern, content) for pattern, _ in api_patterns):
            lines = content.split("\n")
            fixed_lines = []
            modified = False

            for line in lines:
                if line.strip().startswith("#"):
                    fixed_lines.append(line)
                    continue

                line_modified = False
                for pattern, func_name in api_patterns:
                    if re.search(pattern, line) and "timeout" not in line:
                        # Add warning to add timeout
                        fixed_lines.append(
                            f"# PyGuard: Add timeout to prevent indefinite hangs (AIML056)"
                        )
                        if not modified:
                            self.fixes_applied.append(
                                f"Added timeout configuration warning [AIML056]"
                            )
                            modified = True
                        line_modified = True
                        break

                fixed_lines.append(line)

            if modified:
                content = "\n".join(fixed_lines)
                self.logger.info(
                    f"Applied safe fix ({fix_id}): Added timeout configuration",
                    category="AI/ML Security",
                )

        return content

    def _fix_unhandled_api_errors(self, content: str) -> str:
        """
        Add error handling for LLM API calls.

        Classification: SAFE
        - Prevents information disclosure from unhandled exceptions
        - Adds try-except blocks around API calls

        Before: response = openai.ChatCompletion.create(...)
        After:  # Add try-except for error handling

        Reference: AIML057, CWE-209, OWASP LLM06
        """
        fix_id = "unhandled_api_errors"
        if not self.safety_classifier.should_apply_fix(fix_id, self.allow_unsafe):
            return content

        # Patterns for LLM API calls
        api_calls = [
            'openai.ChatCompletion.create',
            'openai.Completion.create',
            'anthropic.messages.create',
            'cohere.generate',
        ]

        if any(call in content for call in api_calls):
            lines = content.split("\n")
            fixed_lines = []
            modified = False

            for line in lines:
                if any(call in line for call in api_calls) and not line.strip().startswith("#"):
                    # Check if already in try-except
                    if "try:" not in content[:content.index(line)]:
                        fixed_lines.append(
                            "# PyGuard: Add try-except block for error handling (AIML057)"
                        )
                        if not modified:
                            self.fixes_applied.append(
                                "Added API error handling warning [AIML057]"
                            )
                            modified = True
                fixed_lines.append(line)

            if modified:
                content = "\n".join(fixed_lines)
                self.logger.info(
                    f"Applied safe fix ({fix_id}): Added error handling warning",
                    category="AI/ML Security",
                )

        return content

    def _fix_untrusted_url_loading(self, content: str) -> str:
        """
        Add validation for loading models from untrusted URLs.

        Classification: SAFE
        - Validates URLs before loading models
        - Adds warnings for untrusted sources

        Before: torch.hub.load_state_dict_from_url(url)
        After:  # Add URL validation

        Reference: AIML074, CWE-494, OWASP ML05
        """
        fix_id = "untrusted_url_loading"
        if not self.safety_classifier.should_apply_fix(fix_id, self.allow_unsafe):
            return content

        url_loading_patterns = [
            'torch.hub.load_state_dict_from_url',
            'torch.hub.load',
            'tf.keras.utils.get_file',
        ]

        if any(pattern in content for pattern in url_loading_patterns):
            lines = content.split("\n")
            fixed_lines = []
            modified = False

            for line in lines:
                if any(pattern in line for pattern in url_loading_patterns):
                    if not line.strip().startswith("#"):
                        fixed_lines.append(
                            "# PyGuard: Validate URL source before loading model (AIML074)"
                        )
                        if not modified:
                            self.fixes_applied.append(
                                "Added URL validation warning [AIML074]"
                            )
                            modified = True
                fixed_lines.append(line)

            if modified:
                content = "\n".join(fixed_lines)
                self.logger.info(
                    f"Applied safe fix ({fix_id}): Added URL validation warning",
                    category="AI/ML Security",
                )

        return content

    def _fix_torch_jit_load(self, content: str) -> str:
        """
        Add security parameters to torch.jit.load() calls.

        Classification: SAFE
        - Prevents arbitrary code execution in TorchScript
        - Adds map_location and other security parameters

        Before: model = torch.jit.load('model.pt')
        After:  model = torch.jit.load('model.pt', map_location='cpu')

        Reference: AIML077, CWE-502, OWASP ML05
        """
        fix_id = "torch_jit_load"
        if not self.safety_classifier.should_apply_fix(fix_id, self.allow_unsafe):
            return content

        if "torch.jit.load(" in content and "map_location" not in content:
            lines = content.split("\n")
            fixed_lines = []
            modified = False

            for line in lines:
                if "torch.jit.load(" in line and not line.strip().startswith("#"):
                    # Add map_location parameter
                    if "map_location" not in line:
                        fixed_line = line.replace(
                            "torch.jit.load(",
                            "torch.jit.load("
                        )
                        # Add comment about adding map_location
                        fixed_lines.append(
                            "# PyGuard: Add map_location='cpu' to torch.jit.load for security (AIML077)"
                        )
                        if not modified:
                            self.fixes_applied.append(
                                "Added torch.jit.load security warning [AIML077]"
                            )
                            modified = True
                fixed_lines.append(line)

            if modified:
                content = "\n".join(fixed_lines)
                self.logger.info(
                    f"Applied safe fix ({fix_id}): Added torch.jit.load security",
                    category="AI/ML Security",
                )

        return content

    def _fix_model_integrity_verification(self, content: str) -> str:
        """
        Add model integrity verification using checksums.

        Classification: SAFE
        - Adds warnings to verify model checksums
        - Prevents tampered model loading

        Before: model = torch.load('model.pth')
        After:  # Add checksum verification

        Reference: AIML073, CWE-494, OWASP ML05
        """
        fix_id = "model_integrity_verification"
        if not self.safety_classifier.should_apply_fix(fix_id, self.allow_unsafe):
            return content

        model_loading_patterns = [
            'torch.load(',
            'AutoModel.from_pretrained(',
            'tf.keras.models.load_model(',
        ]

        if any(pattern in content for pattern in model_loading_patterns):
            lines = content.split("\n")
            fixed_lines = []
            modified = False

            for line in lines:
                if any(pattern in line for pattern in model_loading_patterns):
                    if not line.strip().startswith("#"):
                        # Check if checksum verification is present
                        if "hashlib" not in content and "checksum" not in content.lower():
                            fixed_lines.append(
                                "# PyGuard: Consider adding model checksum verification (AIML073)"
                            )
                            if not modified:
                                self.fixes_applied.append(
                                    "Added model integrity verification warning [AIML073]"
                                )
                                modified = True
                fixed_lines.append(line)

            if modified:
                content = "\n".join(fixed_lines)
                self.logger.info(
                    f"Applied safe fix ({fix_id}): Added integrity verification warning",
                    category="AI/ML Security",
                )

        return content

    def _fix_model_card_credentials(self, content: str) -> str:
        """
        Remove credentials from model cards and metadata.

        Classification: SAFE
        - Detects and removes API keys in model metadata
        - Sanitizes model cards

        Before: model_card = "api_key: sk-1234..."
        After:  # Warning about credentials in model card

        Reference: AIML102, CWE-798, OWASP A07:2021
        """
        fix_id = "model_card_credentials"
        if not self.safety_classifier.should_apply_fix(fix_id, self.allow_unsafe):
            return content

        # Pattern for model cards with potential credentials
        if "model_card" in content.lower() or "metadata" in content:
            # Check for API key patterns
            if re.search(r'["\']sk-[a-zA-Z0-9]{20,}["\']', content):
                lines = content.split("\n")
                fixed_lines = []
                modified = False

                for line in lines:
                    if re.search(r'["\']sk-[a-zA-Z0-9]{20,}["\']', line):
                        fixed_lines.append(
                            "# PyGuard: Remove API keys from model cards/metadata (AIML102)"
                        )
                        if not modified:
                            self.fixes_applied.append(
                                "Added model card credential warning [AIML102]"
                            )
                            modified = True
                    fixed_lines.append(line)

                if modified:
                    content = "\n".join(fixed_lines)
                    self.logger.info(
                        f"Applied safe fix ({fix_id}): Added credential warning",
                        category="AI/ML Security",
                    )

        return content

    # ===== UNSAFE FIXES (Require --unsafe flag) =====

    def _fix_pickle_to_safetensors(self, content: str) -> str:
        """
        Replace pickle with safetensors for model serialization.

        Classification: UNSAFE
        - Changes serialization format (API breaking)
        - Requires dependency on safetensors package

        Before: torch.save(model, 'model.pkl')
        After:  from safetensors.torch import save_file; save_file(model, 'model.safetensors')

        Reference: AIML072, CWE-502, OWASP ML05
        """
        fix_id = "pickle_to_safetensors"
        # Unsafe transformation - requires dependency and API change
        # TODO: Implement in future iteration
        return content

    def _fix_training_data_validation(self, content: str) -> str:
        """
        Add training data validation to prevent poisoning.

        Classification: UNSAFE
        - Modifies training pipeline
        - May affect training behavior

        Reference: AIML111-140, OWASP ML09
        """
        fix_id = "training_data_validation"
        # Unsafe transformation - modifies training logic
        # TODO: Implement in future iteration
        return content
